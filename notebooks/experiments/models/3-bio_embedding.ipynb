{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix max_length to be 500\n",
    "max_length = 500\n",
    "\n",
    "#drop rows that exceeds this value\n",
    "\n",
    "train_df = train_df[train_df['length'] < max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_seq(sequence):\n",
    "    alphabet = ['A', 'C', 'D', 'E', 'F', 'G','H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] # aa letters\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet)) \n",
    "    integer_encoded = [char_to_int[char] for char in sequence] #each character becomes int\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(alphabet))] #0 for all letters\n",
    "        letter[value] = 1 #modify the column corresponding to the letter to 1\n",
    "        onehot_encoded.append(letter) #put in the array (1 letter = 1 array of 20 columns)\n",
    "    \n",
    "    ar =   np.transpose(np.array(onehot_encoded))\n",
    "    zeros = np.zeros([len(alphabet),max_length - len(integer_encoded)] )\n",
    "    onehot_encoded = np.concatenate((ar, zeros), axis = 1) #zero padding\n",
    "\n",
    "\n",
    "    return onehot_encoded #we have all arrays, corresponding to the whole sequence\n",
    "\n",
    "\n",
    "# new column with encoded sequence (apply for each sequence)\n",
    "train_df['encoded_sequence'] = train_df['protein_sequence'].apply(lambda x: encode_seq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['encoded_sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding inverted sequences (DISCRADED) \n",
    "**Add it to report as an experiment ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "inverted_df = train_df.copy()\n",
    "\n",
    "#inver the protein sequence\n",
    "inverted_df['protein_sequence'] = inverted_df.apply(lambda row : row['protein_sequence'][::-1],axis=1)\n",
    "inverted_df['numerical_sequence'] = inverted_df.apply(lambda row : chr_to_int(row['protein_sequence']),axis=1)\n",
    "\n",
    "# merge the original and inverted dataframes (size of dataset is doubled)\n",
    "train_df = pd.concat([train_df,inverted_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Padding with zeros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add 0 to numerical_sequence to make all of them the same length\n",
    "padded_train_df = train_df.copy()\n",
    "padded_train_df['encoded_sequence'] = train_df.apply(lambda row : row['encoded_sequence'][:] + [0]*(max_length - row['length']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prepare dataframe to Pass to the dataloader (will be put in a function later)\n",
    "padded_train_df.drop(columns=['protein_sequence'],inplace=True)\n",
    "padded_train_df['y'] = padded_train_df['tm']\n",
    "padded_train_df.drop(columns=['tm'],inplace=True)\n",
    "padded_train_df['numerical_sequence'] = padded_train_df.apply(lambda row : np.array(row['numerical_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = df.sample(frac=0.8,random_state=24)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add space betwwen each letter in the sequence\n",
    "train_df['protein_sequence'] = train_df['protein_sequence'].apply(lambda x: ' '.join(x))\n",
    "val_df['protein_sequence'] = val_df['protein_sequence'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#for each protein sequence, we create a list of words (each word is a letter)\n",
    "train_df['tokenized_sequence'] = train_df['protein_sequence'].apply(lambda x: word_tokenize(x))\n",
    "val_df['tokenized_sequence'] = val_df['protein_sequence'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M K A L I V L G L V L L S V T V Q G K V F E R ...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>57.8</td>\n",
       "      <td>148</td>\n",
       "      <td>[M, K, A, L, I, V, L, G, L, V, L, L, S, V, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M N Y G Y F I E T G I P G D D I E N I K K F L ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>74.7</td>\n",
       "      <td>453</td>\n",
       "      <td>[M, N, Y, G, Y, F, I, E, T, G, I, P, G, D, D, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M A F V A T Q G A T V V D Q T T L M K K Y L Q ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>51.2</td>\n",
       "      <td>3828</td>\n",
       "      <td>[M, A, F, V, A, T, Q, G, A, T, V, V, D, Q, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M P P K T R K T K K E D V E E E A V E E I L E ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>289</td>\n",
       "      <td>[M, P, P, K, T, R, K, T, K, K, E, D, V, E, E, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M Q F K V Y T Y K R E S R Y R L F V D V Q S D ...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>53.2</td>\n",
       "      <td>101</td>\n",
       "      <td>[M, Q, F, K, V, Y, T, Y, K, R, E, S, R, Y, R, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22951</th>\n",
       "      <td>G I P I L V I F L A I G M L A G V D G V G G I ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>547</td>\n",
       "      <td>[G, I, P, I, L, V, I, F, L, A, I, G, M, L, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22952</th>\n",
       "      <td>M M L R G E Q T D V M V T K E Q K E R F L I L ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>62.1</td>\n",
       "      <td>1443</td>\n",
       "      <td>[M, M, L, R, G, E, Q, T, D, V, M, V, T, K, E, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22953</th>\n",
       "      <td>M E R Y G R P G E E G S R S D P S L E W T S H ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>461</td>\n",
       "      <td>[M, E, R, Y, G, R, P, G, E, E, G, S, R, S, D, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22954</th>\n",
       "      <td>M A A T T N S F L V G S N N T Q I P A L K P K ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>429</td>\n",
       "      <td>[M, A, A, T, T, N, S, F, L, V, G, S, N, N, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22955</th>\n",
       "      <td>M A T S S H L L P Q A L H M I P R T P S F S S ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>228</td>\n",
       "      <td>[M, A, T, S, S, H, L, L, P, Q, A, L, H, M, I, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22956 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        protein_sequence   pH    tm  length  \\\n",
       "0      M K A L I V L G L V L L S V T V Q G K V F E R ...  2.2  57.8     148   \n",
       "1      M N Y G Y F I E T G I P G D D I E N I K K F L ...  7.0  74.7     453   \n",
       "2      M A F V A T Q G A T V V D Q T T L M K K Y L Q ...  7.0  51.2    3828   \n",
       "3      M P P K T R K T K K E D V E E E A V E E I L E ...  7.0  36.3     289   \n",
       "4      M Q F K V Y T Y K R E S R Y R L F V D V Q S D ...  7.5  53.2     101   \n",
       "...                                                  ...  ...   ...     ...   \n",
       "22951  G I P I L V I F L A I G M L A G V D G V G G I ...  7.0  50.8     547   \n",
       "22952  M M L R G E Q T D V M V T K E Q K E R F L I L ...  7.0  62.1    1443   \n",
       "22953  M E R Y G R P G E E G S R S D P S L E W T S H ...  7.0  44.6     461   \n",
       "22954  M A A T T N S F L V G S N N T Q I P A L K P K ...  7.0  44.4     429   \n",
       "22955  M A T S S H L L P Q A L H M I P R T P S F S S ...  7.0  36.7     228   \n",
       "\n",
       "                                      tokenized_sequence  \n",
       "0      [M, K, A, L, I, V, L, G, L, V, L, L, S, V, T, ...  \n",
       "1      [M, N, Y, G, Y, F, I, E, T, G, I, P, G, D, D, ...  \n",
       "2      [M, A, F, V, A, T, Q, G, A, T, V, V, D, Q, T, ...  \n",
       "3      [M, P, P, K, T, R, K, T, K, K, E, D, V, E, E, ...  \n",
       "4      [M, Q, F, K, V, Y, T, Y, K, R, E, S, R, Y, R, ...  \n",
       "...                                                  ...  \n",
       "22951  [G, I, P, I, L, V, I, F, L, A, I, G, M, L, A, ...  \n",
       "22952  [M, M, L, R, G, E, Q, T, D, V, M, V, T, K, E, ...  \n",
       "22953  [M, E, R, Y, G, R, P, G, E, E, G, S, R, S, D, ...  \n",
       "22954  [M, A, A, T, T, N, S, F, L, V, G, S, N, N, T, ...  \n",
       "22955  [M, A, T, S, S, H, L, L, P, Q, A, L, H, M, I, ...  \n",
       "\n",
       "[22956 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag each amino acid with its index\n",
    "train_tagged = train_df.apply(lambda row: TaggedDocument(words=row['tokenized_sequence'], tags=[row.name]), axis=1)\n",
    "val_tagged = val_df.apply(lambda row: TaggedDocument(words=row['tokenized_sequence'], tags=[row.name]), axis=1)\n",
    "\n",
    "#train doc2vec model\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=100)\n",
    "model.build_vocab(train_tagged)\n",
    "\n",
    "model.train(train_tagged, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "\n",
    "#transform each sequence into a vector\n",
    "train_df['doc2vec'] = train_df['tokenized_sequence'].apply(lambda x: model.infer_vector(x))\n",
    "val_df['doc2vec'] = val_df['tokenized_sequence'].apply(lambda x: model.infer_vector(x))\n",
    "\n",
    "#convert doc2vec column into a list of vectors\n",
    "train_doc2vec = np.array(train_df['doc2vec'].tolist())\n",
    "val_doc2vec = np.array(val_df['doc2vec'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df),len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si met la transformation dans le dataframe : le kernel dies\n",
    "Si met avant, dans le panda, les dimensions sont pas les bonnes (peut être transposer ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "    def __init__(self,df):\n",
    "    \n",
    "        # the Amino acid sequences as an int array\n",
    "        sequence= df['encoded_sequence']\n",
    "        # numerical : pH and length\n",
    "        numerical = df.iloc[:,[1,3]].values\n",
    "\n",
    "        # y : the target (tm)\n",
    "        y=df.iloc[:,2].values\n",
    "\n",
    "        #creta tensors from the numpy arrays\n",
    "        self.x_sequence=torch.tensor(sequence)\n",
    "        self.y=torch.tensor(y,dtype=torch.float32)\n",
    "        self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    "   \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a faire : apres avoir fait one hot encoding, trouver comment mettre l'info de plusieurs channels dans le dataframe, sans qu'il mette d'erreur sur la taille. \n",
    "Voir comment mettre un tableau = 1 aa puis la longueur de la ligne = longueur totale (juste transposer ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class MyLoss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, classes):\n",
    "        super(MyLoss, self).__init__()\n",
    "        # define some attributes\n",
    "        self.y_true_one_hot = torch.FloatTensor(batch_size, classes, length)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        with torch.no_grad():\n",
    "            self.y_true_one_hot.zero_().scatter_(1, y_true, 1) # permet one hot encoding\n",
    "        # do some operations\n",
    "        return loss\n",
    "\n",
    "\n",
    "Or use cross entropy loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataframes\n",
    "train_d = EnzymesDataset(train_df)\n",
    "val_d = EnzymesDataset(val_df)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "            #With pooling only at the end (seen in paper)\n",
    "\n",
    "            nn.Conv1d(20, 20,kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "\n",
    "            \n",
    "            nn.Conv1d(20, 32,kernel_size=5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "         \n",
    "            nn.AdaptiveAvgPool1d(32),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(64), #argument = output size \n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=11, stride=1, padding=5), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(128),\n",
    "            \n",
    "            nn.Conv1d(128, 1, kernel_size=5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(130, 128),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "           \n",
    "           \n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        \n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "       \n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "      \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1D_OneChannel()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scoring(df_te, df_predicted):\n",
    "    df = {\n",
    "    \"true\": df_te['tm'],\n",
    "    \"predicted\": df_predicted['tm']\n",
    "}\n",
    "    pearson = df.corr(method='pearson')\n",
    "    rmse = mean_squared_error(df_te['tm'], df_predicted['tm'], squared=False)\n",
    "    auc = metrics.roc_auc_score(df_te['tm'], df_predicted['tm'])\n",
    "    \n",
    "    print('Pearson: %.3f, RMSE %.3f, AUC: %.3f' %(pearson, rmse, auc))\n",
    "    return pearson, rmse, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    train_loss = 0 \n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq,num)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.squeeze().cpu().detach().numpy())\n",
    "        rho += p\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    if epoch % 50 == 0 :\n",
    "        print(   f\"Train Epoch: {epoch} \" f\" loss={train_loss:0.2e} \" )\n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "    return train_loss , rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model = model.eval()\n",
    "    test_loss = 0\n",
    "    rho = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            test_loss += criterion(output.squeeze(), target).item()  # sum up batch loss\n",
    "            # calculate pearson correlation \n",
    "            #pearson, rmse, auc = Scoring(target.cpu().detach(), output.cpu().detach())\n",
    "            p, _ =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            rho += p\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss ,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = EnzymesDataset(df.reset_index(drop=True))\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=64, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=64, sampler=test_subsampler)\n",
    "\n",
    "    model = Conv1D_OneChannel()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'for fold {fold} : \\n train_loss :  {train_loss}     test_loss : {test_loss} \\n \\n')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    #torch.save(model.state_dict(), f\"2-Conv1d_OneHot_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-Loss-2pool.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-rho-2pool.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale data\n",
    "increase the number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "k_folds = 5\n",
    "num_epochs = 100\n",
    "\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "dataset = EnzymesDataset(df)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=64, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=64, sampler=test_subsampler)\n",
    "\n",
    "    # Init the neural network\n",
    "    network = Conv1D_OneChannel()\n",
    "    network.apply(reset_weights)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "        seq, target , num = data\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = network(seq,num)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "\n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "      for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Generate outputs\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Set total and correct\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "      # Print accuracy\n",
    "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "      print('--------------------------------')\n",
    "      results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "print(f'Fold {key}: {value} %')\n",
    "sum += value\n",
    "print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bed6b85c9b3e27cd25415a3806882bf36aa2546112033e7d0217f843875ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
