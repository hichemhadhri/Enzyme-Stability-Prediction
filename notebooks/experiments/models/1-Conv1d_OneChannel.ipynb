{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import torch \n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each amino acid in the training set\n",
    "map_dict = {} \n",
    "def count_frequency(s):\n",
    "    counter = Counter(s)\n",
    "    for p in s  : \n",
    "        if p in map_dict : \n",
    "            map_dict[p]+=counter[p]\n",
    "        else : \n",
    "            map_dict[p]= counter[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the map_dict with the frequency of each amino acid  (TODO:  better code )\n",
    "_ = train_df.apply(lambda row : count_frequency(row['protein_sequence']),axis=1)\n",
    "map_dict = dict(sorted(map_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "# Assign a number to each amino acid based on its frequency in decreasing order\n",
    "i = 1\n",
    "for key, value in map_dict.items():\n",
    "    map_dict[key]= i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map each amino acid to its number\n",
    "def chr_to_int(s):\n",
    "    l = []\n",
    "    for ch in s :\n",
    "       l.append(map_dict[ch])\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column in the dataframe with the  amino acids in numerical form (array of numbers from 1 to 20)\n",
    "train_df['numerical_sequence'] = train_df.apply(lambda row : chr_to_int(row['protein_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()\n",
    "\n",
    "max_length=np.max(train_df['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Padding with zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add 0 to numerical_sequence to make all of them the same length\n",
    "padded_train_df = train_df.copy()\n",
    "padded_train_df['numerical_sequence'] = train_df.apply(lambda row : row['numerical_sequence'] + [0]*(max_length - row['length']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataframe to Pass to the dataloader (will be put in a function later)\n",
    "padded_train_df.drop(columns=['protein_sequence'],inplace=True)\n",
    "padded_train_df['y'] = padded_train_df['tm']\n",
    "padded_train_df.drop(columns=['tm'],inplace=True)\n",
    "padded_train_df['numerical_sequence'] = padded_train_df.apply(lambda row : np.array(row['numerical_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = padded_train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN + MLP\n",
    "\n",
    "number of channels : 1 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "  def __init__(self,df):\n",
    "    \n",
    "    # the Amino acid sequences as an int array\n",
    "    sequence= df.iloc[:]['numerical_sequence']\n",
    "    # numerical : pH and length\n",
    "    numerical = df.iloc[:,[0,2]].values\n",
    "\n",
    "    # y : the target (tm)\n",
    "    y=df.iloc[:,3].values\n",
    "  \n",
    "    #create tensors from the numpy arrays\n",
    "    self.x_sequence=torch.tensor(sequence)\n",
    "    self.y=torch.tensor(y,dtype=torch.float32)\n",
    "    self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.protein_sequence = nn.Sequential(\n",
    "            nn.Conv1d(1, 1,kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "            #With pooling only at the end (seen in paper)\n",
    "\n",
    "            nn.Conv1d(1, 64,kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 32, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            #nn.AdaptiveAvgPool1d(32), #argument = output size \n",
    "            nn.Conv1d(32, 1, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "\n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(94, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#CROSS VALIDATION \n",
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = EnzymesDataset(df.reset_index(drop=True))\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=32, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "\n",
    "    model = Conv1D_OneChannel()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n",
    "    print(f'for fold {fold} : \\n train_loss :  {train_loss}     test_loss : {test_loss} \\n \\n')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train model and plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df , val_df = split_train_test(df,frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch datasets\n",
    "train_d = EnzymesDataset(train_df)\n",
    "val_d = EnzymesDataset(val_df)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1D_OneChannel()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    \n",
    "#torch.save(model.state_dict(), f\"1-Conv1d_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss plot\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "plt.savefig('Rendu1-conv1d_Loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "plt.savefig('Rendu1-conv1d_rho.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \n[Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bed6b85c9b3e27cd25415a3806882bf36aa2546112033e7d0217f843875ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
