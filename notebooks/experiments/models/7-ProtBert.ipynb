{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+ 'test.csv',index_col='seq_id')\n",
    "test_df = test_df.drop(columns=['data_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['length'] = test_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix max_length to be 500\n",
    "max_length = 500\n",
    "\n",
    "#drop rows that exceeds this value\n",
    "\n",
    "train_df = train_df[train_df['length'] < max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = df.sample(frac=0.8,random_state=24)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df),len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'clean_train_data.csv')\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['position2','pH2','data_source1','data_source2','group2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['operation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protSeq1</th>\n",
       "      <th>protSeq2</th>\n",
       "      <th>position1</th>\n",
       "      <th>change1</th>\n",
       "      <th>change2</th>\n",
       "      <th>pH1</th>\n",
       "      <th>tm1</th>\n",
       "      <th>group1</th>\n",
       "      <th>tm2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>MNDFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>6.5</td>\n",
       "      <td>62.9</td>\n",
       "      <td>5</td>\n",
       "      <td>56.2</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>MNEFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>6.5</td>\n",
       "      <td>62.9</td>\n",
       "      <td>5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>MNFFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>6.5</td>\n",
       "      <td>62.9</td>\n",
       "      <td>5</td>\n",
       "      <td>61.7</td>\n",
       "      <td>-1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>MNGFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>6.5</td>\n",
       "      <td>62.9</td>\n",
       "      <td>5</td>\n",
       "      <td>58.9</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>MNLFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>L</td>\n",
       "      <td>6.5</td>\n",
       "      <td>62.9</td>\n",
       "      <td>5</td>\n",
       "      <td>65.6</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>86</td>\n",
       "      <td>S</td>\n",
       "      <td>L</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.9</td>\n",
       "      <td>57</td>\n",
       "      <td>65.5</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>86</td>\n",
       "      <td>S</td>\n",
       "      <td>V</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.9</td>\n",
       "      <td>57</td>\n",
       "      <td>78.8</td>\n",
       "      <td>25.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>86</td>\n",
       "      <td>V</td>\n",
       "      <td>A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.8</td>\n",
       "      <td>57</td>\n",
       "      <td>71.2</td>\n",
       "      <td>-7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>86</td>\n",
       "      <td>V</td>\n",
       "      <td>L</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.8</td>\n",
       "      <td>57</td>\n",
       "      <td>65.5</td>\n",
       "      <td>-13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>86</td>\n",
       "      <td>V</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.8</td>\n",
       "      <td>57</td>\n",
       "      <td>52.9</td>\n",
       "      <td>-25.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2296 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               protSeq1  \\\n",
       "0     MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "1     MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "2     MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "3     MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "4     MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "...                                                 ...   \n",
       "2291  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...   \n",
       "2292  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...   \n",
       "2293  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...   \n",
       "2294  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...   \n",
       "2295  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...   \n",
       "\n",
       "                                               protSeq2  position1 change1  \\\n",
       "0     MNDFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...          2       A   \n",
       "1     MNEFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...          2       A   \n",
       "2     MNFFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...          2       A   \n",
       "3     MNGFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...          2       A   \n",
       "4     MNLFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...          2       A   \n",
       "...                                                 ...        ...     ...   \n",
       "2291  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...         86       S   \n",
       "2292  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...         86       S   \n",
       "2293  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...         86       V   \n",
       "2294  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...         86       V   \n",
       "2295  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...         86       V   \n",
       "\n",
       "     change2  pH1   tm1  group1   tm2  target  \n",
       "0          D  6.5  62.9       5  56.2    -6.7  \n",
       "1          E  6.5  62.9       5  59.0    -3.9  \n",
       "2          F  6.5  62.9       5  61.7    -1.2  \n",
       "3          G  6.5  62.9       5  58.9    -4.0  \n",
       "4          L  6.5  62.9       5  65.6     2.7  \n",
       "...      ...  ...   ...     ...   ...     ...  \n",
       "2291       L  8.0  52.9      57  65.5    12.6  \n",
       "2292       V  8.0  52.9      57  78.8    25.9  \n",
       "2293       A  8.0  78.8      57  71.2    -7.6  \n",
       "2294       L  8.0  78.8      57  65.5   -13.3  \n",
       "2295       S  8.0  78.8      57  52.9   -25.9  \n",
       "\n",
       "[2296 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 222/2296 [01:37<15:13,  2.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sequence_Example \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39m*\u001b[39mtrain_df[\u001b[39m'\u001b[39m\u001b[39mprotSeq1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[i]])\n\u001b[1;32m      4\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(sequence_Example, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m      6\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m embeddings_list_1\u001b[39m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1020\u001b[0m )\n\u001b[0;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:538\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    536\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 538\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    541\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    543\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    550\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 551\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    552\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:463\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_list_1 = []\n",
    "for i in tqdm(range(0, len(train_df))):\n",
    "    sequence_Example = ' '.join([*train_df['protSeq1'].iloc[i]])\n",
    "    encoded_input = tokenizer(sequence_Example, add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    output = model(**encoded_input)\n",
    "    output = output[1].detach().cpu().numpy()[0]\n",
    "    embeddings_list_1.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list_2 = []\n",
    "for i in tqdm(range(0, len(train_df))):\n",
    "    sequence_Example = ' '.join([*train_df['protSeq2'].iloc[i]])\n",
    "    encoded_input = tokenizer(sequence_Example, add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    output = model(**encoded_input)\n",
    "    output = output[1].detach().cpu().numpy()[0]\n",
    "    embeddings_list_2.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "    def __init__(self,df,train=True):\n",
    "    \n",
    "        # the Amino acid sequences as an int array\n",
    "        sequence= df['encoded_sequence']\n",
    "        # numerical : pH and length\n",
    "        numerical = df[['pH','length']].values\n",
    "\n",
    "        # y : the target (tm)\n",
    "        if train == True : \n",
    "            y=df['tm'].values\n",
    "        else : \n",
    "            y = np.zeros(len(sequence))\n",
    "        self.y=torch.tensor(y,dtype=torch.float32)\n",
    "        #creta tensors from the numpy arrays\n",
    "        self.x_sequence=torch.tensor(sequence)\n",
    "       \n",
    "        self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    "   \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a faire : apres avoir fait one hot encoding, trouver comment mettre l'info de plusieurs channels dans le dataframe, sans qu'il mette d'erreur sur la taille. \n",
    "Voir comment mettre un tableau = 1 aa puis la longueur de la ligne = longueur totale (juste transposer ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class MyLoss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, classes):\n",
    "        super(MyLoss, self).__init__()\n",
    "        # define some attributes\n",
    "        self.y_true_one_hot = torch.FloatTensor(batch_size, classes, length)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        with torch.no_grad():\n",
    "            self.y_true_one_hot.zero_().scatter_(1, y_true, 1) # permet one hot encoding\n",
    "        # do some operations\n",
    "        return loss\n",
    "\n",
    "\n",
    "Or use cross entropy loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataframes\n",
    "train_d = EnzymesDataset(df)\n",
    "val_d = EnzymesDataset(test_df,train=False)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "\n",
    "            nn.Conv1d(20, 10,kernel_size=6, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "\n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            \n",
    "         \n",
    "            torch.nn.MaxPool1d(10), \n",
    "            \n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout2d(p=0.5, inplace=False),\n",
    "            \n",
    " \n",
    "            \n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(492, 128),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "            \n",
    "           \n",
    "           \n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        \n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "       \n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "      \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1D_OneChannel()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scoring(df_te, df_predicted):\n",
    "    df = {\n",
    "    \"true\": df_te['tm'],\n",
    "    \"predicted\": df_predicted['tm']\n",
    "}\n",
    "    pearson = df.corr(method='pearson')\n",
    "    rmse = mean_squared_error(df_te['tm'], df_predicted['tm'], squared=False)\n",
    "    auc = metrics.roc_auc_score(df_te['tm'], df_predicted['tm'])\n",
    "    \n",
    "    print('Pearson: %.3f, RMSE %.3f, AUC: %.3f' %(pearson, rmse, auc))\n",
    "    return pearson, rmse, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    train_loss = 0 \n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq,num)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.squeeze().cpu().detach().numpy())\n",
    "        rho += p\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "   \n",
    "    print(   f\"Train Epoch: {epoch} \" f\" loss={train_loss:0.2e} \" )\n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "    return train_loss , rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model = model.eval()\n",
    "    test_loss = 0\n",
    "    rho = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            test_loss += criterion(output.squeeze(), target).item()  # sum up batch loss\n",
    "            # calculate pearson correlation \n",
    "            #pearson, rmse, auc = Scoring(target.cpu().detach(), output.cpu().detach())\n",
    "            p, _ =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            rho += p\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss ,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader):\n",
    "    model = model.eval()\n",
    "    df_predicted = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = EnzymesDataset(df.reset_index(drop=True))\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=32, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "\n",
    "    model = Conv1D_OneChannel()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'for fold {fold} : \\n train_loss :  {train_loss}     test_loss : {test_loss} \\n \\n')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    #torch.save(model.state_dict(), f\"2-Conv1d_OneHot_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model and preditct\n",
    "train_rho_history = []\n",
    "train_loss_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "submission_df =  predict(model,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "#plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-Loss-2pool.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "#plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-rho-2pool.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"4-Test_Model_DeepSF.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+ 'test.csv',index_col='seq_id')\n",
    "test_df['tm']=submission_df['tm'].values\n",
    "test_df = test_df.drop(columns=['protein_sequence','pH','data_source'])\n",
    "test_df.to_csv('4-Test_Model_DeepSF.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bed6b85c9b3e27cd25415a3806882bf36aa2546112033e7d0217f843875ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
