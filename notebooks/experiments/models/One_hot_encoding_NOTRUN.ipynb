{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>47.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence   pH    tm\n",
       "seq_id                                                              \n",
       "0       AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0  75.7\n",
       "1       AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0  50.5\n",
       "2       AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0  40.5\n",
       "3       AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0  47.2\n",
       "4       AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...  7.0  49.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sequences (one hot encoding)\n",
    "max_length = max(train_df['length'])\n",
    "def encode_seq(sequence):\n",
    "    alphabet = ['A', 'C', 'D', 'E', 'F', 'G','H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] # aa letters\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet)) \n",
    "    integer_encoded = [char_to_int[char] for char in sequence] #each character becomes int\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(alphabet))] #0 for all letters\n",
    "        letter[value] = 1 #modify the column corresponding to the letter to 1\n",
    "        onehot_encoded.append(letter) #put in the array (1 letter = 1 array of 20 columns)\n",
    "    \n",
    "    ar =   np.transpose(np.array(onehot_encoded))\n",
    "    \n",
    "    zeros = np.zeros([len(alphabet),max_length   - len(integer_encoded)] )\n",
    "\n",
    "    onehot_encoded = np.concatenate((ar, zeros), axis = 1) #zero padding\n",
    "\n",
    "    \n",
    "\n",
    "    return onehot_encoded #we have all arrays, corresponding to the whole sequence\n",
    "\n",
    "\n",
    "# new column with encoded sequence (apply for each sequence)\n",
    "train_df['encoded_sequence'] = train_df['protein_sequence'].apply(lambda x: encode_seq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28695,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['encoded_sequence'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final dataframe \n",
    "train_df.iloc[0]['encoded_sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8798"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(train_df['length'])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding inverted sequences (DISCRADED) \n",
    "**Add it to report as an experiment ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "inverted_df = train_df.copy()\n",
    "\n",
    "#inver the protein sequence\n",
    "inverted_df['protein_sequence'] = inverted_df.apply(lambda row : row['protein_sequence'][::-1],axis=1)\n",
    "inverted_df['numerical_sequence'] = inverted_df.apply(lambda row : chr_to_int(row['protein_sequence']),axis=1)\n",
    "\n",
    "# merge the original and inverted dataframes (size of dataset is doubled)\n",
    "train_df = pd.concat([train_df,inverted_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Padding with zeros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add 0 to numerical_sequence to make all of them the same length\n",
    "padded_train_df = train_df.copy()\n",
    "padded_train_df['encoded_sequence'] = train_df.apply(lambda row : row['encoded_sequence'][:] + [0]*(max_length - row['length']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prepare dataframe to Pass to the dataloader (will be put in a function later)\n",
    "padded_train_df.drop(columns=['protein_sequence'],inplace=True)\n",
    "padded_train_df['y'] = padded_train_df['tm']\n",
    "padded_train_df.drop(columns=['tm'],inplace=True)\n",
    "padded_train_df['numerical_sequence'] = padded_train_df.apply(lambda row : np.array(row['numerical_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = df.sample(frac=0.8,random_state=200)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MPQESSDAKRIEPPRKVDFMLKPRFTNTVPDVPFDAKFMTCPFVPL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>34.3</td>\n",
       "      <td>424</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIKNIVISFEEQKEESRGSVEFQVFSFTNKIRRLTSHLELHRKDYL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MASYPGPGKSKAKYPFKKRAGLQASAAAPEARSGLGASPLQSARSL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>55.6</td>\n",
       "      <td>823</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSRGSRFVRAPGLLLCRVNLQPQPKIPSFSYSLRSDYRLHNGFSNY...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>727</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MFWAKEEEENSLMRLLKTDNFTLEDVLLNEFVVQESRYGKAELVQY...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>716</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22951</th>\n",
       "      <td>MRGQVGDLSPQQQEALARFRETLQDLLPTLPKADDYFLLRWLRARN...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>402</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22952</th>\n",
       "      <td>MALKRLTRQRKAILEVVRQARHHPDAAWIYQEVRKRVPKVSLGTIY...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>130</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22953</th>\n",
       "      <td>MGEQQLDCALDLMRRLPPQHCDKNLTDLIDLCPHLVDDLLSTIDQP...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.9</td>\n",
       "      <td>269</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22954</th>\n",
       "      <td>MSEKLPFKVADISLAEWGRKAIEIAENEMPGLMKMRELYGQSKPLK...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>432</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22955</th>\n",
       "      <td>MNWQTKDELLALLTSLVQYESITGSKGEVALAEYLYFILKDKPYFQ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.8</td>\n",
       "      <td>546</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22956 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        protein_sequence   pH    tm  length  \\\n",
       "0      MPQESSDAKRIEPPRKVDFMLKPRFTNTVPDVPFDAKFMTCPFVPL...  7.0  34.3     424   \n",
       "1      MIKNIVISFEEQKEESRGSVEFQVFSFTNKIRRLTSHLELHRKDYL...  7.0  35.5      87   \n",
       "2      MASYPGPGKSKAKYPFKKRAGLQASAAAPEARSGLGASPLQSARSL...  7.0  55.6     823   \n",
       "3      MSRGSRFVRAPGLLLCRVNLQPQPKIPSFSYSLRSDYRLHNGFSNY...  7.0  43.3     727   \n",
       "4      MFWAKEEEENSLMRLLKTDNFTLEDVLLNEFVVQESRYGKAELVQY...  7.0  43.5     716   \n",
       "...                                                  ...  ...   ...     ...   \n",
       "22951  MRGQVGDLSPQQQEALARFRETLQDLLPTLPKADDYFLLRWLRARN...  7.0  48.0     402   \n",
       "22952  MALKRLTRQRKAILEVVRQARHHPDAAWIYQEVRKRVPKVSLGTIY...  7.0  80.0     130   \n",
       "22953  MGEQQLDCALDLMRRLPPQHCDKNLTDLIDLCPHLVDDLLSTIDQP...  7.0  46.9     269   \n",
       "22954  MSEKLPFKVADISLAEWGRKAIEIAENEMPGLMKMRELYGQSKPLK...  7.0  50.6     432   \n",
       "22955  MNWQTKDELLALLTSLVQYESITGSKGEVALAEYLYFILKDKPYFQ...  7.0  57.8     546   \n",
       "\n",
       "                                        encoded_sequence  \n",
       "0      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4      [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                  ...  \n",
       "22951  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "22952  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "22953  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "22954  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "22955  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[22956 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si met la transformation dans le dataframe : le kernel dies\n",
    "Si met avant, dans le panda, les dimensions sont pas les bonnes (peut être transposer ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15623</th>\n",
       "      <td>MATVTNFKLVTPPESSRADKPGATKASDAFQEKKSVSVNYDRGEHE...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>821</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21468</th>\n",
       "      <td>MFVPRSLKIKRSSNDDLKSGEAKKSKPEAGGLQVEGDRDTPVHTSV...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.8</td>\n",
       "      <td>618</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21105</th>\n",
       "      <td>MANFDLTRINCQFLDRHLTFPLLEFLCGKEIYNQQELLEYILETVN...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>433</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>FSGRIRKKLRLAGDQRNASYPHSLQFYLQPPTENISLTEFENLAFD...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.6</td>\n",
       "      <td>502</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>MTAINRILIVDDEDNVRRMLSTAFALQGFETHCANNGRTALHLFAD...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>460</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        protein_sequence   pH    tm  length  \\\n",
       "15623  MATVTNFKLVTPPESSRADKPGATKASDAFQEKKSVSVNYDRGEHE...  7.0  59.9     821   \n",
       "21468  MFVPRSLKIKRSSNDDLKSGEAKKSKPEAGGLQVEGDRDTPVHTSV...  7.0  49.8     618   \n",
       "21105  MANFDLTRINCQFLDRHLTFPLLEFLCGKEIYNQQELLEYILETVN...  7.0  39.5     433   \n",
       "11852  FSGRIRKKLRLAGDQRNASYPHSLQFYLQPPTENISLTEFENLAFD...  7.0  48.6     502   \n",
       "3624   MTAINRILIVDDEDNVRRMLSTAFALQGFETHCANNGRTALHLFAD...  7.0  50.6     460   \n",
       "\n",
       "                                        encoded_sequence  \n",
       "15623  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "21468  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "21105  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "11852  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3624   [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "  def __init__(self,df):\n",
    "    \n",
    "    # the Amino acid sequences as an int array\n",
    "    sequence= df['encoded_sequence']\n",
    "    # numerical : pH and length\n",
    "    numerical = df.iloc[:,[1,3]].values\n",
    "    print(numerical.shape)\n",
    "    # y : the target (tm)\n",
    "    y=df.iloc[:,2].values\n",
    "    print(sequence.shape)\n",
    "    print(y.shape)\n",
    "    #creta tensors from the numpy arrays\n",
    "    self.x_sequence=torch.tensor(sequence)\n",
    "    self.y=torch.tensor(y,dtype=torch.float32)\n",
    "    self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    "   \n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7., 424.],\n",
       "       [  7.,  87.],\n",
       "       [  7., 823.],\n",
       "       ...,\n",
       "       [  7., 269.],\n",
       "       [  7., 432.],\n",
       "       [  7., 546.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[:,[1,3]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a faire : apres avoir fait one hot encoding, trouver comment mettre l'info de plusieurs channels dans le dataframe, sans qu'il mette d'erreur sur la taille. \n",
    "Voir comment mettre un tableau = 1 aa puis la longueur de la ligne = longueur totale (juste transposer ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class MyLoss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, classes):\n",
    "        super(MyLoss, self).__init__()\n",
    "        # define some attributes\n",
    "        self.y_true_one_hot = torch.FloatTensor(batch_size, classes, length)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        with torch.no_grad():\n",
    "            self.y_true_one_hot.zero_().scatter_(1, y_true, 1) # permet one hot encoding\n",
    "        # do some operations\n",
    "        return loss\n",
    "\n",
    "\n",
    "Or use cross entropy loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>265</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.5</td>\n",
       "      <td>1451</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.4</td>\n",
       "      <td>380</td>\n",
       "      <td>[[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AALANSDISVALIEPNTAQQPPLGKVSCNEFDTRVSAITAQSEALL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>380</td>\n",
       "      <td>[[1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AALPQDEKLITGQLDNGLRYMIYPHAHPKDQVNLWLQIHTGSLQEE...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.7</td>\n",
       "      <td>904</td>\n",
       "      <td>[[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>YSNVEKGAAARARCWTSGNGRPAQWWQEGDQVTRGKYFYECRRGQL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>39.9</td>\n",
       "      <td>349</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>YTIKEDESFLQQPHYASQEQLEDLFAGLEKAYPNQAKVHFLGRSLE...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1380</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>YVAFKGLLEADLDRSLLFYARALAEGRPSPRGEFAFRLAQGGAETK...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>378</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>YWFPAEEMRTRNNVNNCFKKPAFANLLRFPQLYPFLCRADFIKVAA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.6</td>\n",
       "      <td>300</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738</th>\n",
       "      <td>YYLWHKAASTVASIHESIDKSKKRDKEVSINKKDPFSVLIMGVDER...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>274</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5739 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       protein_sequence   pH    tm  length  \\\n",
       "0     AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0  47.2     265   \n",
       "1     AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...  7.0  49.5    1451   \n",
       "2     AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...  7.0  48.4     380   \n",
       "3     AALANSDISVALIEPNTAQQPPLGKVSCNEFDTRVSAITAQSEALL...  7.0  43.3     380   \n",
       "4     AALPQDEKLITGQLDNGLRYMIYPHAHPKDQVNLWLQIHTGSLQEE...  7.0  45.7     904   \n",
       "...                                                 ...  ...   ...     ...   \n",
       "5734  YSNVEKGAAARARCWTSGNGRPAQWWQEGDQVTRGKYFYECRRGQL...  7.0  39.9     349   \n",
       "5735  YTIKEDESFLQQPHYASQEQLEDLFAGLEKAYPNQAKVHFLGRSLE...  7.0  42.9    1380   \n",
       "5736  YVAFKGLLEADLDRSLLFYARALAEGRPSPRGEFAFRLAQGGAETK...  7.0  86.0     378   \n",
       "5737  YWFPAEEMRTRNNVNNCFKKPAFANLLRFPQLYPFLCRADFIKVAA...  7.0  48.6     300   \n",
       "5738  YYLWHKAASTVASIHESIDKSKKRDKEVSINKKDPFSVLIMGVDER...  7.0  42.1     274   \n",
       "\n",
       "                                       encoded_sequence  \n",
       "0     [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "1     [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2     [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3     [[1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4     [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                 ...  \n",
       "5734  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0,...  \n",
       "5735  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "5736  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "5737  [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "5738  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...  \n",
       "\n",
       "[5739 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22956, 2)\n",
      "(22956,)\n",
      "(22956,)\n",
      "(5739, 2)\n",
      "(5739,)\n",
      "(5739,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/t_7t2xyj4n760sxfr2jkq6z40000gn/T/ipykernel_2998/3031152663.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1666646703877/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  self.x_sequence=torch.tensor(sequence)\n"
     ]
    }
   ],
   "source": [
    "# create pytorch dataframes\n",
    "train_d = EnzymesDataset(train_df)\n",
    "val_d = EnzymesDataset(val_df)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "            #With pooling only at the end (seen in paper)\n",
    "\n",
    "            nn.Conv1d(20, 64,kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 32, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.AdaptiveAvgPool1d(32), #argument = output size \n",
    "            nn.Conv1d(32, 1, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "\n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(94, 64),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1D_OneChannel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    optimizer = optimizer.cuda()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq,num)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "        rho += p\n",
    "        \n",
    "        print(\n",
    "            f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "            f\"batch_loss={loss.item():0.2e} \"\n",
    "            \n",
    "        )\n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "    return loss.item() , rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "       \n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            test_loss += criterion(output.squeeze(), target).item()  # sum up batch loss\n",
    "           # calculate pearson correlation \n",
    "            p, _ =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            rho += p\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss ,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 8798])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 21, 8], expected input[128, 20, 8798] to have 21 channels, but got 20 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m test_rho_history \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     train_loss , rho_train \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m      8\u001b[0m         model, optimizer, criterion, train_dl, epoch\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     train_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     11\u001b[0m     train_rho_history\u001b[39m.\u001b[39mappend(rho_train)\n",
      "Cell \u001b[0;32mIn [36], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, criterion, train_loader, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m     num \u001b[39m=\u001b[39m num\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m model(seq,num)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39msqueeze(), target)\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [33], line 51\u001b[0m, in \u001b[0;36mConv1D_OneChannel.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x,y):\n\u001b[0;32m---> 51\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprot_seq_one_pooling(x\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     52\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical(y)\n\u001b[1;32m     54\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), y), \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 21, 8], expected input[128, 20, 8798] to have 21 channels, but got 20 channels instead"
     ]
    }
   ],
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_df)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"2-conv1d_OneHotEncoding_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "plt.savefig('plots/2-conv1d_OneHotEncoding-Loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "plt.savefig('plots/2-conv1d_OneHotEncoding-rho.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
