{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model :ProtBert  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Rostlab/prot_bert'\n",
    "SAVE_PATH = 'ProtConfig/'\n",
    "\n",
    "MAX_LEN = 512 # protein sequence max length \n",
    "BATCH_SIZE = 6\n",
    "VER= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset  lean_train_data.csv (from kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### put in function ####\n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "\n",
    "train = pd.read_csv(path+'train_v1.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "submission = pd.read_csv(path+'sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add spaces betwwen Amnino acids letter to tokenize\n",
    "def add_spaces(x):\n",
    "    return \" \".join(list(x))\n",
    "\n",
    "\n",
    "train.protein_sequence = train.protein_sequence.map(add_spaces)\n",
    "test.protein_sequence = test.protein_sequence.map(add_spaces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index (to pass to dataset)\n",
    "train = train.reset_index(drop=True)\n",
    "test  = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained Protbert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(tokenizer, text):\n",
    "    # tokenize text (add special tokens and pad/truncate to max length)\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class MutationDataset(Dataset):\n",
    "    def __init__(self,tokenizer, df):\n",
    "        self.tokenizer = tokenizer\n",
    "       \n",
    "        self.inputs1 = df['protein_sequence'].values\n",
    "    \n",
    "        self.pH = df['pH'].values\n",
    "        self.labels = df['tm'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        #tokenize input texts\n",
    "        inputs1 = prepare_input(self.tokenizer, self.inputs1[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        pH = torch.tensor(self.pH[item],dtype = torch.float)\n",
    "        \n",
    "        return inputs1, label , pH\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        # expandig the attention mask to match the shape of the hidden states\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "\n",
    "        #averaging the embeddings\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class ProtBertStab(nn.Module):\n",
    "    def __init__(self, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "       \n",
    "        # for model loading\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
    "        else:\n",
    "            # for model inference\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            # load pretrained model\n",
    "            self.model = AutoModel.from_pretrained(MODEL, config=self.config)\n",
    "        else:\n",
    "            # load model from config\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "      \n",
    "\n",
    "\n",
    "        self.pool = MeanPooling() # for mean pooling\n",
    "\n",
    "        # modify last layer \n",
    "        self.lin = nn.Linear(self.config.hidden_size,1)\n",
    "\n",
    "      \n",
    "         \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def feature(self, inputs,position):\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        \n",
    "        feature = self.pool(last_hidden_states, position)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs1):\n",
    "        #get embedding from model \n",
    "        output = self.lin(self.feature(inputs1,inputs1['attention_mask']))\n",
    "      \n",
    "       \n",
    "        #concatenate all the features with the difference between each two features (we study the difference in melting point)\n",
    "       \n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "  \n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to helper\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    train_loss = 0 \n",
    "   \n",
    "    for batch_idx, (inputs1, pH) in enumerate(train_loader):\n",
    "        # inputs to device\n",
    "        for k, v in inputs1.items():\n",
    "            inputs1[k] = v.to(device)     \n",
    "      \n",
    "      \n",
    "        target = target.to(device)\n",
    "        pH = pH.to(device)\n",
    "\n",
    "        #batch_size = target.size(0)\n",
    "\n",
    "        \n",
    "        output = model(inputs1,pH)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.squeeze().cpu().detach().numpy())\n",
    "        rho += p\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(inputs1)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f} \\t spearman: {p:.6f}\")\n",
    "\n",
    "    \n",
    "    train_loss /= len(train_loader)  \n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "\n",
    "    print(   f\"Train Epoch: {epoch} \" f\" loss={train_loss:0.2e} \" f\" rho={rho:0.2f} \" )\n",
    "    return train_loss , rho\n",
    "\n",
    "\n",
    "def test_epoch(model, criterion, test_loader):\n",
    "    model = model.eval()\n",
    "    test_loss = 0\n",
    "    rho = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs1, target , pH) in enumerate(test_loader):\n",
    "            # inputs to device\n",
    "           \n",
    "            position = position.to(device)\n",
    "            target = target.to(device)\n",
    "            pH = pH.to(device)\n",
    "          \n",
    "            # predict\n",
    "            output = model(inputs1,pH)\n",
    "            test_loss += criterion(output, target).item()\n",
    "          \n",
    "            p =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy()).correlation\n",
    "            rho += p\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Test Epoch: [{batch_idx * len(inputs1)}/{len(test_loader.dataset)} ({100. * batch_idx / len(test_loader):.0f}%)]\\tLoss: {test_loss:.6f} \\t spearman: {p:.6f}\")\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(   f\"Test Epoch: \" f\" loss={test_loss:0.2e} \" f\" rho={rho:0.2f} \" )\n",
    "\n",
    "    return test_loss ,rho\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = MutationDataset(train)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=BATCH_SIZE, sampler=test_subsampler)\n",
    "\n",
    "    model = ProtBertStab(pretrained=True)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = RMSELoss()\n",
    " \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "       \n",
    "\n",
    "        test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_rho_history.append(rho_train)\n",
    "        test_loss_history.append(test_loss)\n",
    "        \n",
    "        test_rho_history.append(rho_test)\n",
    "\n",
    "    break  # for debug purposes\n",
    "    \n",
    "    \n",
    "torch.save(model, f'ProtConfig/fold-{fold}_{VER}.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the train and test rho\n",
    "plt.plot(train_rho_history, label='train rho')\n",
    "plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' train and test rho')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bed6b85c9b3e27cd25415a3806882bf36aa2546112033e7d0217f843875ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
