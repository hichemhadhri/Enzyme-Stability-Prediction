{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>47.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence   pH    tm\n",
       "seq_id                                                              \n",
       "0       AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0  75.7\n",
       "1       AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0  50.5\n",
       "2       AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0  40.5\n",
       "3       AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0  47.2\n",
       "4       AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...  7.0  49.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load training data (will be put in a function later)  \n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+ 'test.csv',index_col='seq_id')\n",
    "test_df = test_df.drop(columns=['data_source'])\n",
    "test_df['length'] = test_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix max_length to be 500\n",
    "max_length = 500\n",
    "\n",
    "#drop rows that exceeds this value\n",
    "\n",
    "train_df = train_df[train_df['length'] < max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_seq(sequence):\n",
    "    alphabet = ['A', 'C', 'D', 'E', 'F', 'G','H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] # aa letters\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet)) \n",
    "    integer_encoded = [char_to_int[char] for char in sequence] #each character becomes int\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(alphabet))] #0 for all letters\n",
    "        letter[value] = 1 #modify the column corresponding to the letter to 1\n",
    "        onehot_encoded.append(letter) #put in the array (1 letter = 1 array of 20 columns)\n",
    "    \n",
    "    ar =   np.transpose(np.array(onehot_encoded))\n",
    "    zeros = np.zeros([len(alphabet),max_length - len(integer_encoded)] )\n",
    "    onehot_encoded = np.concatenate((ar, zeros), axis = 1) #zero padding\n",
    "\n",
    "\n",
    "    return onehot_encoded #we have all arrays, corresponding to the whole sequence\n",
    "\n",
    "\n",
    "# new column with encoded sequence (apply for each sequence)\n",
    "train_df['encoded_sequence'] = train_df['protein_sequence'].apply(lambda x: encode_seq(x))\n",
    "test_df['encoded_sequence'] = test_df['protein_sequence'].apply(lambda x: encode_seq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20510,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['encoded_sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75.7</td>\n",
       "      <td>341</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>286</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>497</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>265</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.4</td>\n",
       "      <td>380</td>\n",
       "      <td>[[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence   pH    tm  length  \\\n",
       "seq_id                                                                         \n",
       "0       AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0  75.7     341   \n",
       "1       AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0  50.5     286   \n",
       "2       AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0  40.5     497   \n",
       "3       AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0  47.2     265   \n",
       "5       AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...  7.0  48.4     380   \n",
       "\n",
       "                                         encoded_sequence  \n",
       "seq_id                                                     \n",
       "0       [[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0,...  \n",
       "1       [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2       [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "3       [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "5       [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final dataframe \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = df.sample(frac=0.8,random_state=24)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16408 4102\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df),len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si met la transformation dans le dataframe : le kernel dies\n",
    "Si met avant, dans le panda, les dimensions sont pas les bonnes (peut être transposer ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>length</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75.7</td>\n",
       "      <td>341</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>286</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>497</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>265</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.4</td>\n",
       "      <td>380</td>\n",
       "      <td>[[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20505</th>\n",
       "      <td>YWFPAEEMRTRNNVNNCFKKPAFANLLRFPQLYPFLCRADFIKVAA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.6</td>\n",
       "      <td>300</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20506</th>\n",
       "      <td>YYAYVVELCVSTISRTGEKGKTVVYLVAFHLFFVMFVWSYWMTIFT...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>350</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20507</th>\n",
       "      <td>YYLWHKAASTVASIHESIDKSKKRDKEVSINKKDPFSVLIMGVDER...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>274</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>YYNDQHRLSSYSVETAMFLSWERAIVKPGAMFKKAVIGFNCNVDLI...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>469</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20509</th>\n",
       "      <td>YYQRTLGAELLYKISFGEMPKSAQDSAENCPSGMQFPDTAIAHANV...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.6</td>\n",
       "      <td>128</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20510 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        protein_sequence   pH    tm  length  \\\n",
       "0      AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0  75.7     341   \n",
       "1      AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0  50.5     286   \n",
       "2      AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0  40.5     497   \n",
       "3      AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0  47.2     265   \n",
       "4      AACFWRRTVIPKPPFRGISTTSARSTVMPAWVIDKYGKNEVLRFTQ...  7.0  48.4     380   \n",
       "...                                                  ...  ...   ...     ...   \n",
       "20505  YWFPAEEMRTRNNVNNCFKKPAFANLLRFPQLYPFLCRADFIKVAA...  7.0  48.6     300   \n",
       "20506  YYAYVVELCVSTISRTGEKGKTVVYLVAFHLFFVMFVWSYWMTIFT...  7.0  49.4     350   \n",
       "20507  YYLWHKAASTVASIHESIDKSKKRDKEVSINKKDPFSVLIMGVDER...  7.0  42.1     274   \n",
       "20508  YYNDQHRLSSYSVETAMFLSWERAIVKPGAMFKKAVIGFNCNVDLI...  7.0  37.2     469   \n",
       "20509  YYQRTLGAELLYKISFGEMPKSAQDSAENCPSGMQFPDTAIAHANV...  7.0  64.6     128   \n",
       "\n",
       "                                        encoded_sequence  \n",
       "0      [[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0,...  \n",
       "1      [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2      [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "3      [[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "4      [[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                  ...  \n",
       "20505  [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "20506  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "20507  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...  \n",
       "20508  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "20509  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "\n",
       "[20510 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "    def __init__(self,df,train=True):\n",
    "    \n",
    "        # the Amino acid sequences as an int array\n",
    "        sequence= df['encoded_sequence']\n",
    "        # numerical : pH and length\n",
    "        numerical = df[['pH','length']].values\n",
    "\n",
    "        # y : the target (tm)\n",
    "        if train == True : \n",
    "            y=df['tm'].values\n",
    "        else : \n",
    "            y = np.zeros(len(sequence))\n",
    "        self.y=torch.tensor(y,dtype=torch.float32)\n",
    "        #creta tensors from the numpy arrays\n",
    "        self.x_sequence=torch.tensor(sequence)\n",
    "       \n",
    "        self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    "   \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_831317/545355855.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  self.x_sequence=torch.tensor(sequence)\n"
     ]
    }
   ],
   "source": [
    "# create pytorch dataframes\n",
    "train_d = EnzymesDataset(df)\n",
    "val_d = EnzymesDataset(test_df,train=False)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader):\n",
    "    model = model.eval()\n",
    "    df_predicted = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_fc1):\n",
    "        \"\"\"Parameters:\n",
    "            - trial (optuna.trial._trial.Trial): Optuna trial\n",
    "            - num_conv_layers (int):             Number of convolutional layers\n",
    "            - num_filters (list):                Number of filters of conv layers\n",
    "            - num_neurons (int):                 Number of neurons of FC layers\n",
    "            - drop_conv2 (float):                Dropout ratio for conv layer 2\n",
    "            - drop_fc1 (float):                  Dropout ratio for FC1\n",
    "        \"\"\"\n",
    "        super(Conv1D_OneChannel, self).__init__()                                                     # Initialize parent class\n",
    "        in_size = 20                                                                    # Input image size (28 pixels)\n",
    "        kernel_size = 3                                                                 # Convolution filter size\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_size, num_filters[0], kernel_size=(3, 3))])  # List with the Conv layers\n",
    "        out_size = in_size - kernel_size + 1                                            # Size of the output kernel\n",
    "        out_size = int(out_size / 2)                                                    # Size after pooling\n",
    "        for i in range(1, num_conv_layers):\n",
    "            self.convs.append(nn.Conv1d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=(3, 3)))\n",
    "            self.convs.append(nn.ReLU())\n",
    "            self.convs.append(nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2))\n",
    "            self.convs.append(nn.MaxPool1d(kernel_size=3, stride=2))\n",
    "            \n",
    "            out_size = out_size - kernel_size + 1                                       # Size of the output kernel\n",
    "            out_size = int(out_size/2)                                                  # Size after pooling\n",
    "        self.convs.append(nn.Dropout(p=drop_conv2))\n",
    "        self.convs.append(nn.Flatten())\n",
    "        \n",
    "        self.out_feature = num_filters[num_conv_layers-1] * out_size * out_size         # Size of flattened features\n",
    "           \n",
    "        \n",
    "        \n",
    "        \"\"\"self.conv2_drop = nn.Dropout(p=drop_conv2)                                    # Dropout for conv2\n",
    "        self.out_feature = num_filters[num_conv_layers-1] * out_size * out_size         # Size of flattened features\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons)                             # Fully Connected layer 1\n",
    "        self.fc2 = nn.Linear(num_neurons, 10)                                           # Fully Connected layer 2\n",
    "        self.p1 = drop_fc1  \"\"\"\n",
    "        \n",
    "        \n",
    "        # Initialize weights with the He initialization\n",
    "        \"\"\"for i in range(1, num_conv_layers):\n",
    "            nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity='relu')\n",
    "            if self.convs[i].bias is not None:\n",
    "                nn.init.constant_(self.convs[i].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 + self.out_feature, 128),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward (self,x,y):\n",
    "        \n",
    "        \"\"\"for i, conv_i in enumerate(self.convs):  # For each convolutional layer\n",
    "            if i == 2:  # Add dropout if layer 2\n",
    "                x = F.relu(F.max_pool1d(self.conv2_drop(conv_i(x)), 2))  # Conv_i, dropout, max-pooling, RelU\n",
    "            else:\n",
    "                x = F.relu(F.max_pool1d(conv_i(x), 2))                   # Conv_i, max-pooling, RelU\n",
    "\n",
    "        x = x.view(-1, self.out_feature)                     # Flatten tensor\n",
    "        x = F.relu(self.fc1(x))                              # FC1, RelU\n",
    "        x = F.dropout(x, p=self.p1, training=self.training)  # Apply dropout after FC1 only when training\n",
    "        x = self.fc2(x)\"\"\"\n",
    "        \n",
    "        for i, conv_i in enumerate(self.convs):\n",
    "            x = conv_i(x)\n",
    "        #x = self.convs(x)\n",
    "        #x = self.conv2_drop(x)\n",
    "        #x = self.flat(x)\n",
    "\n",
    "        y = self.numerical(y)\n",
    "\n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer):\n",
    "    \"\"\"Trains the model.\n",
    "    Parameters:\n",
    "        - network (__main__.Net):              The CNN\n",
    "        - optimizer (torch.optim.<optimizer>): The optimizer for the CNN\n",
    "    \"\"\"\n",
    "    network.train()  # Set the module in training mode (only affects certain modules)\n",
    "    for batch_i, (seq, target,num) in enumerate(train_dl):  # For each batch\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "\n",
    "        # Limit training data for faster computation\n",
    "        if batch_i * batch_size_train > number_of_train_examples:\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()                                 # Clear gradients\n",
    "        output = network(seq.unsqueeze(1),num)                     # Forward propagation\n",
    "        loss = F.nll_loss(output.squeeze(), target.to(device))          # Compute loss (negative log likelihood: −log(y))\n",
    "        loss.backward()                                       # Compute gradients\n",
    "        optimizer.step()                                      # Update weights\n",
    "\n",
    "\n",
    "def test(network):\n",
    "    \"\"\"Tests the model.\n",
    "    Parameters:\n",
    "        - network (__main__.Net): The CNN\n",
    "    Returns:\n",
    "        - accuracy_test (torch.Tensor): The test accuracy\n",
    "    \"\"\"\n",
    "    network.eval()         # Set the module in evaluation mode (only affects certain modules)\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation (when you are sure that you will not call Tensor.backward())\n",
    "        for batch_i, (data, target,num) in enumerate(test_dl):  # For each batch\n",
    "\n",
    "            # Limit testing data for faster computation\n",
    "            if batch_i * batch_size_test > number_of_test_examples:\n",
    "                break\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "\n",
    "            output = network(seq, num)               # Forward propagation\n",
    "            pred = output.data.max(1, keepdim=True)[1]      # Find max value in each row, return indexes of max values\n",
    "            correct += pred.eq(target.to(device).data.view_as(pred)).sum()  # Compute correct predictions\n",
    "\n",
    "    accuracy_test = correct / len(test_loader.dataset)\n",
    "\n",
    "    return accuracy_test\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function to be optimized by Optuna.\n",
    "    Hyperparameters chosen to be optimized: optimizer, learning rate,\n",
    "    dropout values, number of convolutional layers, number of filters of\n",
    "    convolutional layers, number of neurons of fully connected layers.\n",
    "    Inputs:\n",
    "        - trial (optuna.trial._trial.Trial): Optuna trial\n",
    "    Returns:\n",
    "        - accuracy(torch.Tensor): The test accuracy. Parameter to be maximized.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 3)  # Number of convolutional layers\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16)) for i in range(num_conv_layers)]              # Number of filters for the convolutional layers\n",
    "    num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of FC1 layer\n",
    "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)     # Dropout for convolutional layer 2\n",
    "    drop_fc1 = trial.suggest_float(\"drop_fc1\", 0.2, 0.5)         # Dropout for FC1 layer\n",
    "\n",
    "    # Generate the model\n",
    "    model = Conv1D_OneChannel(trial, num_conv_layers, num_filters, num_neurons, drop_conv2,  drop_fc1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate the optimizers\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])  # Optimizers\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)                                 # Learning rates\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training of the model\n",
    "    for epoch in range(n_epochs):\n",
    "        train(model, optimizer)  # Train the model\n",
    "        accuracy = test(model)   # Evaluate the model\n",
    "\n",
    "        # For pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        #if trial.should_prune():\n",
    "            #raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-11 14:03:56,491]\u001b[0m A new study created in memory with name: no-name-523b37b0-934f-41cc-8e5f-7c8f173fdcad\u001b[0m\n",
      "/tmp/ipykernel_831317/2566750686.py:69: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16)) for i in range(num_conv_layers)]              # Number of filters for the convolutional layers\n",
      "\u001b[33m[W 2022-12-11 14:03:56,517]\u001b[0m Trial 0 failed because of the following error: RuntimeError('Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [64, 1, 20, 500]')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ml4science/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_831317/2566750686.py\", line 85, in objective\n",
      "    train(model, optimizer)  # Train the model\n",
      "  File \"/tmp/ipykernel_831317/2566750686.py\", line 20, in train\n",
      "    output = network(seq.unsqueeze(1),num)                     # Forward propagation\n",
      "  File \"/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_831317/3324008054.py\", line 88, in forward\n",
      "    x = conv_i(x)\n",
      "  File \"/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 313, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 309, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "RuntimeError: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [64, 1, 20, 500]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [64, 1, 20, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_831317/3734785634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Create an Optuna study to maximize test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# -------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_831317/2566750686.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Training of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_831317/2566750686.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# Compute loss (negative log likelihood: −log(y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                       \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_831317/3324008054.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m#x = self.convs(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#x = self.conv2_drop(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [64, 1, 20, 500]"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Use cuda if available for faster computations\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Parameters ----------------------------------------------------------\n",
    "    n_epochs = 10                         # Number of training epochs\n",
    "    batch_size_train = 64                 # Batch size for training data\n",
    "    batch_size_test = 1000                # Batch size for testing data\n",
    "    number_of_trials = 100                # Number of Optuna trials\n",
    "    limit_obs = True                      # Limit number of observations for faster computation\n",
    "\n",
    "    # *** Note: For more accurate results, do not limit the observations.\n",
    "    #           If not limited, however, it might take a very long time to run.\n",
    "    #           Another option is to limit the number of epochs. ***\n",
    "\n",
    "    if limit_obs:  # Limit number of observations\n",
    "        number_of_train_examples = 500 * batch_size_train  # Max train observations\n",
    "        number_of_test_examples = 5 * batch_size_test      # Max test observations\n",
    "    else:\n",
    "        number_of_train_examples = 60000                   # Max train observations\n",
    "        number_of_test_examples = 10000                    # Max test observations\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Make runs repeatable\n",
    "    random_seed = 1\n",
    "    torch.backends.cudnn.enabled = False  # Disable cuDNN use of nondeterministic algorithms\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Create directory 'files', if it doesn't exist, to save the dataset\n",
    "    directory_name = 'files_optimization'\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.mkdir(directory_name)\n",
    "\n",
    "    # Download MNIST dataset to 'files' directory and normalize it\n",
    "    train_loader = train_dl\n",
    "\n",
    "    test_loader = val_dl\n",
    "\n",
    "    # Create an Optuna study to maximize test accuracy\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=number_of_trials)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Results\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Find number of pruned and completed trials\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    # Display the study statistics\n",
    "    print(\"\\nStudy statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    trial = study.best_trial\n",
    "    print(\"Best trial:\")\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    # Save results to csv file\n",
    "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
    "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
    "    df = df.drop('state', axis=1)                 # Exclude state column\n",
    "    df = df.sort_values('value')                  # Sort based on accuracy\n",
    "    df.to_csv('optuna_results.csv', index=False)  # Save to csv file\n",
    "\n",
    "    # Display results in a dataframe\n",
    "    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n",
    "\n",
    "    # Find the most important hyperparameters\n",
    "    most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "    # Display the most important hyperparameters\n",
    "    print('\\nMost important hyperparameters:')\n",
    "    for key, value in most_important_parameters.items():\n",
    "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 6 required positional arguments: 'trial', 'num_conv_layers', 'num_filters', 'num_neurons', 'drop_conv2', and 'drop_fc1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_831317/3360441915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D_OneChannel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# defining the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# checking if GPU is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 6 required positional arguments: 'trial', 'num_conv_layers', 'num_filters', 'num_neurons', 'drop_conv2', and 'drop_fc1'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Conv1D_OneChannel()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scoring(df_te, df_predicted):\n",
    "    df = {\n",
    "    \"true\": df_te['tm'],\n",
    "    \"predicted\": df_predicted['tm']\n",
    "}\n",
    "    pearson = df.corr(method='pearson')\n",
    "    rmse = mean_squared_error(df_te['tm'], df_predicted['tm'], squared=False)\n",
    "    auc = metrics.roc_auc_score(df_te['tm'], df_predicted['tm'])\n",
    "    \n",
    "    print('Pearson: %.3f, RMSE %.3f, AUC: %.3f' %(pearson, rmse, auc))\n",
    "    return pearson, rmse, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    train_loss = 0 \n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq,num)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.squeeze().cpu().detach().numpy())\n",
    "        rho += p\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "  \n",
    "    print(   f\"Train Epoch: {epoch} \" f\" loss={train_loss:0.2e} \" )\n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "    return train_loss , rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model = model.eval()\n",
    "    test_loss = 0\n",
    "    rho = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            test_loss += criterion(output.squeeze(), target).item()  # sum up batch loss\n",
    "            # calculate pearson correlation \n",
    "            #pearson, rmse, auc = Scoring(target.cpu().detach(), output.cpu().detach())\n",
    "            p, _ =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            rho += p\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss ,rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = EnzymesDataset(df.reset_index(drop=True))\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=32, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "\n",
    "    model = Conv1D_OneChannel()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'for fold {fold} : \\n train_loss :  {train_loss}     test_loss : {test_loss} \\n \\n')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    #torch.save(model.state_dict(), f\"2-Conv1d_OneHot_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ===== 0 \n",
      "Train Epoch: 1  loss=4.24e+02 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 2  loss=1.30e+02 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 3  loss=1.19e+02 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 4  loss=1.08e+02 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 5  loss=9.69e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 6  loss=8.96e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 7  loss=8.77e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 8  loss=8.34e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 9  loss=7.84e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 10  loss=7.65e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 11  loss=7.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 12  loss=6.91e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 13  loss=6.63e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 14  loss=6.50e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 15  loss=6.23e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 16  loss=6.09e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 17  loss=5.84e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 18  loss=5.72e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 19  loss=5.64e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 20  loss=5.46e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 21  loss=5.36e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 22  loss=5.25e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 23  loss=5.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 24  loss=5.04e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 25  loss=5.12e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 26  loss=4.90e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 27  loss=4.81e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 28  loss=4.84e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 29  loss=4.68e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 30  loss=4.58e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 31  loss=4.61e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 32  loss=4.43e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 33  loss=4.65e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 34  loss=4.34e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 35  loss=4.34e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 36  loss=4.48e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 37  loss=4.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 38  loss=4.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 39  loss=4.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 40  loss=4.22e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 41  loss=4.17e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 42  loss=3.97e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 43  loss=3.98e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 44  loss=3.97e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 45  loss=3.90e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 46  loss=3.94e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 47  loss=3.86e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 48  loss=3.76e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 49  loss=3.70e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 50  loss=3.65e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 51  loss=3.69e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 52  loss=3.72e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 53  loss=3.69e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 54  loss=3.64e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 55  loss=3.63e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 56  loss=3.53e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 57  loss=3.65e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 58  loss=3.52e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 59  loss=3.47e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 60  loss=3.45e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 61  loss=3.42e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 62  loss=3.46e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 63  loss=3.58e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 64  loss=3.44e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 65  loss=3.39e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 66  loss=3.22e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 67  loss=3.29e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 68  loss=3.32e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 69  loss=3.33e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 70  loss=3.32e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 71  loss=3.25e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 72  loss=3.21e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 73  loss=3.27e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 74  loss=3.12e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 75  loss=3.15e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 76  loss=3.21e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 77  loss=3.09e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 78  loss=3.12e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 79  loss=3.26e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 80  loss=3.12e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 81  loss=3.05e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 82  loss=3.03e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 83  loss=3.03e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 84  loss=3.05e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 85  loss=3.00e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 86  loss=3.00e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 87  loss=2.93e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 88  loss=2.91e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 89  loss=2.98e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 90  loss=2.84e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 91  loss=2.94e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 92  loss=2.92e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 93  loss=2.86e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 94  loss=2.84e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 95  loss=2.85e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 96  loss=2.80e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 97  loss=2.86e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 98  loss=2.79e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 99  loss=2.77e+01 \n",
      "epoch ===== 0 \n",
      "Train Epoch: 100  loss=2.77e+01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n",
      "/tmp/ipykernel_715862/636985354.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_predicted = df_predicted.append(pd.DataFrame({'tm':output.squeeze().cpu().detach().numpy()}))\n"
     ]
    }
   ],
   "source": [
    "#train model and preditct\n",
    "train_rho_history = []\n",
    "train_loss_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print('epoch ===== 0 ')\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "submission_df =  predict(model,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc641468b80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "#plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-Loss-2pool.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc5c9ac5d30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "#plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-rho-2pool.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"5-AlexNet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+ 'test.csv',index_col='seq_id')\n",
    "test_df['tm']=submission_df['tm'].values\n",
    "test_df = test_df.drop(columns=['protein_sequence','pH','data_source'])\n",
    "test_df.to_csv('5-AlexNet.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a401f25d14e4726c47ec3d51a0ef0f076129e7cc070ddb98f69a4ab74ec023d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
