{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blosum as bl\n",
    "matrix = bl.BLOSUM(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protSeq1</th>\n",
       "      <th>operation</th>\n",
       "      <th>position1</th>\n",
       "      <th>position2</th>\n",
       "      <th>change1</th>\n",
       "      <th>change2</th>\n",
       "      <th>pH1</th>\n",
       "      <th>pH2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>L</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            protSeq1 operation  position1  \\\n",
       "0  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   replace          2   \n",
       "1  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   replace          2   \n",
       "2  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   replace          2   \n",
       "3  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   replace          2   \n",
       "4  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   replace          2   \n",
       "\n",
       "   position2 change1 change2  pH1  pH2  target  \n",
       "0          2       A       D  6.5  6.5    -6.7  \n",
       "1          2       A       E  6.5  6.5    -3.9  \n",
       "2          2       A       F  6.5  6.5    -1.2  \n",
       "3          2       A       G  6.5  6.5    -4.0  \n",
       "4          2       A       L  6.5  6.5     2.7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load training data (will be put in a function later) \n",
    "#TO CHANGE\n",
    "path = os.getcwd()\n",
    "for i in range(3) :\n",
    "\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "path += '/data/'\n",
    "train_df = pd.read_csv(path + 'clean_train_data.csv')\n",
    "train_df = train_df.drop(columns=['data_source1'])\n",
    "train_df = train_df.drop(columns=['data_source2'])\n",
    "\n",
    "train_df = train_df.drop(columns=['protSeq2'])\n",
    "train_df = train_df.drop(columns=['tm1'])\n",
    "train_df = train_df.drop(columns=['tm2'])\n",
    "train_df = train_df.drop(columns=['group1'])\n",
    "train_df = train_df.drop(columns=['group2'])\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "train_df.head()\n",
    "#dT = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>modif</th>\n",
       "      <th>score_adj</th>\n",
       "      <th>operation</th>\n",
       "      <th>change1</th>\n",
       "      <th>change2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31390</th>\n",
       "      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>E</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31391</th>\n",
       "      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31392</th>\n",
       "      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>delete</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31393</th>\n",
       "      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>C</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>F</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence  pH  modif  \\\n",
       "seq_id                                                                 \n",
       "31390   VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     16   \n",
       "31391   VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     16   \n",
       "31392   VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8     16   \n",
       "31393   VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     17   \n",
       "31394   VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     17   \n",
       "\n",
       "        score_adj operation change1 change2  \n",
       "seq_id                                       \n",
       "31390    0.880797   replace       E       L  \n",
       "31391    0.880797   replace       K       L  \n",
       "31392    0.999955    delete       K     NaN  \n",
       "31393    0.952574   replace       C       K  \n",
       "31394    0.952574   replace       F       K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(path+ 'test_mutations.csv', index_col=\"seq_id\")\n",
    "test_df = test_df.drop(columns=['data_source'])\n",
    "test_df = test_df.drop(columns=['b_factor'])\n",
    "test_df = test_df.drop(columns=['bFactorAdj'])\n",
    "test_df = test_df.drop(columns=['score'])\n",
    "test_df = test_df.drop(columns=['position1'])\n",
    "test_df = test_df.drop(columns=['position2'])\n",
    "\n",
    "test_df = test_df[test_df['pH']==8]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['length'] = test_df['protein_sequence'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protSeq1'].str.len()\n",
    "train_df['protein_sequence'] = train_df['protSeq1']\n",
    "train_df = train_df.drop(columns=['protSeq1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the distance of the 2 aa as a feature, with blosum: \n",
    "#hypothesis : this score influences the delta Tm. \n",
    "#Allows to encode the \"mutation\" and the info : which aa into which aa\n",
    "\n",
    "def blosum_apply(row):\n",
    "        if (row['operation']=='replace'):\n",
    "            res = matrix[row['change1'] + row['change2']]\n",
    "        else:\n",
    "                res = -10\n",
    "        return res\n",
    "\n",
    "train_df['dist_mutation'] = train_df.apply(blosum_apply, axis=1)\n",
    "test_df['dist_mutation'] = test_df.apply(blosum_apply, axis=1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['change1'])\n",
    "test_df = test_df.drop(columns=['change2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['change1'])\n",
    "train_df = train_df.drop(columns=['change2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "      <th>position1</th>\n",
       "      <th>position2</th>\n",
       "      <th>pH1</th>\n",
       "      <th>pH2</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dist_mutation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  operation  position1  position2  pH1  pH2  target  length  \\\n",
       "0   replace          2          2  6.5  6.5    -6.7     164   \n",
       "1   replace          2          2  6.5  6.5    -3.9     164   \n",
       "2   replace          2          2  6.5  6.5    -1.2     164   \n",
       "3   replace          2          2  6.5  6.5    -4.0     164   \n",
       "4   replace          2          2  6.5  6.5     2.7     164   \n",
       "\n",
       "                                    protein_sequence  dist_mutation  \n",
       "0  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0  \n",
       "1  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0  \n",
       "2  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0  \n",
       "3  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...            0.0  \n",
       "4  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>modif</th>\n",
       "      <th>score_adj</th>\n",
       "      <th>operation</th>\n",
       "      <th>length</th>\n",
       "      <th>dist_mutation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31390</th>\n",
       "      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31391</th>\n",
       "      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31392</th>\n",
       "      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>delete</td>\n",
       "      <td>220</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31393</th>\n",
       "      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence  pH  modif  \\\n",
       "seq_id                                                                 \n",
       "31390   VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     16   \n",
       "31391   VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     16   \n",
       "31392   VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8     16   \n",
       "31393   VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     17   \n",
       "31394   VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8     17   \n",
       "\n",
       "        score_adj operation  length  dist_mutation  \n",
       "seq_id                                              \n",
       "31390    0.880797   replace     221           -3.0  \n",
       "31391    0.880797   replace     221           -2.0  \n",
       "31392    0.999955    delete     220          -10.0  \n",
       "31393    0.952574   replace     221           -3.0  \n",
       "31394    0.952574   replace     221           -3.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the relative position of the mutation\n",
    "\n",
    "train_df['relative_position'] = train_df['position1']/train_df['length']\n",
    "train_df = train_df.drop(columns=['position1'])\n",
    "train_df = train_df.drop(columns=['position2'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['relative_position'] = test_df['modif']/test_df['length']\n",
    "test_df = test_df.drop(columns=['modif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "      <th>pH1</th>\n",
       "      <th>pH2</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  operation  pH1  pH2  target  length  \\\n",
       "0   replace  6.5  6.5    -6.7     164   \n",
       "1   replace  6.5  6.5    -3.9     164   \n",
       "2   replace  6.5  6.5    -1.2     164   \n",
       "3   replace  6.5  6.5    -4.0     164   \n",
       "4   replace  6.5  6.5     2.7     164   \n",
       "\n",
       "                                    protein_sequence  dist_mutation  \\\n",
       "0  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "1  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "2  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "3  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...            0.0   \n",
       "4  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "\n",
       "   relative_position  \n",
       "0           0.012195  \n",
       "1           0.012195  \n",
       "2           0.012195  \n",
       "3           0.012195  \n",
       "4           0.012195  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>score_adj</th>\n",
       "      <th>operation</th>\n",
       "      <th>length</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31390</th>\n",
       "      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.072398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31391</th>\n",
       "      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.072398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31392</th>\n",
       "      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>delete</td>\n",
       "      <td>220</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.072727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31393</th>\n",
       "      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>replace</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence  pH  score_adj  \\\n",
       "seq_id                                                                     \n",
       "31390   VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   0.880797   \n",
       "31391   VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   0.880797   \n",
       "31392   VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8   0.999955   \n",
       "31393   VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   0.952574   \n",
       "31394   VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   0.952574   \n",
       "\n",
       "       operation  length  dist_mutation  relative_position  \n",
       "seq_id                                                      \n",
       "31390    replace     221           -3.0           0.072398  \n",
       "31391    replace     221           -2.0           0.072398  \n",
       "31392     delete     220          -10.0           0.072727  \n",
       "31393    replace     221           -3.0           0.076923  \n",
       "31394    replace     221           -3.0           0.076923  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute difference of pH\n",
    "train_df['dPH'] = train_df['pH1']-train_df['pH2']\n",
    "train_df = train_df.drop(columns=['pH1'])\n",
    "train_df = train_df.drop(columns=['pH2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "      <th>dPH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>replace</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replace</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace</td>\n",
       "      <td>2.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  operation  target  length  \\\n",
       "0   replace    -6.7     164   \n",
       "1   replace    -3.9     164   \n",
       "2   replace    -1.2     164   \n",
       "3   replace    -4.0     164   \n",
       "4   replace     2.7     164   \n",
       "\n",
       "                                    protein_sequence  dist_mutation  \\\n",
       "0  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "1  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "2  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "3  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...            0.0   \n",
       "4  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "\n",
       "   relative_position  dPH  \n",
       "0           0.012195  0.0  \n",
       "1           0.012195  0.0  \n",
       "2           0.012195  0.0  \n",
       "3           0.012195  0.0  \n",
       "4           0.012195  0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['dPH'] =test_df['pH'] - 8\n",
    "test_df = test_df.drop(columns=['pH'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['score_adj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['operation'])\n",
    "trian_df = train_df.drop(columns=['operation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>length</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "      <th>dPH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31390</th>\n",
       "      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.072398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31391</th>\n",
       "      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>221</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.072398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31392</th>\n",
       "      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n",
       "      <td>220</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31393</th>\n",
       "      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n",
       "      <td>221</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_sequence  length  \\\n",
       "seq_id                                                              \n",
       "31390   VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...     221   \n",
       "31391   VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...     221   \n",
       "31392   VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...     220   \n",
       "31393   VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...     221   \n",
       "31394   VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...     221   \n",
       "\n",
       "        dist_mutation  relative_position  dPH  \n",
       "seq_id                                         \n",
       "31390            -3.0           0.072398    0  \n",
       "31391            -2.0           0.072398    0  \n",
       "31392           -10.0           0.072727    0  \n",
       "31393            -3.0           0.076923    0  \n",
       "31394            -3.0           0.076923    0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "      <th>dPH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>replace</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replace</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace</td>\n",
       "      <td>2.7</td>\n",
       "      <td>164</td>\n",
       "      <td>MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  operation  target  length  \\\n",
       "0   replace    -6.7     164   \n",
       "1   replace    -3.9     164   \n",
       "2   replace    -1.2     164   \n",
       "3   replace    -4.0     164   \n",
       "4   replace     2.7     164   \n",
       "\n",
       "                                    protein_sequence  dist_mutation  \\\n",
       "0  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "1  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "2  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -2.0   \n",
       "3  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...            0.0   \n",
       "4  MNAFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...           -1.0   \n",
       "\n",
       "   relative_position  dPH  \n",
       "0           0.012195  0.0  \n",
       "1           0.012195  0.0  \n",
       "2           0.012195  0.0  \n",
       "3           0.012195  0.0  \n",
       "4           0.012195  0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix max_length to be 500\n",
    "\n",
    "max_length = np.max(train_df['length'])\n",
    "\n",
    "#drop rows that exceeds this value\n",
    "\n",
    "#train_df = train_df[train_df['length'] < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(sequence):\n",
    "    alphabet = ['A', 'C', 'D', 'E', 'F', 'G','H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] # aa letters\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet)) \n",
    "    integer_encoded = [char_to_int[char] for char in sequence] #each character becomes int\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(alphabet))] #0 for all letters\n",
    "        letter[value] = 1 #modify the column corresponding to the letter to 1\n",
    "        onehot_encoded.append(letter) #put in the array (1 letter = 1 array of 20 columns)\n",
    "    \n",
    "    ar =   np.transpose(np.array(onehot_encoded))\n",
    "    zeros = np.zeros([len(alphabet),max_length - len(integer_encoded)] )\n",
    "    onehot_encoded = np.concatenate((ar, zeros), axis = 1) #zero padding\n",
    "\n",
    "\n",
    "    return onehot_encoded #we have all arrays, corresponding to the whole sequence\n",
    "\n",
    "\n",
    "# new column with encoded sequence (apply for each sequence)\n",
    "train_df['encoded_sequence'] = train_df['protein_sequence'].apply(lambda x: encode_seq(x))\n",
    "test_df['encoded_sequence'] = test_df['protein_sequence'].apply(lambda x: encode_seq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.iloc[np.random.permutation(train_df.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = df.sample(frac=0.8,random_state=24)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837 459\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df),len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si met la transformation dans le dataframe : le kernel dies\n",
    "Si met avant, dans le panda, les dimensions sont pas les bonnes (peut être transposer ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dist_mutation</th>\n",
       "      <th>relative_position</th>\n",
       "      <th>dPH</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>replace</td>\n",
       "      <td>0.3</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replace</td>\n",
       "      <td>-11.3</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace</td>\n",
       "      <td>6.5</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>replace</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96</td>\n",
       "      <td>DVSGTVCLSALPPEATDTLNLIASDGPFPYSQDGVVFQNRESVLPT...</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>replace</td>\n",
       "      <td>12.5</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>replace</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>replace</td>\n",
       "      <td>1.8</td>\n",
       "      <td>231</td>\n",
       "      <td>MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.528139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>replace</td>\n",
       "      <td>11.4</td>\n",
       "      <td>537</td>\n",
       "      <td>MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2296 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     operation  target  length  \\\n",
       "0      replace     0.3     231   \n",
       "1      replace   -11.3     231   \n",
       "2      replace    -3.4     231   \n",
       "3      replace    -7.1     231   \n",
       "4      replace     6.5     231   \n",
       "...        ...     ...     ...   \n",
       "2291   replace     4.0      96   \n",
       "2292   replace    12.5     231   \n",
       "2293   replace   -10.2     231   \n",
       "2294   replace     1.8     231   \n",
       "2295   replace    11.4     537   \n",
       "\n",
       "                                       protein_sequence  dist_mutation  \\\n",
       "0     MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -1.0   \n",
       "1     MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -2.0   \n",
       "2     MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -1.0   \n",
       "3     MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -2.0   \n",
       "4     MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -1.0   \n",
       "...                                                 ...            ...   \n",
       "2291  DVSGTVCLSALPPEATDTLNLIASDGPFPYSQDGVVFQNRESVLPT...           -3.0   \n",
       "2292  MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -2.0   \n",
       "2293  MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...           -2.0   \n",
       "2294  MLVMTEYLLSAGICMAIVSILLIGMAISNVSKGQYAKRFFFFATSC...            3.0   \n",
       "2295  MGCVQCKDKEATKLTEERDGSLNQSSGYRYGTDPTPQHYPSFGVTS...            0.0   \n",
       "\n",
       "      relative_position  dPH  \\\n",
       "0              0.870130  0.0   \n",
       "1              0.619048  0.0   \n",
       "2              0.619048  0.0   \n",
       "3              0.619048  0.0   \n",
       "4              0.619048  0.0   \n",
       "...                 ...  ...   \n",
       "2291           0.812500  0.0   \n",
       "2292           0.619048  0.0   \n",
       "2293           0.619048  0.0   \n",
       "2294           0.528139  0.0   \n",
       "2295           0.197393  0.0   \n",
       "\n",
       "                                       encoded_sequence  \n",
       "0     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                 ...  \n",
       "2291  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2292  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2293  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2294  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2295  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[2296 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "    def __init__(self,df,train=True):\n",
    "        sequence= df['encoded_sequence']\n",
    "        \n",
    "        # numerical : pH and length\n",
    "        numerical = df[['dPH','length', 'relative_position', 'dist_mutation']].values\n",
    "\n",
    "        # y : the target (tm)\n",
    "        if train == True : \n",
    "            y=df['target'].values\n",
    "        else : \n",
    "            y = np.zeros(np.shape(numerical)[0])\n",
    "        self.y=torch.tensor(y,dtype=torch.float32)\n",
    "        #creta tensors from the numpy arrays\n",
    "        self.x_sequence=torch.tensor(sequence)\n",
    "        self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    "   \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_sequence[idx], self.y[idx],self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a faire : apres avoir fait one hot encoding, trouver comment mettre l'info de plusieurs channels dans le dataframe, sans qu'il mette d'erreur sur la taille. \n",
    "Voir comment mettre un tableau = 1 aa puis la longueur de la ligne = longueur totale (juste transposer ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class MyLoss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, classes):\n",
    "        super(MyLoss, self).__init__()\n",
    "        # define some attributes\n",
    "        self.y_true_one_hot = torch.FloatTensor(batch_size, classes, length)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        with torch.no_grad():\n",
    "            self.y_true_one_hot.zero_().scatter_(1, y_true, 1) # permet one hot encoding\n",
    "        # do some operations\n",
    "        return loss\n",
    "\n",
    "\n",
    "Or use cross entropy loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataframes\n",
    "train_d = EnzymesDataset(train_df)\n",
    "val_d = EnzymesDataset(val_df, train=False)\n",
    "\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_d, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "            \n",
    "            nn.Conv1d(20, 10,kernel_size=6, stride=1, padding=3),\n",
    "            nn.ELU(),\n",
    "            \n",
    "\n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            nn.Conv1d(10, 10,kernel_size=6, stride=1, padding=2), \n",
    "            nn.ELU(), \n",
    "            \n",
    "            \n",
    "         \n",
    "            torch.nn.MaxPool1d(10), \n",
    "            \n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout2d(p=0.5, inplace=False),\n",
    "            \n",
    " \n",
    "            \n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(4, 20),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(20, 100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100, 4),\n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(524, 128),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ELU(),\n",
    " \n",
    "            nn.Linear(128, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        \n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "       \n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "      \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voir pour changer activation fonction\n",
    "#voir pq probleme de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1D_OneChannel()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scoring(df_te, df_predicted):\n",
    "    df = {\n",
    "    \"true\": df_te['target'],\n",
    "    \"predicted\": df_predicted['target']\n",
    "}\n",
    "    pearson = df.corr(method='pearson')\n",
    "    rmse = mean_squared_error(df_te['target'], df_predicted['target'], squared=False)\n",
    "    auc = metrics.roc_auc_score(df_te['target'], df_predicted['target'])\n",
    "    \n",
    "    print('Pearson: %.3f, RMSE %.3f, AUC: %.3f' %(pearson, rmse, auc))\n",
    "    return pearson, rmse, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    rho = 0 \n",
    "    train_loss = 0 \n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            seq = seq.cuda()\n",
    "            target = target.cuda()\n",
    "            num = num.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq,num)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # calculate Spearman's rank correlation coefficient\n",
    "        p, _ = spearmanr(target.cpu().detach().numpy(), output.squeeze().cpu().detach().numpy())\n",
    "        rho += p\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "   \n",
    "    print(   f\"Train Epoch: {epoch} \" f\" loss={train_loss:0.2e} \" )\n",
    "\n",
    "    rho = rho / len(train_loader)\n",
    "    return train_loss , rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model = model.eval()\n",
    "    test_loss = 0\n",
    "    rho = 0\n",
    "    with torch.no_grad():\n",
    "        for (seq, target,num) in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            test_loss += criterion(output.squeeze(), target).item()  # sum up batch loss\n",
    "            # calculate pearson correlation \n",
    "            #pearson, rmse, auc = Scoring(target.cpu().detach(), output.cpu().detach())\n",
    "            p, _ =  spearmanr(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            rho += p\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    rho = rho / len(test_loader)\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss ,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader):\n",
    "    model = model.eval()\n",
    "    df_predicted = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for (seq, _,num) in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                seq = seq.cuda()\n",
    "                #target = target.cuda()\n",
    "                num = num.cuda()\n",
    "            output = model(seq,num)\n",
    "            df_predicted = df_predicted.append(pd.DataFrame({'target':output.squeeze().cpu().detach().numpy()}))\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "k_folds = 5\n",
    "learning_rate = 1e-4\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = EnzymesDataset(df.reset_index(drop=True))\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=32, sampler=train_subsampler)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "\n",
    "    model = Conv1D_OneChannel()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss , rho_train = train_epoch( model, optimizer, criterion, train_dl, epoch)\n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "        \n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'for fold {fold} : \\n train_loss :  {train_loss}     test_loss : {test_loss} \\n \\n')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_rho_history = []\n",
    "test_rho_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_loss , rho_test = test_epoch(model, criterion, val_dl)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "    \n",
    "    #torch.save(model.state_dict(), f\"2-Conv1d_OneHot_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  loss=8.34e+01 \n",
      "Test set: Average loss: 6.57e-02 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4science/anaconda3/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4529: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2  loss=8.33e+01 \n",
      "Test set: Average loss: 5.06e-03 \n",
      "Train Epoch: 3  loss=8.37e+01 \n",
      "Test set: Average loss: 3.58e-03 \n",
      "Train Epoch: 4  loss=8.36e+01 \n",
      "Test set: Average loss: 6.12e-03 \n",
      "Train Epoch: 5  loss=8.34e+01 \n",
      "Test set: Average loss: 1.68e-03 \n",
      "Train Epoch: 6  loss=8.32e+01 \n",
      "Test set: Average loss: 2.09e-03 \n",
      "Train Epoch: 7  loss=8.33e+01 \n",
      "Test set: Average loss: 1.03e-02 \n",
      "Train Epoch: 8  loss=8.32e+01 \n",
      "Test set: Average loss: 2.23e-03 \n",
      "Train Epoch: 9  loss=8.34e+01 \n",
      "Test set: Average loss: 2.36e-03 \n",
      "Train Epoch: 10  loss=8.39e+01 \n",
      "Test set: Average loss: 2.54e-03 \n",
      "Train Epoch: 11  loss=8.33e+01 \n",
      "Test set: Average loss: 4.77e-03 \n",
      "Train Epoch: 12  loss=8.34e+01 \n",
      "Test set: Average loss: 9.45e-03 \n",
      "Train Epoch: 13  loss=8.33e+01 \n",
      "Test set: Average loss: 4.83e-03 \n",
      "Train Epoch: 14  loss=8.34e+01 \n",
      "Test set: Average loss: 4.46e-03 \n",
      "Train Epoch: 15  loss=8.31e+01 \n",
      "Test set: Average loss: 7.76e-03 \n",
      "Train Epoch: 16  loss=8.33e+01 \n",
      "Test set: Average loss: 1.39e-02 \n",
      "Train Epoch: 17  loss=8.30e+01 \n",
      "Test set: Average loss: 4.14e-03 \n",
      "Train Epoch: 18  loss=8.31e+01 \n",
      "Test set: Average loss: 4.39e-03 \n",
      "Train Epoch: 19  loss=8.30e+01 \n",
      "Test set: Average loss: 1.60e-02 \n",
      "Train Epoch: 20  loss=8.35e+01 \n",
      "Test set: Average loss: 5.43e-03 \n",
      "Train Epoch: 21  loss=8.32e+01 \n",
      "Test set: Average loss: 7.69e-03 \n",
      "Train Epoch: 22  loss=8.34e+01 \n",
      "Test set: Average loss: 8.05e-03 \n",
      "Train Epoch: 23  loss=8.34e+01 \n",
      "Test set: Average loss: 9.83e-03 \n",
      "Train Epoch: 24  loss=8.33e+01 \n",
      "Test set: Average loss: 1.62e-02 \n",
      "Train Epoch: 25  loss=8.37e+01 \n",
      "Test set: Average loss: 1.33e-02 \n",
      "Train Epoch: 26  loss=8.32e+01 \n",
      "Test set: Average loss: 7.49e-03 \n",
      "Train Epoch: 27  loss=8.30e+01 \n",
      "Test set: Average loss: 1.79e-02 \n",
      "Train Epoch: 28  loss=8.29e+01 \n",
      "Test set: Average loss: 1.91e+00 \n",
      "Train Epoch: 29  loss=8.08e+01 \n",
      "Test set: Average loss: 1.13e+01 \n",
      "Train Epoch: 30  loss=7.84e+01 \n",
      "Test set: Average loss: 1.04e+00 \n",
      "Train Epoch: 31  loss=7.71e+01 \n",
      "Test set: Average loss: 1.25e+01 \n",
      "Train Epoch: 32  loss=7.58e+01 \n",
      "Test set: Average loss: 9.72e+00 \n",
      "Train Epoch: 33  loss=7.50e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 34  loss=7.49e+01 \n",
      "Test set: Average loss: 8.33e+00 \n",
      "Train Epoch: 35  loss=7.32e+01 \n",
      "Test set: Average loss: 1.05e+01 \n",
      "Train Epoch: 36  loss=7.25e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 37  loss=7.25e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 38  loss=7.14e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 39  loss=7.20e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 40  loss=7.12e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 41  loss=7.14e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 42  loss=7.08e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 43  loss=7.06e+01 \n",
      "Test set: Average loss: 1.14e+01 \n",
      "Train Epoch: 44  loss=7.09e+01 \n",
      "Test set: Average loss: 1.19e+01 \n",
      "Train Epoch: 45  loss=7.02e+01 \n",
      "Test set: Average loss: 2.00e+01 \n",
      "Train Epoch: 46  loss=7.13e+01 \n",
      "Test set: Average loss: 1.99e+01 \n",
      "Train Epoch: 47  loss=7.04e+01 \n",
      "Test set: Average loss: 1.30e+01 \n",
      "Train Epoch: 48  loss=7.09e+01 \n",
      "Test set: Average loss: 9.57e+00 \n",
      "Train Epoch: 49  loss=7.01e+01 \n",
      "Test set: Average loss: 1.33e+01 \n",
      "Train Epoch: 50  loss=6.99e+01 \n",
      "Test set: Average loss: 1.11e+01 \n",
      "Train Epoch: 51  loss=7.01e+01 \n",
      "Test set: Average loss: 9.74e+00 \n",
      "Train Epoch: 52  loss=7.08e+01 \n",
      "Test set: Average loss: 9.70e+00 \n",
      "Train Epoch: 53  loss=7.03e+01 \n",
      "Test set: Average loss: 1.11e+01 \n",
      "Train Epoch: 54  loss=7.08e+01 \n",
      "Test set: Average loss: 1.11e+01 \n",
      "Train Epoch: 55  loss=7.10e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 56  loss=6.98e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 57  loss=7.04e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 58  loss=7.08e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 59  loss=7.03e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 60  loss=6.99e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 61  loss=7.02e+01 \n",
      "Test set: Average loss: 1.90e+01 \n",
      "Train Epoch: 62  loss=7.04e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 63  loss=6.97e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 64  loss=7.09e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 65  loss=7.09e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 66  loss=7.02e+01 \n",
      "Test set: Average loss: 1.25e+01 \n",
      "Train Epoch: 67  loss=7.08e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 68  loss=7.05e+01 \n",
      "Test set: Average loss: 1.03e+01 \n",
      "Train Epoch: 69  loss=6.95e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 70  loss=6.99e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 71  loss=6.98e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 72  loss=6.95e+01 \n",
      "Test set: Average loss: 1.26e+01 \n",
      "Train Epoch: 73  loss=7.10e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 74  loss=6.97e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 75  loss=7.02e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 76  loss=7.06e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 77  loss=6.99e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 78  loss=6.99e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 79  loss=7.02e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 80  loss=7.01e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 81  loss=6.94e+01 \n",
      "Test set: Average loss: 1.38e+01 \n",
      "Train Epoch: 82  loss=6.93e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 83  loss=7.00e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 84  loss=7.00e+01 \n",
      "Test set: Average loss: 1.38e+01 \n",
      "Train Epoch: 85  loss=7.04e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 86  loss=6.99e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 87  loss=6.94e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 88  loss=7.02e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 89  loss=7.06e+01 \n",
      "Test set: Average loss: 1.44e+01 \n",
      "Train Epoch: 90  loss=7.02e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 91  loss=7.02e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 92  loss=7.03e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 93  loss=6.97e+01 \n",
      "Test set: Average loss: 9.74e+00 \n",
      "Train Epoch: 94  loss=6.98e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 95  loss=7.01e+01 \n",
      "Test set: Average loss: 1.38e+01 \n",
      "Train Epoch: 96  loss=7.00e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 97  loss=7.00e+01 \n",
      "Test set: Average loss: 1.10e+01 \n",
      "Train Epoch: 98  loss=7.04e+01 \n",
      "Test set: Average loss: 1.16e+01 \n",
      "Train Epoch: 99  loss=6.94e+01 \n",
      "Test set: Average loss: 1.27e+01 \n",
      "Train Epoch: 100  loss=7.00e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 101  loss=6.98e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 102  loss=6.97e+01 \n",
      "Test set: Average loss: 1.25e+01 \n",
      "Train Epoch: 103  loss=6.91e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 104  loss=6.97e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 105  loss=6.96e+01 \n",
      "Test set: Average loss: 1.23e+01 \n",
      "Train Epoch: 106  loss=6.92e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 107  loss=6.96e+01 \n",
      "Test set: Average loss: 1.93e+01 \n",
      "Train Epoch: 108  loss=6.95e+01 \n",
      "Test set: Average loss: 1.21e+01 \n",
      "Train Epoch: 109  loss=6.94e+01 \n",
      "Test set: Average loss: 1.15e+01 \n",
      "Train Epoch: 110  loss=6.97e+01 \n",
      "Test set: Average loss: 1.24e+01 \n",
      "Train Epoch: 111  loss=6.93e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 112  loss=6.96e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 113  loss=6.93e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 114  loss=6.98e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 115  loss=6.98e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 116  loss=6.98e+01 \n",
      "Test set: Average loss: 1.08e+01 \n",
      "Train Epoch: 117  loss=6.94e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 118  loss=6.93e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 119  loss=6.92e+01 \n",
      "Test set: Average loss: 2.19e+01 \n",
      "Train Epoch: 120  loss=7.02e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 121  loss=6.96e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 122  loss=6.99e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 123  loss=6.94e+01 \n",
      "Test set: Average loss: 1.12e+01 \n",
      "Train Epoch: 124  loss=7.05e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 125  loss=6.92e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 126  loss=6.93e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 127  loss=6.99e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 128  loss=6.95e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 129  loss=6.95e+01 \n",
      "Test set: Average loss: 1.09e+01 \n",
      "Train Epoch: 130  loss=7.03e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 131  loss=6.95e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 132  loss=6.97e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 133  loss=6.94e+01 \n",
      "Test set: Average loss: 1.34e+01 \n",
      "Train Epoch: 134  loss=6.95e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 135  loss=6.91e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 136  loss=6.94e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 137  loss=6.96e+01 \n",
      "Test set: Average loss: 1.14e+01 \n",
      "Train Epoch: 138  loss=6.89e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 139  loss=6.93e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 140  loss=6.99e+01 \n",
      "Test set: Average loss: 1.38e+01 \n",
      "Train Epoch: 141  loss=6.90e+01 \n",
      "Test set: Average loss: 1.26e+01 \n",
      "Train Epoch: 142  loss=6.94e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 143  loss=6.94e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 144  loss=6.97e+01 \n",
      "Test set: Average loss: 1.21e+01 \n",
      "Train Epoch: 145  loss=6.90e+01 \n",
      "Test set: Average loss: 2.09e+01 \n",
      "Train Epoch: 146  loss=6.88e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 147  loss=6.97e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 148  loss=6.92e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 149  loss=6.89e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 150  loss=6.92e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 151  loss=6.87e+01 \n",
      "Test set: Average loss: 1.21e+01 \n",
      "Train Epoch: 152  loss=7.00e+01 \n",
      "Test set: Average loss: 1.30e+01 \n",
      "Train Epoch: 153  loss=6.91e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 154  loss=6.91e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 155  loss=6.87e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 156  loss=6.85e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 157  loss=6.90e+01 \n",
      "Test set: Average loss: 1.33e+01 \n",
      "Train Epoch: 158  loss=6.92e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 159  loss=6.92e+01 \n",
      "Test set: Average loss: 1.23e+01 \n",
      "Train Epoch: 160  loss=6.90e+01 \n",
      "Test set: Average loss: 1.13e+01 \n",
      "Train Epoch: 161  loss=6.95e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 162  loss=6.88e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 163  loss=6.92e+01 \n",
      "Test set: Average loss: 2.11e+01 \n",
      "Train Epoch: 164  loss=6.94e+01 \n",
      "Test set: Average loss: 1.18e+01 \n",
      "Train Epoch: 165  loss=6.88e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 166  loss=6.88e+01 \n",
      "Test set: Average loss: 1.22e+01 \n",
      "Train Epoch: 167  loss=6.96e+01 \n",
      "Test set: Average loss: 1.30e+01 \n",
      "Train Epoch: 168  loss=6.95e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 169  loss=6.93e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 170  loss=6.92e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 171  loss=6.89e+01 \n",
      "Test set: Average loss: 1.22e+01 \n",
      "Train Epoch: 172  loss=6.88e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 173  loss=6.90e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 174  loss=6.94e+01 \n",
      "Test set: Average loss: 1.18e+01 \n",
      "Train Epoch: 175  loss=6.95e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 176  loss=6.88e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 177  loss=6.94e+01 \n",
      "Test set: Average loss: 1.38e+01 \n",
      "Train Epoch: 178  loss=6.87e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 179  loss=6.91e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 180  loss=6.93e+01 \n",
      "Test set: Average loss: 1.24e+01 \n",
      "Train Epoch: 181  loss=6.87e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 182  loss=6.84e+01 \n",
      "Test set: Average loss: 1.44e+01 \n",
      "Train Epoch: 183  loss=6.87e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 184  loss=6.96e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 185  loss=6.89e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 186  loss=6.96e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 187  loss=6.88e+01 \n",
      "Test set: Average loss: 2.07e+01 \n",
      "Train Epoch: 188  loss=6.87e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 189  loss=6.90e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 190  loss=6.92e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 191  loss=6.87e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 192  loss=6.85e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 193  loss=6.91e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 194  loss=6.87e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 195  loss=6.91e+01 \n",
      "Test set: Average loss: 2.40e+01 \n",
      "Train Epoch: 196  loss=6.89e+01 \n",
      "Test set: Average loss: 1.04e+01 \n",
      "Train Epoch: 197  loss=6.93e+01 \n",
      "Test set: Average loss: 1.33e+01 \n",
      "Train Epoch: 198  loss=6.94e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 199  loss=6.94e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 200  loss=6.95e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 201  loss=6.89e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 202  loss=6.87e+01 \n",
      "Test set: Average loss: 1.21e+01 \n",
      "Train Epoch: 203  loss=6.89e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 204  loss=6.88e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 205  loss=6.90e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 206  loss=6.91e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 207  loss=6.90e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 208  loss=6.87e+01 \n",
      "Test set: Average loss: 1.19e+01 \n",
      "Train Epoch: 209  loss=6.92e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 210  loss=6.89e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 211  loss=6.87e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 212  loss=6.84e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 213  loss=6.82e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 214  loss=6.81e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 215  loss=6.81e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 216  loss=6.88e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 217  loss=6.86e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 218  loss=6.86e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 219  loss=6.85e+01 \n",
      "Test set: Average loss: 1.90e+01 \n",
      "Train Epoch: 220  loss=6.82e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 221  loss=6.84e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 222  loss=6.84e+01 \n",
      "Test set: Average loss: 1.21e+01 \n",
      "Train Epoch: 223  loss=6.83e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 224  loss=6.81e+01 \n",
      "Test set: Average loss: 1.34e+01 \n",
      "Train Epoch: 225  loss=6.91e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 226  loss=6.91e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 227  loss=6.87e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 228  loss=6.91e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 229  loss=6.91e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 230  loss=6.85e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 231  loss=6.87e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 232  loss=6.87e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 233  loss=6.85e+01 \n",
      "Test set: Average loss: 1.33e+01 \n",
      "Train Epoch: 234  loss=6.80e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 235  loss=6.82e+01 \n",
      "Test set: Average loss: 1.90e+01 \n",
      "Train Epoch: 236  loss=6.88e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 237  loss=6.79e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 238  loss=6.86e+01 \n",
      "Test set: Average loss: 1.34e+01 \n",
      "Train Epoch: 239  loss=6.89e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 240  loss=6.83e+01 \n",
      "Test set: Average loss: 1.96e+01 \n",
      "Train Epoch: 241  loss=6.95e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 242  loss=6.77e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 243  loss=6.81e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 244  loss=6.81e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 245  loss=6.87e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 246  loss=6.94e+01 \n",
      "Test set: Average loss: 1.23e+01 \n",
      "Train Epoch: 247  loss=6.89e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 248  loss=6.87e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 249  loss=6.82e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 250  loss=6.82e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 251  loss=6.87e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 252  loss=6.83e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 253  loss=6.91e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 254  loss=6.80e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 255  loss=6.81e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 256  loss=6.84e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 257  loss=6.82e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 258  loss=6.94e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 259  loss=6.84e+01 \n",
      "Test set: Average loss: 1.92e+01 \n",
      "Train Epoch: 260  loss=6.85e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 261  loss=6.79e+01 \n",
      "Test set: Average loss: 1.37e+01 \n",
      "Train Epoch: 262  loss=6.79e+01 \n",
      "Test set: Average loss: 1.37e+01 \n",
      "Train Epoch: 263  loss=6.78e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 264  loss=6.77e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 265  loss=6.83e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 266  loss=6.78e+01 \n",
      "Test set: Average loss: 1.27e+01 \n",
      "Train Epoch: 267  loss=6.88e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 268  loss=6.76e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 269  loss=6.80e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 270  loss=6.81e+01 \n",
      "Test set: Average loss: 1.22e+01 \n",
      "Train Epoch: 271  loss=6.89e+01 \n",
      "Test set: Average loss: 1.28e+01 \n",
      "Train Epoch: 272  loss=6.84e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 273  loss=6.81e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 274  loss=6.87e+01 \n",
      "Test set: Average loss: 1.15e+01 \n",
      "Train Epoch: 275  loss=6.78e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 276  loss=6.78e+01 \n",
      "Test set: Average loss: 1.99e+01 \n",
      "Train Epoch: 277  loss=6.78e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 278  loss=6.83e+01 \n",
      "Test set: Average loss: 1.22e+01 \n",
      "Train Epoch: 279  loss=6.87e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 280  loss=6.77e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 281  loss=6.79e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 282  loss=6.79e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 283  loss=6.78e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 284  loss=6.77e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 285  loss=6.77e+01 \n",
      "Test set: Average loss: 1.37e+01 \n",
      "Train Epoch: 286  loss=6.78e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 287  loss=6.80e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 288  loss=6.79e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 289  loss=6.80e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 290  loss=6.77e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 291  loss=6.78e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 292  loss=6.78e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 293  loss=6.84e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 294  loss=6.86e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 295  loss=6.77e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 296  loss=6.77e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 297  loss=6.77e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 298  loss=6.80e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 299  loss=6.77e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 300  loss=6.82e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 301  loss=6.80e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 302  loss=6.79e+01 \n",
      "Test set: Average loss: 1.30e+01 \n",
      "Train Epoch: 303  loss=6.79e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 304  loss=6.76e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 305  loss=6.81e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 306  loss=6.77e+01 \n",
      "Test set: Average loss: 1.22e+01 \n",
      "Train Epoch: 307  loss=6.81e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 308  loss=6.79e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 309  loss=6.80e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 310  loss=6.74e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 311  loss=6.78e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 312  loss=6.79e+01 \n",
      "Test set: Average loss: 1.35e+01 \n",
      "Train Epoch: 313  loss=6.84e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 314  loss=6.77e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 315  loss=6.77e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 316  loss=6.78e+01 \n",
      "Test set: Average loss: 1.19e+01 \n",
      "Train Epoch: 317  loss=6.81e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 318  loss=6.80e+01 \n",
      "Test set: Average loss: 1.44e+01 \n",
      "Train Epoch: 319  loss=6.74e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 320  loss=6.75e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 321  loss=6.81e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 322  loss=6.82e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 323  loss=6.74e+01 \n",
      "Test set: Average loss: 1.29e+01 \n",
      "Train Epoch: 324  loss=6.77e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 325  loss=6.79e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 326  loss=6.73e+01 \n",
      "Test set: Average loss: 1.36e+01 \n",
      "Train Epoch: 327  loss=6.75e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 328  loss=6.75e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 329  loss=6.76e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 330  loss=6.79e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 331  loss=6.80e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 332  loss=6.80e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 333  loss=6.81e+01 \n",
      "Test set: Average loss: 1.97e+01 \n",
      "Train Epoch: 334  loss=6.76e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 335  loss=6.88e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 336  loss=6.78e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 337  loss=6.81e+01 \n",
      "Test set: Average loss: 1.89e+01 \n",
      "Train Epoch: 338  loss=6.77e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 339  loss=6.83e+01 \n",
      "Test set: Average loss: 1.92e+01 \n",
      "Train Epoch: 340  loss=6.72e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 341  loss=6.77e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 342  loss=6.79e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 343  loss=6.81e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 344  loss=6.79e+01 \n",
      "Test set: Average loss: 1.96e+01 \n",
      "Train Epoch: 345  loss=6.80e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 346  loss=6.75e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 347  loss=6.82e+01 \n",
      "Test set: Average loss: 1.23e+01 \n",
      "Train Epoch: 348  loss=6.79e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 349  loss=6.77e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 350  loss=6.74e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 351  loss=6.74e+01 \n",
      "Test set: Average loss: 1.97e+01 \n",
      "Train Epoch: 352  loss=6.75e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 353  loss=6.71e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 354  loss=6.72e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 355  loss=6.78e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 356  loss=6.75e+01 \n",
      "Test set: Average loss: 1.99e+01 \n",
      "Train Epoch: 357  loss=6.73e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 358  loss=6.72e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 359  loss=6.73e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 360  loss=6.70e+01 \n",
      "Test set: Average loss: 1.90e+01 \n",
      "Train Epoch: 361  loss=6.73e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 362  loss=6.76e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 363  loss=6.75e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 364  loss=6.72e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 365  loss=6.71e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 366  loss=6.72e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 367  loss=6.72e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 368  loss=6.70e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 369  loss=6.75e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 370  loss=6.71e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 371  loss=6.71e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 372  loss=6.72e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 373  loss=6.70e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 374  loss=6.69e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 375  loss=6.72e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 376  loss=6.76e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 377  loss=6.73e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 378  loss=6.69e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 379  loss=6.77e+01 \n",
      "Test set: Average loss: 1.27e+01 \n",
      "Train Epoch: 380  loss=6.81e+01 \n",
      "Test set: Average loss: 1.94e+01 \n",
      "Train Epoch: 381  loss=6.82e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 382  loss=6.77e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 383  loss=6.82e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 384  loss=6.73e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 385  loss=6.74e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 386  loss=6.79e+01 \n",
      "Test set: Average loss: 1.33e+01 \n",
      "Train Epoch: 387  loss=6.81e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 388  loss=6.74e+01 \n",
      "Test set: Average loss: 2.40e+01 \n",
      "Train Epoch: 389  loss=6.74e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 390  loss=6.75e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 391  loss=6.70e+01 \n",
      "Test set: Average loss: 2.07e+01 \n",
      "Train Epoch: 392  loss=6.71e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 393  loss=6.70e+01 \n",
      "Test set: Average loss: 1.91e+01 \n",
      "Train Epoch: 394  loss=6.75e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 395  loss=6.76e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 396  loss=6.72e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 397  loss=6.76e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 398  loss=6.73e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 399  loss=6.71e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 400  loss=6.74e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 401  loss=6.72e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 402  loss=6.79e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 403  loss=6.69e+01 \n",
      "Test set: Average loss: 2.01e+01 \n",
      "Train Epoch: 404  loss=6.72e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 405  loss=6.74e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 406  loss=6.71e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 407  loss=6.73e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 408  loss=6.71e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 409  loss=6.73e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 410  loss=6.71e+01 \n",
      "Test set: Average loss: 1.95e+01 \n",
      "Train Epoch: 411  loss=6.79e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 412  loss=6.74e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 413  loss=6.72e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 414  loss=6.77e+01 \n",
      "Test set: Average loss: 1.32e+01 \n",
      "Train Epoch: 415  loss=6.75e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 416  loss=6.71e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 417  loss=6.72e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 418  loss=6.69e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 419  loss=6.76e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 420  loss=6.72e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 421  loss=6.79e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 422  loss=6.72e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 423  loss=6.77e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 424  loss=6.74e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 425  loss=6.71e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 426  loss=6.73e+01 \n",
      "Test set: Average loss: 1.92e+01 \n",
      "Train Epoch: 427  loss=6.74e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 428  loss=6.77e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 429  loss=6.76e+01 \n",
      "Test set: Average loss: 1.30e+01 \n",
      "Train Epoch: 430  loss=6.76e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 431  loss=6.77e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 432  loss=6.75e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 433  loss=6.75e+01 \n",
      "Test set: Average loss: 1.42e+01 \n",
      "Train Epoch: 434  loss=6.77e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 435  loss=6.74e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 436  loss=6.77e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 437  loss=6.76e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 438  loss=6.76e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 439  loss=6.72e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 440  loss=6.73e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 441  loss=6.75e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 442  loss=6.73e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 443  loss=6.75e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 444  loss=6.69e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 445  loss=6.77e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 446  loss=6.73e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 447  loss=6.69e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 448  loss=6.72e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 449  loss=6.73e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 450  loss=6.73e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 451  loss=6.76e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 452  loss=6.77e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 453  loss=6.74e+01 \n",
      "Test set: Average loss: 1.99e+01 \n",
      "Train Epoch: 454  loss=6.77e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 455  loss=6.70e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 456  loss=6.70e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 457  loss=6.67e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 458  loss=6.71e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 459  loss=6.75e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 460  loss=6.75e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 461  loss=6.67e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 462  loss=6.71e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 463  loss=6.67e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 464  loss=6.74e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 465  loss=6.69e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 466  loss=6.71e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 467  loss=6.67e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 468  loss=6.75e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 469  loss=6.73e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 470  loss=6.69e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 471  loss=6.69e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 472  loss=6.74e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 473  loss=6.67e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 474  loss=6.73e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 475  loss=6.74e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 476  loss=6.72e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 477  loss=6.70e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 478  loss=6.74e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 479  loss=6.67e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 480  loss=6.69e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 481  loss=6.81e+01 \n",
      "Test set: Average loss: 1.44e+01 \n",
      "Train Epoch: 482  loss=6.72e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 483  loss=6.73e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 484  loss=6.71e+01 \n",
      "Test set: Average loss: 1.40e+01 \n",
      "Train Epoch: 485  loss=6.70e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 486  loss=6.73e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 487  loss=6.73e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 488  loss=6.71e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 489  loss=6.72e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 490  loss=6.72e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 491  loss=6.78e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 492  loss=6.72e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 493  loss=6.77e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 494  loss=6.68e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 495  loss=6.67e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 496  loss=6.75e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 497  loss=6.66e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 498  loss=6.71e+01 \n",
      "Test set: Average loss: 1.34e+01 \n",
      "Train Epoch: 499  loss=6.67e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 500  loss=6.73e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 501  loss=6.76e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 502  loss=6.69e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 503  loss=6.72e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 504  loss=6.69e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 505  loss=6.69e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 506  loss=6.77e+01 \n",
      "Test set: Average loss: 1.48e+01 \n",
      "Train Epoch: 507  loss=6.69e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 508  loss=6.71e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 509  loss=6.70e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 510  loss=6.70e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 511  loss=6.72e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 512  loss=6.68e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 513  loss=6.71e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 514  loss=6.64e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 515  loss=6.71e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 516  loss=6.78e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 517  loss=6.65e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 518  loss=6.72e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 519  loss=6.59e+01 \n",
      "Test set: Average loss: 2.01e+01 \n",
      "Train Epoch: 520  loss=6.84e+01 \n",
      "Test set: Average loss: 1.43e+01 \n",
      "Train Epoch: 521  loss=6.85e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 522  loss=6.81e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 523  loss=6.77e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 524  loss=6.72e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 525  loss=6.76e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 526  loss=6.73e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 527  loss=6.69e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 528  loss=6.72e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 529  loss=6.73e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 530  loss=6.73e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 531  loss=6.66e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 532  loss=6.69e+01 \n",
      "Test set: Average loss: 1.24e+01 \n",
      "Train Epoch: 533  loss=6.75e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 534  loss=6.67e+01 \n",
      "Test set: Average loss: 1.26e+01 \n",
      "Train Epoch: 535  loss=6.70e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 536  loss=6.72e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 537  loss=6.69e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 538  loss=6.73e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 539  loss=6.68e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 540  loss=6.69e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 541  loss=6.70e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 542  loss=6.66e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 543  loss=6.75e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 544  loss=6.71e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 545  loss=6.70e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 546  loss=6.69e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 547  loss=6.70e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 548  loss=6.71e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 549  loss=6.70e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 550  loss=6.71e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 551  loss=6.66e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 552  loss=6.70e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 553  loss=6.71e+01 \n",
      "Test set: Average loss: 2.18e+01 \n",
      "Train Epoch: 554  loss=6.66e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 555  loss=6.71e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 556  loss=6.68e+01 \n",
      "Test set: Average loss: 1.41e+01 \n",
      "Train Epoch: 557  loss=6.71e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 558  loss=6.67e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 559  loss=6.73e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 560  loss=6.70e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 561  loss=6.65e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 562  loss=6.76e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 563  loss=6.73e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 564  loss=6.73e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 565  loss=6.68e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 566  loss=6.70e+01 \n",
      "Test set: Average loss: 2.11e+01 \n",
      "Train Epoch: 567  loss=6.67e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 568  loss=6.71e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 569  loss=6.72e+01 \n",
      "Test set: Average loss: 1.34e+01 \n",
      "Train Epoch: 570  loss=6.69e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 571  loss=6.71e+01 \n",
      "Test set: Average loss: 1.45e+01 \n",
      "Train Epoch: 572  loss=6.72e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 573  loss=6.69e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 574  loss=6.71e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 575  loss=6.68e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 576  loss=6.70e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 577  loss=6.72e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 578  loss=6.66e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 579  loss=6.72e+01 \n",
      "Test set: Average loss: 1.50e+01 \n",
      "Train Epoch: 580  loss=6.71e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 581  loss=6.69e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 582  loss=6.71e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 583  loss=6.68e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 584  loss=6.67e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 585  loss=6.66e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 586  loss=6.65e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 587  loss=6.66e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 588  loss=6.70e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 589  loss=6.65e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 590  loss=6.72e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 591  loss=6.64e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 592  loss=6.70e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 593  loss=6.64e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 594  loss=6.68e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 595  loss=6.67e+01 \n",
      "Test set: Average loss: 1.98e+01 \n",
      "Train Epoch: 596  loss=6.65e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 597  loss=6.68e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 598  loss=6.72e+01 \n",
      "Test set: Average loss: 1.89e+01 \n",
      "Train Epoch: 599  loss=6.69e+01 \n",
      "Test set: Average loss: 2.02e+01 \n",
      "Train Epoch: 600  loss=6.71e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 601  loss=6.66e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 602  loss=6.70e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 603  loss=6.69e+01 \n",
      "Test set: Average loss: 1.89e+01 \n",
      "Train Epoch: 604  loss=6.67e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 605  loss=6.73e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 606  loss=6.66e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 607  loss=6.66e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 608  loss=6.71e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 609  loss=6.67e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 610  loss=6.72e+01 \n",
      "Test set: Average loss: 1.58e+01 \n",
      "Train Epoch: 611  loss=6.74e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 612  loss=6.65e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 613  loss=6.69e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 614  loss=6.70e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 615  loss=6.68e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 616  loss=6.73e+01 \n",
      "Test set: Average loss: 1.47e+01 \n",
      "Train Epoch: 617  loss=6.75e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 618  loss=6.72e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 619  loss=6.71e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 620  loss=6.74e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 621  loss=6.70e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 622  loss=6.68e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 623  loss=6.71e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 624  loss=6.69e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 625  loss=6.69e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 626  loss=6.73e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 627  loss=6.74e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 628  loss=6.68e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 629  loss=6.72e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 630  loss=6.72e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 631  loss=6.69e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 632  loss=6.68e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 633  loss=6.67e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 634  loss=6.68e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 635  loss=6.69e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 636  loss=6.72e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 637  loss=6.70e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 638  loss=6.69e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 639  loss=6.72e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 640  loss=6.73e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 641  loss=6.65e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 642  loss=6.73e+01 \n",
      "Test set: Average loss: 2.02e+01 \n",
      "Train Epoch: 643  loss=6.71e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 644  loss=6.77e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 645  loss=6.67e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 646  loss=6.69e+01 \n",
      "Test set: Average loss: 1.80e+01 \n",
      "Train Epoch: 647  loss=6.67e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 648  loss=6.65e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 649  loss=6.70e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 650  loss=6.64e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 651  loss=6.66e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 652  loss=6.69e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 653  loss=6.63e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 654  loss=6.69e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 655  loss=6.64e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 656  loss=6.68e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 657  loss=6.69e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 658  loss=6.66e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 659  loss=6.70e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 660  loss=6.73e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 661  loss=6.74e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 662  loss=6.66e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 663  loss=6.64e+01 \n",
      "Test set: Average loss: 1.85e+01 \n",
      "Train Epoch: 664  loss=6.66e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 665  loss=6.65e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 666  loss=6.64e+01 \n",
      "Test set: Average loss: 1.88e+01 \n",
      "Train Epoch: 667  loss=6.65e+01 \n",
      "Test set: Average loss: 2.05e+01 \n",
      "Train Epoch: 668  loss=6.70e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 669  loss=6.69e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 670  loss=6.74e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 671  loss=6.70e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 672  loss=6.69e+01 \n",
      "Test set: Average loss: 1.13e+01 \n",
      "Train Epoch: 673  loss=6.87e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 674  loss=6.68e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 675  loss=6.73e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 676  loss=6.69e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 677  loss=6.70e+01 \n",
      "Test set: Average loss: 1.93e+01 \n",
      "Train Epoch: 678  loss=6.68e+01 \n",
      "Test set: Average loss: 1.83e+01 \n",
      "Train Epoch: 679  loss=6.67e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 680  loss=6.67e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 681  loss=6.70e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 682  loss=6.69e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 683  loss=6.72e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 684  loss=6.69e+01 \n",
      "Test set: Average loss: 1.98e+01 \n",
      "Train Epoch: 685  loss=6.68e+01 \n",
      "Test set: Average loss: 1.93e+01 \n",
      "Train Epoch: 686  loss=6.64e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 687  loss=6.67e+01 \n",
      "Test set: Average loss: 1.39e+01 \n",
      "Train Epoch: 688  loss=6.68e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 689  loss=6.70e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 690  loss=6.67e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 691  loss=6.70e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 692  loss=6.67e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 693  loss=6.64e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 694  loss=6.70e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 695  loss=6.65e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 696  loss=6.70e+01 \n",
      "Test set: Average loss: 1.97e+01 \n",
      "Train Epoch: 697  loss=6.65e+01 \n",
      "Test set: Average loss: 1.46e+01 \n",
      "Train Epoch: 698  loss=6.69e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 699  loss=6.65e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 700  loss=6.70e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 701  loss=6.68e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 702  loss=6.65e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 703  loss=6.71e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 704  loss=6.68e+01 \n",
      "Test set: Average loss: 1.75e+01 \n",
      "Train Epoch: 705  loss=6.69e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 706  loss=6.76e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 707  loss=6.66e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 708  loss=6.63e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 709  loss=6.70e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 710  loss=6.65e+01 \n",
      "Test set: Average loss: 1.81e+01 \n",
      "Train Epoch: 711  loss=6.67e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 712  loss=6.69e+01 \n",
      "Test set: Average loss: 1.97e+01 \n",
      "Train Epoch: 713  loss=6.66e+01 \n",
      "Test set: Average loss: 1.82e+01 \n",
      "Train Epoch: 714  loss=6.65e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 715  loss=6.68e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 716  loss=6.69e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 717  loss=6.68e+01 \n",
      "Test set: Average loss: 1.94e+01 \n",
      "Train Epoch: 718  loss=6.66e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 719  loss=6.74e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 720  loss=6.71e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 721  loss=6.76e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 722  loss=6.70e+01 \n",
      "Test set: Average loss: 1.52e+01 \n",
      "Train Epoch: 723  loss=6.71e+01 \n",
      "Test set: Average loss: 1.66e+01 \n",
      "Train Epoch: 724  loss=6.69e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 725  loss=6.67e+01 \n",
      "Test set: Average loss: 2.01e+01 \n",
      "Train Epoch: 726  loss=6.71e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 727  loss=6.71e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 728  loss=6.70e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 729  loss=6.67e+01 \n",
      "Test set: Average loss: 1.51e+01 \n",
      "Train Epoch: 730  loss=6.74e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 731  loss=6.65e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 732  loss=6.64e+01 \n",
      "Test set: Average loss: 1.59e+01 \n",
      "Train Epoch: 733  loss=6.64e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 734  loss=6.66e+01 \n",
      "Test set: Average loss: 2.13e+01 \n",
      "Train Epoch: 735  loss=6.75e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 736  loss=6.66e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 737  loss=6.69e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 738  loss=6.76e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 739  loss=6.68e+01 \n",
      "Test set: Average loss: 1.92e+01 \n",
      "Train Epoch: 740  loss=6.72e+01 \n",
      "Test set: Average loss: 1.79e+01 \n",
      "Train Epoch: 741  loss=6.69e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 742  loss=6.69e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 743  loss=6.66e+01 \n",
      "Test set: Average loss: 1.61e+01 \n",
      "Train Epoch: 744  loss=6.70e+01 \n",
      "Test set: Average loss: 1.60e+01 \n",
      "Train Epoch: 745  loss=6.72e+01 \n",
      "Test set: Average loss: 1.84e+01 \n",
      "Train Epoch: 746  loss=6.72e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 747  loss=6.68e+01 \n",
      "Test set: Average loss: 1.77e+01 \n",
      "Train Epoch: 748  loss=6.70e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 749  loss=6.66e+01 \n",
      "Test set: Average loss: 2.01e+01 \n",
      "Train Epoch: 750  loss=6.67e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 751  loss=6.69e+01 \n",
      "Test set: Average loss: 1.93e+01 \n",
      "Train Epoch: 752  loss=6.70e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 753  loss=6.75e+01 \n",
      "Test set: Average loss: 1.31e+01 \n",
      "Train Epoch: 754  loss=6.68e+01 \n",
      "Test set: Average loss: 1.87e+01 \n",
      "Train Epoch: 755  loss=6.69e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 756  loss=6.67e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 757  loss=6.69e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 758  loss=6.66e+01 \n",
      "Test set: Average loss: 1.86e+01 \n",
      "Train Epoch: 759  loss=6.68e+01 \n",
      "Test set: Average loss: 1.69e+01 \n",
      "Train Epoch: 760  loss=6.64e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 761  loss=6.65e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 762  loss=6.62e+01 \n",
      "Test set: Average loss: 1.76e+01 \n",
      "Train Epoch: 763  loss=6.71e+01 \n",
      "Test set: Average loss: 1.74e+01 \n",
      "Train Epoch: 764  loss=6.68e+01 \n",
      "Test set: Average loss: 1.70e+01 \n",
      "Train Epoch: 765  loss=6.68e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 766  loss=6.62e+01 \n",
      "Test set: Average loss: 1.55e+01 \n",
      "Train Epoch: 767  loss=6.72e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 768  loss=6.71e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 769  loss=6.64e+01 \n",
      "Test set: Average loss: 1.90e+01 \n",
      "Train Epoch: 770  loss=6.69e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 771  loss=6.74e+01 \n",
      "Test set: Average loss: 1.54e+01 \n",
      "Train Epoch: 772  loss=6.69e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 773  loss=6.68e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 774  loss=6.63e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 775  loss=6.70e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 776  loss=6.67e+01 \n",
      "Test set: Average loss: 2.01e+01 \n",
      "Train Epoch: 777  loss=6.70e+01 \n",
      "Test set: Average loss: 1.63e+01 \n",
      "Train Epoch: 778  loss=6.67e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 779  loss=6.66e+01 \n",
      "Test set: Average loss: 1.95e+01 \n",
      "Train Epoch: 780  loss=6.66e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 781  loss=6.66e+01 \n",
      "Test set: Average loss: 1.56e+01 \n",
      "Train Epoch: 782  loss=6.65e+01 \n",
      "Test set: Average loss: 1.62e+01 \n",
      "Train Epoch: 783  loss=6.66e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 784  loss=6.71e+01 \n",
      "Test set: Average loss: 1.71e+01 \n",
      "Train Epoch: 785  loss=6.69e+01 \n",
      "Test set: Average loss: 1.67e+01 \n",
      "Train Epoch: 786  loss=6.69e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 787  loss=6.67e+01 \n",
      "Test set: Average loss: 1.73e+01 \n",
      "Train Epoch: 788  loss=6.66e+01 \n",
      "Test set: Average loss: 1.31e+01 \n",
      "Train Epoch: 789  loss=6.64e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 790  loss=6.65e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 791  loss=6.64e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 792  loss=6.66e+01 \n",
      "Test set: Average loss: 1.64e+01 \n",
      "Train Epoch: 793  loss=6.65e+01 \n",
      "Test set: Average loss: 1.78e+01 \n",
      "Train Epoch: 794  loss=6.62e+01 \n",
      "Test set: Average loss: 1.72e+01 \n",
      "Train Epoch: 795  loss=6.67e+01 \n",
      "Test set: Average loss: 1.49e+01 \n",
      "Train Epoch: 796  loss=6.72e+01 \n",
      "Test set: Average loss: 1.68e+01 \n",
      "Train Epoch: 797  loss=6.66e+01 \n",
      "Test set: Average loss: 1.65e+01 \n",
      "Train Epoch: 798  loss=6.69e+01 \n",
      "Test set: Average loss: 1.53e+01 \n",
      "Train Epoch: 799  loss=6.66e+01 \n",
      "Test set: Average loss: 1.57e+01 \n",
      "Train Epoch: 800  loss=6.70e+01 \n",
      "Test set: Average loss: 1.93e+01 \n"
     ]
    }
   ],
   "source": [
    "#train model and preditct\n",
    "train_rho_history = []\n",
    "train_loss_history = []\n",
    "test_rho_history = []\n",
    "test_loss_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss , rho_train = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    test_loss, rho_test = test_epoch(\n",
    "        model, criterion, val_dl\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_rho_history.append(rho_train)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_rho_history.append(rho_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f27fa617280>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-Loss-2pool.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f27e61f0fa0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8oUlEQVR4nO3dd3hT1f8H8HeSNukedBcKhbJXwRZKQQSkUoYgiAiIUkBxAIKCA0UBJw5kqAgu4Ccu0C8gKrsMBZG99y4CXZTunZzfHyVpbpO0aZs2aXi/nqcPzb3n3pybhN5Pzvmcc2RCCAEiIiIiOyG3dgWIiIiILInBDREREdkVBjdERERkVxjcEBERkV1hcENERER2hcENERER2RUGN0RERGRXGNwQERGRXWFwQ0RERHaFwQ0R1ZgdO3ZAJpPh119/tXZVatTs2bMhk8ksek7ta7djxw6LnteWnT9/Hn369IGnpydkMhnWrl0LANi/fz+6du0KV1dXyGQyHDlypMqvec+ePdGzZ0/LVpxsDoMbqhEpKSmYMmUKWrZsCWdnZ/j7+6Nz58549dVXkZ2dbe3q1TnaG92VK1esXRWysC+++ALLly+3djVsQlxcHI4fP4733nsPK1asQGRkJIqKijBs2DCkpaVh/vz5WLFiBRo1amTtqpbrn3/+wezZs5Genm7tqty1HKxdAbI/aWlpiIyMRGZmJsaNG4eWLVvi1q1bOHbsGBYvXoznnnsObm5u1q4mkU344osv4OvrizFjxki233fffcjLy4NSqbROxWpZXl4e9uzZgxkzZmDSpEm67WfOnMHVq1fx9ddf46mnntJtf+ONNzB9+vRKP8/mzZstUt/y/PPPP3jrrbcwZswYeHl51fjzkSEGN2Rx3377LRISErB792507dpVsi8zM9Nm/1gLIZCfnw9nZ2drV6XW5ebmwsXFxdrVqFUajQaFhYVwcnIy2JeTkwNXV1cr1KqUXC43Wjd7lZKSAgAGwUBycrLR7Q4ODnBwqPwtzFb//pBlsVuKLO7ixYtQKBTo0qWLwT4PDw/JH+yePXuibdu2OHjwILp27QpnZ2c0btwYS5YsMTi2oKAAs2bNQtOmTaFSqRASEoJXXnkFBQUFknLLli3D/fffD39/f6hUKrRu3RqLFy82OF9oaCgefPBBbNq0CZGRkXB2dsaXX36p6wJatWoV3nrrLdSvXx/u7u545JFHkJGRgYKCArzwwgvw9/eHm5sbxo4dW+067Nq1C507d4aTkxOaNGmC7777rsLX+fz58xg6dCgCAwPh5OSEBg0aYMSIEcjIyCj3OP3X/L777oOLiwtef/11AMBvv/2GAQMGIDg4GCqVCmFhYXjnnXegVquNnuPUqVPo1asXXFxcUL9+fXz00UcV1rugoAAPPvggPD098c8//5RbNj8/H7Nnz0bz5s3h5OSEoKAgPPzww7h48aKuTE5ODqZNm4aQkBCoVCq0aNECc+fOhRBCci6ZTIZJkybhhx9+QJs2baBSqbBx40YsX74cMpkMO3fuxIQJE+Dv748GDRrojtuwYQO6d+8OV1dXuLu7Y8CAATh58mSF12nOZyA0NBQnT57Ezp07IZPJIJPJdPkgpnJufvnlF0RERMDZ2Rm+vr54/PHHcf36dUmZMWPGwM3NDdevX8fgwYPh5uYGPz8/vPTSSwbvpSkbNmxAjx494O7uDg8PD3Tq1Ak//vhjpesClLS+PPLII6hXrx6cnJwQGRmJdevW6fbPnj1b19X08ssvQyaTITQ0FGPGjEGPHj0AAMOGDZO8PqZybr7//nt07twZLi4u8Pb2xn333SdprTGWc2Pu3xbtZ2jt2rVo27YtVCoV2rRpg40bN0qu5eWXXwYANG7cWPe+sku5drHlhiyuUaNGUKvVWLFiBeLi4iosf/v2bfTv3x+PPvooRo4ciVWrVuG5556DUqnEuHHjAJR8yx40aBB27dqFp59+Gq1atcLx48cxf/58nDt3Tpd4CACLFy9GmzZtMGjQIDg4OOD333/HhAkToNFoMHHiRMlznz17FiNHjsQzzzyD8ePHo0WLFrp9c+bMgbOzM6ZPn44LFy7gs88+g6OjI+RyOW7fvo3Zs2fj33//xfLly9G4cWPMnDmzSnW4cOECHnnkETz55JOIi4vD0qVLMWbMGERERKBNmzZGX7PCwkLExsaioKAAzz//PAIDA3H9+nX88ccfSE9Ph6enZ7mv+a1bt9CvXz+MGDECjz/+OAICAgAAy5cvh5ubG6ZOnQo3Nzds27YNM2fORGZmJj7++GOD961v3754+OGH8eijj+LXX3/Fq6++inbt2qFfv35GnzcvLw8PPfQQDhw4gK1bt6JTp04m66hWq/Hggw8iPj4eI0aMwJQpU5CVlYUtW7bgxIkTCAsLgxACgwYNwvbt2/Hkk0+iQ4cO2LRpE15++WVcv34d8+fPl5xz27ZtWLVqFSZNmgRfX1+EhobiyJEjAIAJEybAz88PM2fORE5ODgDoPsOxsbH48MMPkZubi8WLF+Pee+/F4cOHERoaarL+5nwGFixYgOeffx5ubm6YMWMGAOjeC2OWL1+OsWPHolOnTpgzZw6SkpKwcOFC7N69G4cPH5a0bqjVasTGxiIqKgpz587F1q1b8cknnyAsLAzPPfecyefQPs+4cePQpk0bvPbaa/Dy8sLhw4exceNGPPbYY5Wqy8mTJ9GtWzfUr18f06dPh6urK1atWoXBgwfjf//7H4YMGYKHH34YXl5eePHFFzFy5Ej0798fbm5uCAgIQP369fH+++9j8uTJ6NSpU7mvz1tvvYXZs2eja9euePvtt6FUKrF3715s27YNffr0MXpMZf62AMCuXbuwevVqTJgwAe7u7vj0008xdOhQJCQkwMfHBw8//DDOnTuHn376CfPnz4evry8AwM/Pr9zXnCxMEFlYYmKi8PPzEwBEy5YtxbPPPit+/PFHkZ6eblC2R48eAoD45JNPdNsKCgpEhw4dhL+/vygsLBRCCLFixQohl8vF33//LTl+yZIlAoDYvXu3bltubq7B88TGxoomTZpItjVq1EgAEBs3bpRs3759uwAg2rZtq3t+IYQYOXKkkMlkol+/fpLy0dHRolGjRpJtla3DX3/9pduWnJwsVCqVmDZtmsE5tA4fPiwAiF9++cVkGVO0r/mSJUsM9hmr9zPPPCNcXFxEfn6+wTm+++473baCggIRGBgohg4dqtumfS1/+eUXkZWVJXr06CF8fX3F4cOHK6zn0qVLBQAxb948g30ajUYIIcTatWsFAPHuu+9K9j/yyCNCJpOJCxcu6LYBEHK5XJw8eVJSdtmyZQKAuPfee0VxcbFue1ZWlvDy8hLjx4+XlE9MTBSenp6S7bNmzRJl/5ya+xlo06aN6NGjh0FZ7Wu3fft2IYQQhYWFwt/fX7Rt21bk5eXpyv3xxx8CgJg5c6ZuW1xcnAAg3n77bck5O3bsKCIiIgyeS196erpwd3cXUVFRkucRovR1r0xdevfuLdq1ayf5/Gg0GtG1a1fRrFkz3bbLly8LAOLjjz82+jqU/ayXfc3Pnz8v5HK5GDJkiFCr1UbrLUTJZ1f/9a7M3xYAQqlUSj5XR48eFQDEZ599ptv28ccfCwDi8uXLgqyD3VJkcQEBATh69CieffZZ3L59G0uWLMFjjz0Gf39/vPPOOwbdBQ4ODnjmmWd0j5VKJZ555hkkJyfj4MGDAEqav1u1aoWWLVsiNTVV93P//fcDALZv3647Xj9nJiMjA6mpqejRowcuXbpk0GXTuHFjxMbGGr2O0aNHw9HRUfc4KioKQghda5L+9mvXrqG4uLhKdWjdujW6d++ue+zn54cWLVrg0qVLRusFQNcys2nTJuTm5posZ4pKpcLYsWMNtuvXOysrC6mpqejevTtyc3Nx5swZSVk3Nzc8/vjjusdKpRKdO3c2Wu+MjAz06dMHZ86cwY4dO9ChQ4cK6/i///0Pvr6+eP755w32absj1q9fD4VCgcmTJ0v2T5s2DUIIbNiwQbK9R48eaN26tdHnGz9+PBQKhe7xli1bkJ6ejpEjR0o+cwqFAlFRUZLPnDGV+QyY48CBA0hOTsaECRMkXbsDBgxAy5Yt8eeffxoc8+yzz0oed+/evdzPFVBy3VlZWZg+fbpBzo/2dTe3Lmlpadi2bRseffRR3ecpNTUVt27dQmxsLM6fP2+0G6sq1q5dC41Gg5kzZ0Iul97ayhsyXpm/LQAQExODsLAw3eP27dvDw8OjwteVahe7pahGBAUFYfHixfjiiy9w/vx5bNq0CR9++CFmzpyJoKAgyaiH4OBgg+TN5s2bAwCuXLmCLl264Pz58zh9+rTJpl1t0iEA7N69G7NmzcKePXsMbvwZGRmSLpvGjRubvIaGDRtKHmuPCwkJMdiu0WiQkZEBHx+fSteh7PMAgLe3N27fvm2ybo0bN8bUqVMxb948/PDDD+jevTsGDRqExx9/vMIuKQCoX7++0cTKkydP4o033sC2bduQmZlpUG99DRo0MLhpeHt749ixYwbnfeGFF5Cfn4/Dhw+b7Gor6+LFi2jRokW5SaNXr15FcHAw3N3dJdtbtWql26+vvPe77L7z588DgO4mV5aHh4fpyqNynwFzaK9Fv+tUq2XLlti1a5dkm5OTk8H/l4o+VwB0+Uxt27atdl0uXLgAIQTefPNNvPnmm0bPlZycjPr165dbJ3NcvHgRcrncZPBqSmX+tgBV+/9KtY/BDdUomUyG5s2bo3nz5hgwYACaNWuGH374QRLcmEOj0aBdu3aYN2+e0f3agOPixYvo3bs3WrZsiXnz5iEkJARKpRLr16/H/PnzodFoJMeVNzJK/1u8Odu1LVKVrUNF5zPlk08+wZgxY/Dbb79h8+bNmDx5MubMmYN///1XkhBrjLHrTk9PR48ePeDh4YG3334bYWFhcHJywqFDh/Dqq69Wq94PPfQQfv75Z3zwwQf47rvvDL5Z15by3u+y+7TXu2LFCgQGBhqULy/oquxnoCaYen9qk/Y6X3rpJZMtpE2bNq3NKhkw92+LVlX/v1LtYnBDtaZJkybw9vbGzZs3Jdtv3LhhMPT23LlzAKBL2AwLC8PRo0fRu3fvcpuYf//9dxQUFGDdunWSb1gVdSFYUm3WoV27dmjXrh3eeOMN/PPPP+jWrRuWLFmCd999t9Ln2rFjB27duoXVq1fjvvvu022/fPlytes5ePBg9OnTB2PGjIG7u7vRkWNlhYWFYe/evSgqKpJ0D+pr1KgRtm7diqysLEnrjbYLrTqTvWm7Hvz9/RETE1OpYyvzGTB3ll3ttZw9e9agNens2bMWm9hOe90nTpwwGXiYW5cmTZoAABwdHSv9GlZWWFgYNBoNTp06ZVa3p/5x5vxtqQxLz1ZNlcecG7K4vXv36kab6Nu3bx9u3bpl0JRdXFyML7/8Uve4sLAQX375Jfz8/BAREQEAePTRR3H9+nV8/fXXBufNy8vTPZ/2W5X+t6iMjAwsW7as+hdmptqoQ2ZmpiTHBygJdORyucHwVXMZq3dhYSG++OKLqldUz+jRo/Hpp59iyZIlePXVVyssP3ToUKSmpuLzzz832KetY//+/aFWqw3KzJ8/HzKZzOSoLXPExsbCw8MD77//PoqKigz2a+dlMaYynwFXV1ezZrKNjIyEv78/lixZInmPN2zYgNOnT2PAgAEVnsMcffr0gbu7O+bMmYP8/HzJPu31mFsXf39/9OzZE19++aXBlxqg/NewsgYPHgy5XI63337boGWsvFYVc/+2VIb2ixpnKLYettyQxa1YsQI//PADhgwZgoiICCiVSpw+fRpLly6Fk5OTbk4VreDgYHz44Ye4cuUKmjdvjpUrV+LIkSP46quvdN/Yn3jiCaxatQrPPvsstm/fjm7dukGtVuPMmTNYtWqVbq6aPn36QKlUYuDAgXjmmWeQnZ2Nr7/+Gv7+/kb/uNaE2qjDtm3bMGnSJAwbNgzNmzdHcXExVqxYAYVCgaFDh1bpnF27doW3tzfi4uIwefJkyGQyrFixwqLN7ZMmTUJmZiZmzJgBT09Pg8+CvtGjR+O7777D1KlTsW/fPnTv3h05OTnYunUrJkyYgIceeggDBw5Er169MGPGDFy5cgXh4eHYvHkzfvvtN7zwwguSxM/K8vDwwOLFi/HEE0/gnnvuwYgRI+Dn54eEhAT8+eef6Natm9HAC6jcZyAiIgKLFy/Gu+++i6ZNm8Lf399ono+joyM+/PBDjB07Fj169MDIkSN1w69DQ0Px4osvVvlay173/Pnz8dRTT6FTp0547LHH4O3tjaNHjyI3Nxf/93//V6m6LFq0CPfeey/atWuH8ePHo0mTJkhKSsKePXvw33//4ejRoxapd9OmTTFjxgy888476N69Ox5++GGoVCrs378fwcHBmDNnjtHjzP3bUhnaL2UzZszAiBEj4OjoiIEDB1p9Ysi7Su0P0CJ7d+zYMfHyyy+Le+65R9SrV084ODiIoKAgMWzYMHHo0CFJ2R49eog2bdqIAwcOiOjoaOHk5CQaNWokPv/8c4PzFhYWig8//FC0adNGqFQq4e3tLSIiIsRbb70lMjIydOXWrVsn2rdvL5ycnERoaKj48MMPdcOK9YdmNmrUSAwYMMDgeUwNPdUOGd6/f79ku3ZIakpKisXqUHa4almXLl0S48aNE2FhYcLJyUnUq1dP9OrVS2zdutXkMfrnbtOmjdF9u3fvFl26dBHOzs4iODhYvPLKK2LTpk2SIcnlnSMuLk4yLN7Ua/nKK68IAEbfZ325ublixowZonHjxsLR0VEEBgaKRx55RFy8eFFXJisrS7z44osiODhYODo6imbNmomPP/5YMvxXiJJhvBMnTjR4DlPvq/41xMbGCk9PT+Hk5CTCwsLEmDFjxIEDB3RljA0FN/czkJiYKAYMGCDc3d0FAN37XnYouNbKlStFx44dhUqlEvXq1ROjRo0S//33n6RMXFyccHV1NbgWY/U0Zd26daJr167C2dlZeHh4iM6dO4uffvqp0nURQoiLFy+K0aNHi8DAQOHo6Cjq168vHnzwQfHrr7/qylR3KLjW0qVLdXXy9vYWPXr0EFu2bNHtN/Z/y9y/LaY+Q40aNRJxcXGSbe+8846oX7++kMvlHBZuBTIhmAVF1tOzZ0+kpqbixIkT1q4KERHZCebcEBERkV1hcENERER2hcENERER2RXm3BAREZFdYcsNERER2RUGN0RERGRX7rpJ/DQaDW7cuAF3d3dOkU1ERFRHCCGQlZWF4ODgCtenu+uCmxs3bhgshEZERER1w7Vr1ypcHPiuC260i+tdu3YNHh4eVq4NERERmSMzMxMhISGSRXJNueuCG21XlIeHB4MbIiKiOsaclBImFBMREZFdYXBDREREdoXBDREREdmVuy7nxlxqtRpFRUXWrgZVwNHREQqFwtrVICIiG8LgpgwhBBITE5Genm7tqpCZvLy8EBgYyHmLiIgIAIMbA9rAxt/fHy4uLrxh2jAhBHJzc5GcnAwACAoKsnKNiIjIFjC40aNWq3WBjY+Pj7WrQ2ZwdnYGACQnJ8Pf359dVERExIRifdocGxcXFyvXhCpD+34xR4qIiAAbCW4WLVqE0NBQODk5ISoqCvv27TNZdvny5ZDJZJIfJycni9aHXVF1C98vIiLSZ/XgZuXKlZg6dSpmzZqFQ4cOITw8HLGxsbo8CmM8PDxw8+ZN3c/Vq1drscZERERky6we3MybNw/jx4/H2LFj0bp1ayxZsgQuLi5YunSpyWNkMhkCAwN1PwEBAbVY47tDaGgoFixYYPHzLl++HF5eXhY/LxERkZZVg5vCwkIcPHgQMTExum1yuRwxMTHYs2ePyeOys7PRqFEjhISE4KGHHsLJkydNli0oKEBmZqbkxx717NkTL7zwgsXOt3//fjz99NMWOx8REVFtsWpwk5qaCrVabdDyEhAQgMTERKPHtGjRAkuXLsVvv/2G77//HhqNBl27dsV///1ntPycOXPg6emp+wkJCbH4ddQVQggUFxebVdbPz8/iidWFhYUWPR8RmabRCOQXqa1dDSKrsHq3VGVFR0dj9OjR6NChA3r06IHVq1fDz88PX375pdHyr732GjIyMnQ/165dq+Ua17wxY8Zg586dWLhwoS7J+sqVK9ixYwdkMhk2bNiAiIgIqFQq7Nq1CxcvXsRDDz2EgIAAuLm5oVOnTti6davknGW7pWQyGb755hsMGTIELi4uaNasGdatW1duvUJDQ/HOO+9g9OjR8PDwkLQEbdq0Ca1atYKbmxv69u2Lmzdv6vZpNBq8/fbbaNCgAVQqFTp06ICNGzda5sUiuks89s2/aD1zI27n8EsF3X2sGtz4+vpCoVAgKSlJsj0pKQmBgYFmncPR0REdO3bEhQsXjO5XqVTw8PCQ/FSGEAK5hcVW+RFCmFXHhQsXIjo6GuPHj9clWeu3UE2fPh0ffPABTp8+jfbt2yM7Oxv9+/dHfHw8Dh8+jL59+2LgwIFISEgo93neeustPProozh27Bj69++PUaNGIS0trdxj5s6di/DwcBw+fBhvvvkmACA3Nxdz587FihUr8NdffyEhIQEvvfSS5Ho++eQTzJ07F8eOHUNsbCwGDRqE8+fPm/V6EBHw76U0aASw5VRSxYWJ7IxVJ/FTKpWIiIhAfHw8Bg8eDKDkW3t8fDwmTZpk1jnUajWOHz+O/v3710gd84rUaD1zU42cuyKn3o6Fi7Lit8jT0xNKpRIuLi5Gg8K3334bDzzwgO5xvXr1EB4ernv8zjvvYM2aNVi3bl25r/uYMWMwcuRIAMD777+PTz/9FPv27UPfvn1NHnP//fdj2rRpusd///03ioqKsGTJEoSFhQEAJk2ahLfffltXZu7cuXj11VcxYsQIAMCHH36I7du3Y8GCBVi0aFFFLwcR6VHIOVUC3X2sPkPx1KlTERcXh8jISHTu3BkLFixATk4Oxo4dCwAYPXo06tevjzlz5gAouVF36dIFTZs2RXp6Oj7++GNcvXoVTz31lDUvw6ZFRkZKHmdnZ2P27Nn4888/cfPmTRQXFyMvL6/Clpv27dvrfnd1dYWHh0e5Q/aNPTdQMumeNrABSpZN0J4nMzMTN27cQLdu3STHdOvWDUePHi33uYjIEIMbuhtZPbgZPnw4UlJSMHPmTCQmJuryK7RJxgkJCZDLS3vPbt++jfHjxyMxMRHe3t6IiIjAP//8g9atW9dI/ZwdFTj1dmyNnNuc57YEV1dXyeOXXnoJW7Zswdy5c9G0aVM4OzvjkUceqTDh19HRUfJYJpNBo9FU6rlNncfcLjgiqhw5gxu6C1k9uAFKuiVMdYfs2LFD8nj+/PmYP39+LdSqhEwmM6tryNqUSiXUavNGRuzevRtjxozBkCFDAJS05Fy5cqUGa2c+Dw8PBAcHY/fu3ejRo4du++7du9G5c2cr1oyoblJwBm+6C9n+XZvMEhoair179+LKlStwc3NDvXr1TJZt1qwZVq9ejYEDB0Imk+HNN9+ssAWmNr388suYNWsWwsLC0KFDByxbtgxHjhzBDz/8YO2qEdksIQT+PH4TbYM9EVKvdBoHRZ0bE0tUfQxu7MRLL72EuLg4tG7dGnl5ebh8+bLJsvPmzcO4cePQtWtX+Pr64tVXX7WpyQ0nT56MjIwMTJs2DcnJyWjdujXWrVuHZs2aWbtqRDZr86kkTPrxMADgzDulSf5yttxQDbmdUwhvV6W1q2GUTNxlyQ6ZmZnw9PRERkaGwbDw/Px8XL58GY0bN7b4YpxUc/i+EQHvrz+Nr/66BAA4PrsP2s3eDAD4Ni4SvVtxiRpzZeQVYfnuK3ioQzBCfQ1zBqnEj3sT8Pqa43hjQCs81b1JrTxneffvsthgSURkB/RbaIrVpd9ZOVqqYmk5hej58XbM23wWb6w9gflbz+GRJaaXAKqIOW0GqdkFuJVdoHtcrNZAo5Ee98+FVOw4W/6I1NqQX6TG/itpKFaXpi+8vuY4AODdP09bq1rlYnBDRGQHHPSCmDwuu1Ap3/x9CVdu5eLTbRd0wUSqXuABmBewAMD3/15F+FubcfBq6QSnW08lIer9rdh9IRVASbAQ+e5WRLy7FcVqDQqLNYiZtxPDvyoNqC4kZ+Oxb/ZizLL9yMwvMvpcBcVq7L10C4XF0pzJvMLy3//1x29izWHjSxYZ8/qa4xi2ZA8Wbb9o9jHWxuCGiMgO6DfQpOktuVDXEw8OXElDYka+7rEQwuTNXp9aY/6F5+oFA/qBgnZtrkk/HkLMvJ1mrdX1xtoTyMwvxkcbzwIAPt92Hk99dwBJmQUY9c1e/H70BpIyS68np1CNc0lZuHIrF/uv3EZuYTFSsgoQM2+nroypJTTmrD+D4V/9i/lbz+m2bT2VhDazNmLFv1eNHlOk1mDCD4fw4sqjkte1rPwiNbILStYiXH3oOgBg0Q7jKwFoaTQCf51LQU6BeWsY1iQGN0REdkD/Bn38eobud40NRjerD/2Hz+IrXk7lUMJtPLJkD/rML73Rv/fnabSfvRn7rxgu/ZKVX4T315/GlJ8Po/3sTTj2X3qFz/HbketY/s8V3eMCveBmxFf/Qq0R+OPYTVxMycGei7fKPddWvaUulA5ypGYXYO7mc5Iyz/90GJtPlpab+dsJPPjZLt3jpMwCnL4pHeBxO9cwmFNrhK7ei3eUtqiMX3EAGgG8ufaE0Tpm5JWeq8uceF3X2PYzyfj+TkAkhMCAT/9Gtw+2IbewNFApLNbg3T9OGX3tAeCHfQkYvXQfpq2y/oSrDG6IiOxAtt635ddWH9f9XokGDLMVFmswbvl+LNpe/jd5Y4QQmLrqKD7Zcg5nEk2P0rydU4iv7yRIZ+aXXts3u0pGgs5Zb5jr8cnmc/jqr0v47cgN5BSq8er/jhuUKWvKz0dM7jtyLR2z153UPda23GSZaDn6eNNZ3e/OjgrsvWQ8CFh75Lru99+O3JDsS8rMN2h1up1r2HLzvpHrBypuqdMmnWsN+HQXdp1Pxdjl+/HG2hM4lHAbWQXFuJiSg4y8Ipy+mSUp/82uyxhWJh8pOSsfB6+m4Ys7n4eNJxOtPjErgxsiojri1I1MSZeGviwTXQGV6Z4x14YTN7HtTLLkZq4lhDBIjN1yKglf7rxY0qWUV1pPbW7I6ZuZuP+THfj9aOmNPnbBX9hwIlH3uOw5U7ML8d2eK5LX4+SNDEmZguLyu5Eq2g9A0r2TnFWAFf9eRbvZm/HLgWuScum5hTibVBoIpGYXSB7rK9syo+/otXRsLrPYabqR4ObbXaan+zDl1I1Mg+AmMTMfj3+7V/f44S/+wekbpfW7VSb3yJjO78Vj6OI9uKnXzXUxJbvS9bMkznNDRFQHXEvLRf9P/wYAXPlggG77watpmL3uFC6ZuJnUxDdo/YBCCAGZ3kitJ77dh6TMfPw5uTuUDnLkFaox/rsDAEpafCJDSycY1QZeL/1yFJdScvD8T4fRKsgDP+1LQHKW9KaaVVAsuZaEtFzM/O0kZq87idHRoWjk44LUbGkQUFAkTbTdfDIRcpkMMa1LhsZn51cuNyQxM1/XBfTyr8cwLDJEt08/RwYALqfm4FBCutHzlBdvztlwxmDb7ZySlqJF2y9AIZfh2R5hBmU0GmGw1EZWfhHSc4ugdJDj1I1MvPPnKdNPrGf4V//qfv/vdp5Zx5R17XYemvq7V+lYS2BwQ0RUB+i3SuQXqeHkqEBqdgGGLi5/yLKxG6kQAoVqDdQaUe7yMpdSsnErpxCr9l/D/S39ERHqjflbziMlqzS4KSjWwOnOOng5BcXYdWdE0MkbGejY0Bs7z6Xoyn6yRZp/kluoxrH/0nFSr6Xg4S92S7qhtDaeuGm0m0kjIMmZ0VeoN3R53+U0PL3iIADg9Nt94axUSLryzJGk1zLhqCgNJPKL1AaBlbE8mapKzy1EWk6hrqWskd4M1Fq3cgrh566SbHtk8R5cTMlGcTVa78prZSpPSlbFLT41icENEZGFFak1OJeUhVaBHhZbuFKht4Dwf7fz4KxUoNsH2yo8Tl2m5aawWIPBi3bj1M1MeLk44t/XeuuCE31CCNz/SWlrxC8HjQ8dzswv0h1/M6P0W742T+RUOTfH0Uv3wUUpfW5jgQ0As/JnyirQG92k342Uml2AzPwivGEi6dYU/W4XD6fSBYCvpeVWum6V8c2uy3gwPFj3+LkfDhmUScsphLLMWhumusUAwNPZUZJcbIp+cnplWDu4Yc4N1YgdO3ZAJpMhPT3d2lUh0hFC4NVfj+GlX47WaMLjy78cxYBPd1UpL8KURL2uoGtpudikl49SnrLXefS/dF3AkZ5bhINXb2P3hVQIIfDPxVQM/GwXVu2/ZtASYUrWnWBk1/lUDPi0dNTPzN9OQgiBtJzyb3K5FczJUh2Z+cXIyCvCou0XJMFZanYBBn62C4dNdBuZot8d5+FcEtycTcySdOMAkARsHzzczuzz927pj8m9S5eZaR7gBqDkNTKWQB3dxAdhfiWzKN/KKcDO8ykGZUzJK1JjWEQDybYOIV4G5c4kmg6QypNsIjestjC4sRM9e/bECy+8YNFzjhkzBoMHD7boOYnKSs8txLYzSZLZT2vKieuZWHngGn49+J9kLhhLeO/PU3ht9XEIIbD2zgiYhfHncT4pCz/vS5Akr+YXqfHol3skI3EA4KONZ/DG2uOSUUS3cwox/rsDkqG9CWm5cFOZ1/Cu1gjJfCZl52oZ9c1ejPpmL/44dhOPfb0Xx69nYMW/V5GQlmPW+Z/+7gDOJGbi8W/3SoZR/3c7Dwvjz+PWnSDprUFtMO2B5madU8vZUYHOjU0vAmyOST8eMkh8Xv7PFbNHkb0zuC2WjekEQBpgejiVvP6DPt9l8FnSfx0CPc1fEmZKTDNMfaA5nr6vCRwVMiwc0VF3/dvPGgYuC0d0gI9rSVdUWk5hpWYzbuzjig+GtkcTvSUmnrmvCYbfySN6IcZwLb+H76mPJY9H4Jn7mmB6v5a67R8ONQzgyuZM1TZ2S5HFFRZa9qZB9m3Msv04ci0dbw1qg7iuoTX6XFtOl45CyS+2XDBVUKzG13+XtNLEdW2k255dUIwH5v8FoGR47PKxnQGUjDbadzkN+y6nYfagNgCA6+l5+OJOsur3/ybg39d6I9DTCW/8dgJbyoyeSUjLRQNvZ7Pq9u6fp5GWU4h5j4bj4Xsa6IKNsp7/6bDu98z8IlxJNa+r5WJKDvou+NvovgVbS+eyqeeqNBmQTe7dDA5yGeaVyclp4ueKEG8X7LtsfEi1Of4+n2qwrezw644NvVCk1uDEdWkX2pP3NsYTXRrpAkL9ViYPZ0fkFaolgYyW/gi1yixc2jqoZL2k1/q1xEt9WkDpIEfvlv5Gr39k54bw93BCvTsLV07/33GTOUQtA93xTVwk+sz/C37uKoT5ueG1fi2hkMvQrakvLqXmwM9dhT5tAhHbJhBT+zRHXqFa8v4NCg/GvEc7AAD6tg0EAHxwJ/m5a5ivwXNaO7hhy40dGDNmDHbu3ImFCxdCJpNBJpPhypUrAIATJ06gX79+cHNzQ0BAAJ544gmkppb+Z//111/Rrl07ODs7w8fHBzExMcjJycHs2bPxf//3f/jtt99059yxY4fR5+/ZsycmTZqEF154Ab6+voiNjdXtO3jwICIjI+Hi4oKuXbvi7FnpN6jFixcjLCwMSqUSLVq0wIoVKyz++pBtO3ItHQCw5vB1CCHw+9EbuJJqXquBvgNX0nDZyHF5hWpk3Enu1G8qN2e2WXPpD2++lmZ8dMkOvW/eF5JLRzZpW3QKytRHm8ipnbJfX0JartkJq9pWBe0aQObkQjgq5LiUanoo76/PRpv13Pp83JQI8jJsxQjwUOHxLg0xuXczRDTyluxzd3LAU90bV/q5Kis5swBFxYZNOdoA0slRAS8XR8m+6+l5aDVzo8ExD7YPQtSd1pbGvq4G+UxNjCzGGdsmAG8NagOHOzkzMpkMSoeS30dGNZSUHRQejL9f6YXZg1oDAOq5lQQ35SVHuygVaODtgiMz+2DbtJ5YOqYTmgWUjGSaEtMMHz3SHtum9YBCLoNcLkOAh5NBcvK4ew3fhw1TuuOn8V0QUibBeUC7IPRs7meyPrWBwU1FhAAKc6zzY2ZOwMKFCxEdHY3x48fj5s2buHnzJkJCQpCeno77778fHTt2xIEDB7Bx40YkJSXh0UcfBQDcvHkTI0eOxLhx43D69Gns2LEDDz/8MIQQeOmll/Doo4+ib9++unN27drVZB3+7//+D0qlErt378aSJUt022fMmIFPPvkEBw4cgIODA8aNG6fbt2bNGkyZMgXTpk3DiRMn8Mwzz2Ds2LHYvn17Fd8sskVCCDz1fwfw6Jd7DOYq0c8H8XR2xIYTiXj+p8PoOXdHpZ7jZkYehn25B73m7sD3/15FkV4XV4+PtyP87c3ILiiWfMu2RHCjnapffzkAc+b3uHKrtFVk0faLSM7KN1gPSltXY+sEJdzKNTr3SXmLZGqDp+SsinMh8grVOHj1tsn9FXW1+N654erzcVUhwMPwuG3TesLfvWT7rIGtEaO3grmHkyNaBXmgmb+byecyt3uuPDcz8jCycwhkstLWE0Da6hLsKW0pu5RiGEgPi2iAhSM6Yt7wDoiLboRlYzohspE37m1a2rLRPMBwePSXT0SabLX0cHJEXHRpa6CXiyNC6rlA5VA6Qq0irndeI6WD3OAz4uumwqORIXDXS5DWPwYoaSUylo/TKsgD0WE+BtsXjboHz+vlDlkDu6UqUpQLvB9ccbma8PoNQGkY5Zfl6ekJpVIJFxcXBAYG6rZ//vnn6NixI95//33dtqVLlyIkJATnzp1DdnY2iouL8fDDD6NRo5L/PO3alfadOjs7o6CgQHJOU5o1a4aPPvpI9/jmzZsAgPfeew89evQAAEyfPh0DBgxAfn4+nJycMHfuXIwZMwYTJkwAAEydOhX//vsv5s6di169elX4nFQ3ZBcUY+ud7qCEtFyE6n1z1W+6dlM56IYRm2vvpVvIK1LDw9lR913gjbUnkJFXhIm9mqKwWKN7jtM3M8vkvVSuW6rsfC5HrqVj8KLdAACVQ+n3xA+MzFOitf74TfRvF4Srt0pvjJ/Gn8fiHRfwf+M6S8p+t+cKXlh52Gi3R0JaLpreSTYdHhmClXdGAjnIZSYn7dMOizanuyA9txC37iQC//x0F4wokzAb6OEEZ0eFLiArO/LmxQeaY8Ya6Uikeq5K+LgqcW9TX8n7rH8Tbd/AC28/1Eb3edEm7UY1qYfzycaDxgNvxKDlm4YtKAAQ2cgbB+4EaTGt/HFfcz/M/K00z8lVqUBOoRoN67lgTLfGGNA+GC5KBdrM2gQAkpFuE3qFYdKPh1GeRj4uUMhlqO/ljLceaqvbvuLJzmj82noAQFzXUGw8aV4yuJanc2ng4eUsDULaN/Ay6Gbb+EJ3SVeh/uezKmJa+Ztd1lhgaw1subFjR48exfbt2+Hm5qb7admyJAns4sWLCA8PR+/evdGuXTsMGzYMX3/9NW7fNv1trTwRERFGt7dv3173e1BQEAAgObkk6e306dPo1q2bpHy3bt1w+rTxacWpbtIfdZNfZkbYBL0htFkFxZVKKi4s1mD4V/9izLL9BqNefrszvb1+60axWhhdFNEcO84mo/1bm7H60H/4YMMZ/HnspmSJA2MBiDETfjiEiynZuF5mYrQitTC4hn8u3jIZgOUVqfHPnQBBP+G2WCMwx8TonCK1wNd3liYwJfhOi0xOoRr5RRrU93JGVON6aFffU1LOQSHHiic7I6ZVAP56uReOzuqDucPCdfu7hvliTJmWCF83JeRyGb5/KgrRTQy/7WvVc1XC9c5oowfbl/zNGBRe32jZvm0CjQ5j12ql1woT7OWMns2lN+k1E7uhT+sALBp1DwDAz10lCbY6NPDS/f5g+4q/5Jqqi0wmw/aXemLTC/chOswHPz/dpcJz6fN0KQ0YvF2lwcPjXRqWLY4mvtKWriJ11UYG/jaxG94f0g73t6w4uPnl2Wjc09ALy8Z0rrBsbWDLTUUcXUpaUKz13NWQnZ2NgQMH4sMPPzTYFxQUBIVCgS1btuCff/7B5s2b8dlnn2HGjBnYu3cvGjeuXD+3q6vxFiZHx9JvGdpvvRpNzY+KIetTawQWl1lFOCO3CH8eu4mG9VzQroEnDieUBtO3cwrhq/eH+3p6Hup7lXQF7L10C1//fRmzBrbW9e9fu10aGP20L0HyPIGezsgrVGPr6dLRIxl5RZIgZMIPh3DgjRg4KuQoUmtQWKyR3Ni0hBAYs2w/AGCq3oKA7lXsDpn+v2NG82VMLaugb/L9TXEuKRsbTybqzlFP75uyWiMwsnNDvLH2hNEWnAV6q0fHT+uB3p9IZ9Vt4O2CG3ojqyb2agqZTIbvn4zCtF+OYOvpZLw/pCR4igyth2/0ZhtuodfdEuTpJEl4XjomUtLq5e5k+rVzclRg5TPRkMmANsElQVWnUG8MvacB/neodDj3F6PuwQN3ZhoO83PFRSPdRGWDm4Y+0r+pzQPc8dXoSIPjtr/UE//dzkW7BtKg7ql7G+vWtjJVd1Ma67VYdmnig2d6NMGXOy+Z1Sqi31rTxE8auKgcFBgUHox1ektXKMu01JizzIQx4SFeCDfSHWVMp9B6WD2hW8UFawmDm4rIZGZ1DVmbUqmEWi39AN9zzz343//+h9DQUDg4GH+rZTIZunXrhm7dumHmzJlo1KgR1qxZg6lTpxo9pyW1atUKu3fvRlxcnG7b7t270bp16xp7Tqo9qw/9Z7Ai8razyfhyZ8naNoseuwfvry/twknLKUSBXstNtw+26ZYZ0M4jklNQjJ+e7oLdF0oW+tO6UKbL4q9zKQbJnum5hZLp+DPyirDuyA0MjWiA/gv/xtVbuTg08wGDHA5TCcKm1nKqyP4rxltHzQluPJwd0blxPUm3hn+ZxE+gpLvF2GR42nBnSu9mCPNzww9PRWH/lTTdqBj919/T2REjO5cMC/Z0ccTXoyORklVgkGiq1SrIHZ1D6yHIywlOjgpJAOPtIm1teGNAa9zIyMNT9zYxeq62ZVqKZDIZPnk0HEIIrD5c0irXv12Qbv+P47vgy52XcD45SzI6KqpJafDlcydw7hDipUtiN6Wxr6skGNF6tV9L+LmrjC6RAJQf3JQ17YEW6NLEB51DKx7qrh90NzWSf6SfR/NtnGGwZm7Loj1ht5SdCA0Nxd69e3HlyhWkpqZCo9Fg4sSJSEtLw8iRI7F//35cvHgRmzZtwtixY6FWq7F37168//77OHDgABISErB69WqkpKSgVatWunMeO3YMZ8+eRWpqKoqKLDedOAC8/PLLWL58ORYvXozz589j3rx5WL16NV566SWLPg9ZxuXUHCzecbHcBMb9V9Iw8YdDuJmRh/jThnNubDlZOqR54o/SWVZv5RQYLNJXtvXhwp1k3VHf7JV0MWn5uJru75+++jj2XZEOqb2Qkg2NRuB8cjYK1RocMTKp29H/DLfVhKTMinNhlA5yg2Ref3cngwCnc2PT3T4AMPSeksnbujX1xQsxpXPPBOkl/Dbwdpa0tshkMvh7OEm26XNQyLHq2WgsHNERACQJqmWDm4Y+Lvjj+e4Y3NF4d5MpxpJXASDAwwkzB7bGiiejEKrXOqM/MqmRT8nvn43siGb+bnjnoTaVem6gZBRZIx/TLepOjubfUpUOcvRq4W+0tbAsjV7ifZCRpGz9t6S3XkK2Vtk1tu4GDG7sxEsvvQSFQoHWrVvDz88PCQkJCA4Oxu7du6FWq9GnTx+0a9cOL7zwAry8vCCXy+Hh4YG//voL/fv3R/PmzfHGG2/gk08+Qb9+/QAA48ePR4sWLRAZGQk/Pz/s3r3bonUePHgwFi5ciLlz56JNmzb48ssvsWzZMvTs2dOiz0OW0Wf+Tny48Qw+KdMaI4TAlzsv4rXVxzFsyR78efwmZqw5gXNGpn6/VM4Q7/wiDc4nSVtgyo4IqmgAYbCXeXO/aKVkFehm2AUAF5XhN++yc8xUlv5IF617GnrhvjJDZc2d0TXAozSQUchl8HFVYsGIDpDJgBfvBCofPdLe6LHaeVrKdgv98fy96N8uEG8OLG01rUwrhDH6Saxl80Sqaug9DfDO4LZYP7m7yTL6I7JkMhnWTeqGeY+G63KTQuq5YMvUHngiOrRKdXDQWwZDf30pAHByqN5rZkqvFv4ID/HCMz2aGF3Ow9hcOjMfLH0vHRTmz7VjL9gtZSeaN2+OPXsMF9Br1qwZVq9ebfSYVq1aYeNG46MMAMDPzw+bN2+u8LmNzX/Ts2dPg2nfO3ToYLDtueeew3PPPVfhc1D5rqXl4osdF/DkvU2MNltXZOOJm/i/f67i42HtEejhpJtvQ582KfHA1TRoNAI/7ktAp9B6uJyabdBMf+VWDnIKK99tc6vMTK9pOYXwcSu9mVe0ZEKwl1Ol1sK5eisHaXoBlPb0J29k4OVfjqGpv5skl6EirYI8DBYafDA8GN6uSizYeh4TeoahTbAnBrQPQm5hMXacTcHn2y7g1M1MSa5L92a+RiefE0J68/ZxLUnS7Rrmi+OzY3VdavVclRjSsT7W3OnCKcutTHDTtr4nvhglHRTgXM3gRn+xRo9ycmwqQy6X4YkuhsGivrItW+0beKG9XmJwdekHCvc09EaXJj5YGF/SrVfdgNAUZ6UCv000nc/iY2SE0rh7G8NN5YC5m8/q8qTuJgxuiOzAjLUn8Ne5FGw8kYjDM/uYdczpm5lwclSgsa8rnv2+pIvo3g+3w99dhe+e7IxLKTloWM/FIP9BLpPh14P/6RYdnNSrqcG5HeVy5BRUP1/rVk4h9GfL0FQQ3AR5Vq7l5lBCOq7oDcues/40fn66C2avO4lTNzN1azC1CHAvdxHCcd0ao2cLP0Q1qYd2szZLVqN2dlRgSu9mGNutsWRIr4vSAf3bBWHXhVTJ4pJP39cEr/dvhQNX0vDIEsMvLNo5YYCSbhKtsrlCpibGdXZUSI4zpTJdLMa010vGNdWVVROMzSNjSfotN57Ojri/pb8uuHFWWqcz5LkeYTickI4hZbr5Hu0Ugkc7hVilTtbG4IbIDpy6UXJzNHfW2tTsAvRbWDIPhjZpVys5qwBTVx7V3XDPvttXN2EYUNIVsvtiaauC/gR2Wg4KWZVabrQCPFRIyizAmZuZknWBbucW4Ww5C/nVq2T3h1ojsFkvD+jA1dsYumQPjpZJOH0iulG5K0i7KBW6biZnpQKFeXrBjVIBmUwmCWz0lQ1KtK1TEY28MaFnGOp7OyP+dHLJwpTtg6B0kKOJrysupeboZsI1RmEioChvpJI+VTVbIYI8nRE/rYfJ664pT97bGEevpaOXGcOXq0K/5cZV5SBprVHVULdURbxclFj1TOVnjbZnDG6I7EB9b2ek3knGzS4oNjlr62urj2PLqUR005sxtcjI3DL6s+zuOJuC2DalEzkqZDJJq4z+5G1auYVqcyfYxgOtA6DWCGw7U5KA/P6Qdth5LhmbTiZh9u+nDMq/uPKIyXPlFqrRpUk9/HvJ/LWIyg4j1wY24Q08cex6BiIaeuORiAblBjf6yg7D1V8h2hgHvRyKVkEeeKJLKICS1o5X+pbMSzWyU0MUFGvgfOdc347phCupOZL3sSxTaxqZO3LGEkFJmF/lu0iry8lRYXR4t6Xo59k4OSokLVw11S1FlcfghsgO6N/GfjlwDQeu3sZbg9rA102FXedTse7odbzxYGvdjVx/Ijdjk9np3wCfWXEQf79SOmO0XA7k6rXKGFtd29gaTwPaB0EG4NrtPF0AcW9TX3w9OhK3sgvw494EPBHdCF4uSpy6aTpv5lSZnBZ9t7IL8H/jOmP+lvNYsrNkEcpADyfJas7mWj62MzLyihDg4VThTUt/3SFlmS6finJX9EefrZ98r9EuHLlcpgtsANNDlaXHlP7uIJfpcmCMBaP6Xo5tgZX7r2GKlafPt1UKuX4wI5d8NqrblUeWw+DGiIqSFsm23I3vV0ZuEf5vzxUMCg/Gl39dlMzb8dad1g6FTIZPR3bE49/uBWB6xIo5yxDELd2n+/3fS2kI1Etq/e+28XlgynoltgUa+bhinN78NNo6+bipJGvRtA7yNDje311lsHTAjP6t8N760hmtOzWuB5WDQtLqsHv6/Qh7fb1ZddTaOvU+eLsqKxzl81zPMJy5mYlRUaVJrmW7fSoKjHL01o6yZG6KfsuNs6MC4SFe2HUhVdJSZMzEXk0x0UgeFZXQf/2cHBWSUWHm5DJR7eA7oUc7m25ubm4FJcmWaN8v/dmQ67oNx2/iizKz++qbte4E5m05h55zd+CnfdeMlik7FFs7eV5Z5ixDUHYIt35LiP4SCuXxci4JFPQDj6Ymui2MLdLXPMAdvm7S+VzG31c6CVwzfzfd/C36MYKxxSTb1ffUzX5sTFN/w6TUn8YbTpn/XM8wLBvbWdKqEqY3Wm3Fk50rDG4qaoGpKv3gRuWowOePdcTD99THd0/axvT4dZV+zo2zo0Ly3pubz0Q1j++EHoVCAS8vL93aRy4uLrWa5U+VI4RAbm4ukpOT4eXlBYXCPvq7hRB47oeS0Uv3NfMzGK0EALsv3qrwPIVqjdF8mrKyjMxkWxmmFmosS/uHf1hkA+y/koZgT2eMv8/4Mh/NA9zQPMANRWqh6+JSOcjh4eSgyy0qa2hEA10gE1RmOPCsga11LVoAsHpCV7z7xyn8356rAEryZLSTAhoLYgCgSxPD5F03peGfUP2ArXszP4P9ZY3r1hiZeUXo08Zw8rXq0I/pVA5yeLkoMe/RDhZ9jruRQ5luKZWDAr9PuhdqIeBi5PNA1sF3ogztCtjaAIdsn5eXl1krl9cV+gtNmloTpmxehzFFag0SM4znmjwa2QCrDpSs0zN08T9VqKV5Gvu6IjOvCI18XHSTj3UN88WuV+8v9zgHhRzrJ3eHRgDN39hwZ5tM8q1Zf90gAGgZWNra8mD7YBy5lo5Od6a21w7D1q4N5SCXwUnvG7eTXnBzTyMvo3XSjnjKyCvC+O6NEdsm0OiEaiM7N8Rn286bnE23LGelAq/1b2VW2crQr5uKuSAW41im5QaAwRpUZH0MbsqQyWQICgqCv7+/xZcbIMtzdHS0mxYbrat6864Um1jNt+yIHGOupeWh+0fbDbY39XfDR4+EY/+V27icmoO8SqyOXVm+bkpsmNK9SrkIZScSdFTIJV0tvzxbMvT1f89F4/TNLPTQm/FXIZdh1kDp9Pr63UMymUyS6KtyVAB3WrDKCxy3TL0P55Oy0TXMx2SrbqCnE47M6lNjs9WaS/+1MicYJvMo5NLuPrJNDG5MUCgUdnfTpLrhyq3SHJYf9yXAw9nRoJWi7LTvlaEdJl7dYatN/d0kC1bKZcBDHepjRKcQ3UKXJUNlq/c8LkoFcgvV6NXCXzIKS3sdEY3qIaJRxYsPRjbyBlB6o5cEN3rBYnld0SXrOBmu7VOWh5P187/KdkuRZegH6hz6bbv4iSeqQRqNwMkbGWblvmjpJ+j+duSGbl4X/VFhxlpC+rczr2tOu/q0sWGrzfSSYfXnZ/nr5V4GZcPLTGm/f0YM5g4LR4eGpdvLS9o116YX7sOnIztiSMf6RpODzeXv4YTd0+/H/hkxACBJBLXHm5QkodjKrUj2RDJaikGjzeI7Q1SDvtl1CQM+3YUZa46bfUxSmTyZM4lZOHotHfe8swUr9lwBYNhlMzA8GA93bGDW+W/eOb+xrorvn4rS/R7RyBvfxkXiu3Gd0dDHBf97rqukrK+7dJi0j5sKCrkMKgeFbnHHuK6hZtWpPCH1XDAoPBhyuczkxHTmqu/lDM87c9Lot2bY4/wk+jk35nRjknkkC2fydbVZ7JYiqkHztpSsoL3qwH/46JFwo2VOXM/As98fxMP3NICfuwpX0wwnwBu3fD9u5xbhzd9OolmAu8HijE18XeFqYlbiskbcWWvG2BBw/TwRT2dH9G5VOoKnbDDk6yodlq1v1TPRyC4oNuhOq64XH2iOuKX7MCzCvECuPDLYd8uGfiMXgxvL0U9qr2jOILIeBjdENUQIYZAQ/L+D/8FVpUDftkG6bTPWHMd/t/Pw6Z3F94zRXy17xJ18Fn0ymeEaRVra1aGVCjk+f6wjut6Zst/YEHD9UTVlz+foIP1Drt9yU7ZBpZFPzczd0qO5H/a+3ht+bqYDK3MJlL439piTIu2Wsr/rsxb9rlH9VhyyLQxuiKrgtyPX8f760/hkWAfEn0nCoPBgdGzordv/zd+X8PXfl3RT3gPA/Z/swKWUklaZS+/3h1wuQ36R2iKjlbLyi+GqKm196NXCDwPaByO2TQDcnRzxYkxzeDo76rpkACDTWHDjYDq4aRHgjoHhwfj9aMnSDfVcVWgZ6I4ziVno17b2huIHeFSc0GsO/Ymt7XGSa8loKQY3FqOf71adxH6qWQxuiMxUpNZAIwRUDgpM+fkIAOiWNli2+4pkde13/zxtcLw2sAFKJthTFwn0nLsDKVnGJ6WrDHmZlptgL2c8otd109DHxeCYLL3VvB9oHYDYNoGSkUJlu7lkMhk+GtpeF9y4KBVYOqYTfjtyAyM7h1T7Gmqb/tyDD3UIxp5LtyQJ1XUdh4LXDEnLDV9Xm8XghsgMQgjEzNuJnIJi7Hmtt8lyV2/l4OVfjlV4vvwiNS6mZFc7sHn+/qbYcioJ47s3kQQj5swro7845tdGVlE21s2ln3jbwNsZQZ7OeK5nWGWrbRN6tiiZF6eBtzMejQxBw3ouaBNsP5OxSYaC22HCtDV1DfNBYkY+Wls4p4wsh8ENkRnyizS4emf+mWvlrKX0xtoT2HclrcLzFRRrcDm1+muYTX2gOab1aQFAOlTcnFFFk3s3w6fx5zGum/ElEJoFGLZiyGQy/PJsSbJwkGf1h3lbU7CXM/bN6A0PJ0fI5TJdLpK9kIyW4pxdFvXDU1HQCOPrlpFtYHBDZIZCvVYOTTkJGqaWOygrv0gtmYm4Kr4b11nSjaT/uzl/c6f0boY+rQMkyxYAwP+N64wzNzMlM/7q0y5pYA/MmZCvrmLOTc2RyWRguo1tY3BDZIZ8vTWetMO7jSnvm5zKQQ5HhRzZBcUoKNZIZiIuz6iohlA6yHEhORt/n08FAHz1RATuMxF8ADC65pGxuhpblLNHcz+TgQ3VHZyhmO5m/MQTmaGgqLTlZv3xRJPlygtuOjb0gsedlbHzi9RINZFvE+rjglFRDQEAPq5KvDekHWYNbINm/qUtLBGNvI0eq+VshzPuUuUoOIkf3cXYckNkBlOrc5dVNrhpWM9Ft5xCsKczkjIL7pxPg/Q84wuzbnzhPjjIZega5ovI0NIgpr3eysM+JuZ5eTm2BX4/esNkHg3dPWSc54buYgxu6K5WrNaYNZwzv8i8taHyCqVB0OYX70PLNzcCAOq5KnU3mfwiNTJyCw2OB0puRDKZDAPaB0m2P9QhGBl5ReW22kzs1RQTezU1q65k39gtRXczfuLprnUtLRcd39mC99cbzklTlrktN7dzpa0x+gsyersqobrzuKDIdMuNqVWpZTIZ4rqGGs2TISqLCcV0N+Mnnu5an8afR1Z+Mb7661KFZfXnhDGlWK3BbROtMQDg767SrSKcVVCE3DutPNr8GiJL4sKZdDfjJ57uWmpN+XPuL911Ge+vPw0hhNFFJstKSMs1es6x3UIRHuKFgeHBupabL7Zf1O1/56G26NM6wOA4ouqQLJzJeW7oLsOcG7prqcuZr0YIgbf/OAUA6N8uyKyWm97zdhrdPmtgG93v2pab88nZum1yucysGYWJKkO/W8rdiX/q6e7Cv6h01youp+Vmxb9Xdb/vvpBqVsuNOYsvqkwM0XbgjGBkYQq94MZLb8FUoruBTQQ3ixYtQmhoKJycnBAVFYV9+/aZddzPP/8MmUyGwYMH12wFyS6p1cajkWtpuZj520nd4483ncXZxCyzzzt3WDieua8JfhwfZbDPqUzuw+T7S0Y2dQzxMvv8RObQz0v3clZaryJEVmD1tsqVK1di6tSpWLJkCaKiorBgwQLExsbi7Nmz8Pf3N3nclStX8NJLL6F79+61WFuyJ2VbbtQagU0nE422wPx7ueL1ogCgU6i3ZDXusvQXMHypT3NMur8ZAODxLo1QpBbo2tTHrOchqoh+/pcnW27oLmP1lpt58+Zh/PjxGDt2LFq3bo0lS5bAxcUFS5cuNXmMWq3GqFGj8NZbb6FJkya1WFuq6wqLNboFJvXXiBJCYNWBa5jwwyFM/PGQwXFHr6Wbdf4mvoaLTepzcijtlmoZWLqisINCjvH3NbGrVanJurLyi3W/uxtZ4Z3Inlk1uCksLMTBgwcRExOj2yaXyxETE4M9e/aYPO7tt9+Gv78/nnzyyQqfo6CgAJmZmZIfujslZ+aj49ub8fKvxwBIW24KijXYcTa52s8R6Fn+Qoz6+zlfDdWkzPzSeZTMWWuMyJ5YNZxPTU2FWq1GQIB0GGxAQADOnDlj9Jhdu3bh22+/xZEjR8x6jjlz5uCtt96qblXJDny/NwE5hWr8evA/zB0WDrWmdARUQZFGMuFeRcZ1a4yU7AL8fvSGZHuAR/nBzRPRjRDg4QR/d1WFgRBRdWSamCSS6G5g9W6pysjKysITTzyBr7/+Gr6+vmYd89prryEjI0P3c+3atRquJdkq/e+uxWoNCvWGd+cXqyu12GRUk3pwM9LUX8+1/MRNlYMCA8ODEdWEuTVUs0zNdE10N7Bqy42vry8UCgWSkpIk25OSkhAYGGhQ/uLFi7hy5QoGDhyo26a58+3bwcEBZ8+eRVhYmOQYlUoFlcr4IoNkf66k5sDdyUGysGRGbhE2n0qU5CA8vPgfFOuNllq84yJ+3m9+4KuQyYzOHeLNxE2yERN6heHg1dsY3inE2lUhqnVWDW6USiUiIiIQHx+vG86t0WgQHx+PSZMmGZRv2bIljh8/Ltn2xhtvICsrCwsXLkRICP8T382SMvPRc+4OyGTA5TkDAJSMGOkxdzvSy6z5dOy/DITUc9Y9Xv7PlUo9V26RGs0D3A22V9RyQ1Rb/N2d8Pvz91q7GkRWYfUU+qlTpyIuLg6RkZHo3LkzFixYgJycHIwdOxYAMHr0aNSvXx9z5syBk5MT2rZtKzney8sLAAy2093n2H8ZAEom01NrBBRyGRIz8w0CG63/budV+bmUChkGtq+Pexp64c3fTmD3hVsAShbHJCIi67J6cDN8+HCkpKRg5syZSExMRIcOHbBx40ZdknFCQgLk8jqVGkS1pLBYg8upOWge4AaZTAb9FQzScwvh46bC7RzTC1lWNKNw92a+cHJUYMup0m7T6f1a4sT1DMS0CoBMJkMTPzf4uJZ2gXk5s1uKiMjarB7cAMCkSZOMdkMBwI4dO8o9dvny5ZavENW6lKwC7DibjIHhwWaPWpry82FsOJGIjx9pj2GRIcguKF0iIS2nJLhJKye4qciKJ6Pw0cYzuuDm3qa+eLZHmEE5/antHbhGFBGR1dlEcEP2b+e5FHg4OaBjQ2+j+0d8tQcXU3JwJjELbz7Y2qxzbjiRCAD46q9LGBYZgozc0kDmRkY+1h9PlMz1URkz79RBf3HNFU92Nlq2HeerISKyKQxuqMbdSM9D3NKS9cKufDDAaJmLKTkAgA3Hb5od3Ghpw48MvXk9Fm2/gH1mLplQ1rIxndCrZcnSH/qBi6mhtQ/f0wAJabmIDK1XpecjIiLLYnBDNe5GemnirkYjyp0tVS0EMvOLMPrbfejXNhDPlOkGupSSjdm/n8KU3k0NjtVPHDZ3uQRjnJWl3WID2gUh6+FidGzoZbK8Qi7DtD4tqvx8RERkWQxuqMbp5+0WFGskwUNZGgF8988VHLmWjiPX0g2Cm8+3X8Bf51Lw17kU3bYLydl4ceURySrIjgo5CvQm6asMV2XpfwuZTIaRnRtW6TxERGQdDG7IbEII5BWp4aKs3MdGf3Xi/CI1HBQyTPrxEFoGeuDFB5obPEem3mR7+ttlMhnkJrqG1hy+Die9FbezCwzPYS5fdw7nJiKqyzi0g8w2ddVRtJ65CReSsyt13HW9+WQKijVYf/wmNp1MwsL48wCAv8+XtsKoNQIavWBIoxG4np6HLnPisWDrOZS3/l9+kfGWmveHtEOgiTWfRnZuiDcGtJJsC/J0NlqWiIjqBgY3ZLY1h68DAJbtvmz2MZdSsjHtl6O6x/lFapxJzJKUeeLbfbrfNUI6QulSag6W776MpMwCLNh6Hjl6w73N8fz9TfFYVEPJcG0A6NKkHr4YdQ9e6tMcT3Vvgp/Gd4HSQY6PHmlfqfMTEZHtYbcUVZqivOaTMrTDtbXyi9W4kpqje1yklra2aIRAjl6XUsy8nZL9lZ23RtuF5llmcr33h7RDEz833ePoMB+cfacvFxskIrIDbLmhSrucmgNR0fS+d5QNhAqKNEjNLtA9zi+StsRoNAJpOabnpjl5I6MSNQVcVSXJy/otN1+PjpQENloMbIiI7AODG6q0v8+nYqWZK2g7lAlu8ovUKNbLqckqkzysFgJpOQUwxViycXm0LTdezqVJws5mzoBMRER1E4MbqpIv/7oEIQTiTyfhWlquyXJlg5sTNzJxOCFd97jrB9sk+zUCuG1ioUt9z99vOM+NMa53hp27qEoDmsZ+rmYdS0REdRODG6oSjRDYeS4FT/7fAXT/aDs+2ngGQghsPZWEpMx83EjPw5Fr6Sgsk1Pzzh+nyj2vEMKsvJpB4cH44/l7KyznoippubmVXXrOYE/jI6eIiMg+MKGYzFI2x0YIYK/e8gZf7LiIpv5umLrqKAI8VCgo1khmDDZXkVrollH4blxnjF66z2g5V5UDmgVUPGTb7U6LjY9babcUc2uIiOwbW27IpKz8IhxKuA0hhEELjICAY5kVsL/bcxUAkJRZUKXARp9MBoSHeOkeB3k64YHWAbrHbk7G4/LYNgFwVJQGL8FeJQHQpF5NMbhDMH54Kqpa9SIiItvHlhsyadiSPTiTmIVFj92D9cdvSvYJATiWyac5lySdv6Y6vJwd4eHkgH5tA5GRV4Tvn4xCWm4hrqXlwt3JAe53upte7dsSH248g5kPtkZs20B4uzhi25lkXE7JQY8WfroJ+XzcVFgwoqPF6kdERLaLwQ2ZpJ1sb83h69h6OkmyTwgYrN2UW1i5CfbK4+2qhEwmw+LHI3TbfN1UWD+5O2Sy0q6lZ3s0wYPtg9DA21m37cH2wRarBxER1T3slqIKGZuz73p6HtJyKzehniluKsMYu56L8fWd5HKZJGdGJpMhpJ4L82iIiEiHwQ1VyFTc8OPeBIucf2Ivw2HdXiaCGyIiooqwW4oMnEvKkkyup7+qt6XJZYDCSIhddrkEIiIiczG4ucsIIfDf7Tz4uqlwIyMPYWWWIRBCYPiXeyQT6W09nVzuOZ0dFcgrKj/fpkWAO84aSTj+YtQ9+E9v1XAtD2d+NImIqGrYLXWXWbn/Grp/tB2tZm5E70924p+LqZL919LyzJohWN8HQ9uVu7+JnytGdA4x2H7q7Vj0bRuE9g28DPZ5OLHlhoiIqobBzV1m+urjksdf7ryE9DuJwRl5Rbjv4+2VPmdMq4By9z97X5jRYEW77lPnxvWw5PF7EBfdSLfPg91SRERURQxu7Nj19DykZhcgJasAhxNuGy2z81wKes7dgWW7LyPq/a2Vfo6XY1vAVeWAXi38TJZxUipMTrqn1bdtEKLDfHWPPSooT0REZArvIHYqI68I3e4sSqnNiVn5dBejZdNzi/DW7+Wv+WTM4lH3oF+7IADAR4+EY2H8OXz/r+EIKicHucFw706h3gbl/NxVut/ZckNERFXFlps6JLugGHM3ncWpG5kVlr2cmqP7XZvs+3YFi1ZWlotewOLnrsK7g43n3shkMknLTTN/N/w43jDQ8nMrDW7c2XJDRERVxOCmDvl44xl8vv0C+n/6d4Vl1RqNwbaTZgRFleGiVBhs+3PyvXgxpjnWTuym2yYD4K6Xc9PY19VgXSoA8HUvndvGQc6PJhERVQ2/Htchx65nmF02u8D8pRAGtAuCu5MDft5/rVL1cXY0DG7aBHuiTbCnwXb9bqkgTyej59MmGJdXhoiIqCIMbuzUwavGE4iNqe/tLFlJ21zORlpu9IWHeOFcYhaimtSTtNS0DvYwecwfz9+LtJxChNRzqXR9iIiIAAY3dumfC6n4NP682eWb+LpKZiTWF9PK3+Qkfsa6pfStfq4ritQaODkqIETpLMdd9UZFldW2vmGrDxERUWUwuKlDzG1bmbXupFnllA5yvD+kHQaGB2HNoesmy5ji4lj+x0chl0EhLwmAZDIZ/nj+XuQVqdkqQ0RENYpZm3YmOSsfF1KyzSrbq4UfHoloAJWDAtkFpS03qyd01f1uLPFXy0lZuY9P2/qe6BRar1LHEBERVRaDGzsihED0nG0QZq5zqb84ZbemJV1FgR5OcHIo7W5SlhPclLePiIjIWtgtZeP+Pp+CzSeT8Hr/VibL5BepkZCWC7VGVGoFbzdVaXDTKsgD6yd3R5CnE1KzC3TbTXVL7XntfshklU9CJiIiqmkMbmzcE9/uAwA08jGdp/LY1//iUEI6ujcznahrjJOjNHDRjmJKzytdOFPpIMf0fi3xwYYzkrJBns6Vei4iIqLawuCmjriVUyh5vP1sMnaeTUFmXhEOJaQDAP4+L13he8dLPdFz7g4AJYtbFhSr0TrIA1/+dQkAoHIwPtpJf1i4UiHH092boEdzP/x9PgXvrz9j9BgiIiJbweDGhmXotaDUc1FK9o1dtr/C40N9XfH3K72w+0Iq7m/pD3+PkonxdMGNo/EuJ/0kYqWDHHK5DK2CPHAru9BoeSIiIlvC4MZG3c4pxMu/HtU9FjA/l0ZfSD0XjOjc0Og+lYl8Gv3gRv/3bk198MmwcLQIdK9SXYiIiGoDgxsb9cr/jkkmz0vPLe1+shQPJ+MrbzvodUvppwzLZDIMjWhg0ToQERFZGsfy2qg9F29JHn+x46LFzv1czzBENvLGgPZBRvc76i1ayQFRRERU17DlxkZVZki3MQq56ajk1b4tyz1WP6GYw72JiKiuYcuNjVKbMRNfM383o9vHdWuMLS/eV+Xn1g+MGNsQEVFdw+DGRgkzgptATyej26f0boYmfsYDH3Pot9bIGd0QEVEdw+DGRpnTK+VVZng4ADQPcIOHs+V6GxnaEBFRXcPgxkaZk3PjqjSchO/ZHmEWzZNhww0REdU1DG7qMGPrPkU18bHoc7BbioiI6hoGN3VUfS9nXL+dZ7A92EQeDhER0d2CQ8HrqDB/N8mcxb9Puhc+bkqLD91u7Otq0fMRERHVNAY3dVSP5n7o3swXiRn5eP7+pmjXwNOi5//56S44cT0D97f0t+h5iYiIahqDmzpoQLsgxEU3goNCjvVTutfIc3Rp4oMuFs7fISIiqg0MbuoQP3cV3hrUBv3bGV82gYiIiBjc1AkxrQKgcpBjxoBWCPZytnZ1iIiIbBqDmzogOswHT97b2NrVICIiqhNsYij4okWLEBoaCicnJ0RFRWHfvn0my65evRqRkZHw8vKCq6srOnTogBUrVtRibWufUsG5ZoiIiMxl9eBm5cqVmDp1KmbNmoVDhw4hPDwcsbGxSE5ONlq+Xr16mDFjBvbs2YNjx45h7NixGDt2LDZt2lTLNa89Dgqrv01ERER1htXvmvPmzcP48eMxduxYtG7dGkuWLIGLiwuWLl1qtHzPnj0xZMgQtGrVCmFhYZgyZQrat2+PXbt21XLNa06xWiN57MjghoiIyGxWvWsWFhbi4MGDiImJ0W2Ty+WIiYnBnj17KjxeCIH4+HicPXsW9913n9EyBQUFyMzMlPzYurwiteQxYxsiIiLzWfW2mZqaCrVajYCAAMn2gIAAJCYmmjwuIyMDbm5uUCqVGDBgAD777DM88MADRsvOmTMHnp6eup+QkBCLXkNNyMgrsnYViIiI6qw62Sbg7u6OI0eOYP/+/XjvvfcwdepU7Nixw2jZ1157DRkZGbqfa9eu1W5lqyA9l8ENERFRVVl1KLivry8UCgWSkpIk25OSkhAYGGjyOLlcjqZNmwIAOnTogNOnT2POnDno2bOnQVmVSgWVSmXRete0TLbcEBERVZlVW26USiUiIiIQHx+v26bRaBAfH4/o6Gizz6PRaFBQUFATVbQKdksRERFVndUn8Zs6dSri4uIQGRmJzp07Y8GCBcjJycHYsWMBAKNHj0b9+vUxZ84cACU5NJGRkQgLC0NBQQHWr1+PFStWYPHixda8DItKZ3BDRERUZVYPboYPH46UlBTMnDkTiYmJ6NChAzZu3KhLMk5ISIBcXtrAlJOTgwkTJuC///6Ds7MzWrZsie+//x7Dhw+31iVYXNmWGxk4iR8REZG5ZEIIYe1K1KbMzEx4enoiIyMDHh4e1q6OUR9sOIMlOy/qHs8fHo4hHRtYsUZERETWVZn7d50cLWXvMvOlLTfdwnytVBMiIqK6x+rdUmQo/84kftMeaI7HohrCx61ujfYiIiKyJrbc2KCCopLlFzycHRnYEBERVRKDGxtUUFzScqNy4NtDRERUWbx72qD8Oy03To4KK9eEiIio7mFwY4PYckNERFR1vHvaILbcEBERVR2DGxvElhsiIqKq493TBmlbblRsuSEiIqo0Bjc2iC03REREVce7pw1izg0REVHVMbixQWy5ISIiqjrePW2MEIItN0RERNXA4MbGFKo1ut9Vjnx7iIiIKot3TxujbbUBACcHttwQERFVFoMbG3MlNQcAIJMBjgqZlWtDRERU9zC4sSFHrqXjoUW7AZS02shkDG6IiIgqi8GNDfnfwf90vys5UoqIiKhKeAe1UY4KvjVERERVwTuojWK+DRERUdUwuLFRDgxuiIiIqoTBjY1ykPOtISIiqgreQW2Ug5wtN0RERFXB4MaG6I/8dmBCMRERUZXwDmqjmFBMRERUNQxubJSC3VJERERVwuDGRjkyoZiIiKhKeAe1IfptNRwKTkREVDUMbmwUu6WIiIiqhsENERER2ZUqBzc7d+7EwIED0bRpUzRt2hSDBg3C33//bcm6EREREVValYKb77//HjExMXBxccHkyZMxefJkODs7o3fv3vjxxx8tXUciIiIiszlU5aD33nsPH330EV588UXdtsmTJ2PevHl455138Nhjj1msgkRERESVUaWWm0uXLmHgwIEG2wcNGoTLly9Xu1J3K42wdg2IiIjqvioFNyEhIYiPjzfYvnXrVoSEhFS7UnerYo3G2lUgIiKq86rULTVt2jRMnjwZR44cQdeuXQEAu3fvxvLly7Fw4UKLVvBuUqxm0w0REVF1VSm4ee655xAYGIhPPvkEq1atAgC0atUKK1euxEMPPWTRCt5NitkvRUREVG1VCm4AYMiQIRgyZIgl63LXY3BDRERUfVUObgCgsLAQycnJ0JTJFWnYsGG1KnW3UjPnhoiIqNqqFNycP38e48aNwz///CPZLoSATCaDWq22SOXuNkXMuSEiIqq2KgU3Y8aMgYODA/744w8EBQVBJuM6SJag1uuW4mtKRERUNVUKbo4cOYKDBw+iZcuWlq7PXa1IzW4pIiKi6qrSPDetW7dGamqqpety11MzoZiIiKjazA5uMjMzdT8ffvghXnnlFezYsQO3bt2S7MvMzKzJ+to1znNDRERUfWZ3S3l5eUnyQIQQ6N27t6QME4qrhzMUExERVZ/Zwc327dsBAAUFBejbty+WLFmCFi1a1FjF7kb689wwnZiIiKhqzA5uevToofvdx8cHvXr1QrNmzWqkUncr/W4pdlARERFVTZUSih9//HF8++23lq7LXY/dUkRERNVXpaHgxcXFWLp0KbZu3YqIiAi4urpK9s+bN88ilbvb6HdLTX2guRVrQkREVHdVKbg5ceIE7rnnHgDAuXPnJPs4+VzVabulvn8yCh1CvKxbGSIiojqqSsGNNrmYLEs7z427U7WW/CIiIrqrVSnnhmqGdoZiBwVbv4iIiKrKJoKbRYsWITQ0FE5OToiKisK+fftMlv3666/RvXt3eHt7w9vbGzExMeWWr0u0LTeOCpt4W4iIiOokq99FV65cialTp2LWrFk4dOgQwsPDERsbi+TkZKPld+zYgZEjR2L79u3Ys2cPQkJC0KdPH1y/fr2Wa2552pYbhZwtN0RERFUlE0JYdUqVqKgodOrUCZ9//jkAQKPRICQkBM8//zymT59e4fFqtRre3t74/PPPMXr06ArLZ2ZmwtPTExkZGfDw8Kh2/S2pzcyNyClU46+Xe6Ghj4u1q0NERGQzKnP/tmrLTWFhIQ4ePIiYmBjdNrlcjpiYGOzZs8esc+Tm5qKoqAj16tUzur+goKDOrH1VdKdbSsGcGyIioiqzanCTmpoKtVqNgIAAyfaAgAAkJiaadY5XX30VwcHBkgBJ35w5c+Dp6an7CQkJqXa9a0rxnW4pR3ZLERERVZnVc26q44MPPsDPP/+MNWvWwMnJyWiZ1157DRkZGbqfa9eu1XItzaPRCGjn8GPODRERUdVZdUIVX19fKBQKJCUlSbYnJSUhMDCw3GPnzp2LDz74AFu3bkX79u1NllOpVFCpVBapb03Sn53YgaOliIiIqsyqd1GlUomIiAjEx8frtmk0GsTHxyM6OtrkcR999BHeeecdbNy4EZGRkbVR1Rqn1gtuHJlzQ0REVGVWnwp36tSpiIuLQ2RkJDp37owFCxYgJycHY8eOBQCMHj0a9evXx5w5cwAAH374IWbOnIkff/wRoaGhutwcNzc3uLm5We06qqtIb9FMdksRERFVndWDm+HDhyMlJQUzZ85EYmIiOnTogI0bN+qSjBMSEiCXlzYwLV68GIWFhXjkkUck55k1axZmz55dm1W3KO26UgDgKGe3FBERUVVZfZ6b2mar89wkZ+Wj83vxkMmAy3MGWLs6RERENqXOzHNDpbQtN2y1ISIiqh7eSW2ENqGYi2YSERFVD4MbG8F1pYiIiCyDwY2N4IrgRERElsE7qY0oupNzw5YbIiKi6mFwYyOKNVxXioiIyBIY3NiIYq4ITkREZBEMbmwEh4ITERFZBu+kNkLbLcWh4ERERNXD4MZGFOsSivmWEBERVQfvpDZCl1DMlhsiIqJqYXBjI4o5FJyIiMgiGNzYCO1oKSYUExERVQ/vpDaimGtLERERWQSDGxtRzLWliIiILILBjY0o5tpSREREFsE7qY1gQjEREZFlMLixERwKTkREZBkMbmwEJ/EjIiKyDN5JbQRXBSciIrIMBjc2gkPBiYiILIPBjY1gtxQREZFl8E5qI7Tz3DChmIiIqHoY3NgIbbcUh4ITERFVD4MbG8FJ/IiIiCyDd1Iboc25cWDLDRERUbUwuLER2qHgDG6IiIiqh8GNjSgdCs63hIiIqDp4J7URXBWciIjIMhjc2Ahtzg2HghMREVUPgxsbUToUnG8JERFRdfBOaiO4KjgREZFlMLixEaVDwfmWEBERVQfvpDZCN1qKCcVERETVwuDGRhTdGS3FVcGJiIiqh8GNjVBzbSkiIiKLYHBjI0qHgvMtISIiqg7eSW0El18gIiKyDAY3NqJ0+QUGN0RERNXB4MZGcCg4ERGRZfBOaiPYLUVERGQZDG5shK7lhgnFRERE1cI7qY0o5lBwIiIii2BwYyOK1VxbioiIyBIY3NiI0uUX+JYQERFVB++kNoJDwYmIiCyDwY2N0K0txZwbIiKiamFwYyPU7JYiIiKyCN5JbUTpUHC23BAREVUHgxsbwUn8iIiILIPBjQ0QQuBOrxTkDG6IiIiqhcGNDdAGNgBbboiIiKrL6sHNokWLEBoaCicnJ0RFRWHfvn0my548eRJDhw5FaGgoZDIZFixYUHsVrUHaLimALTdERETVZdXgZuXKlZg6dSpmzZqFQ4cOITw8HLGxsUhOTjZaPjc3F02aNMEHH3yAwMDAWq5tzdGLbaCQMbghIiKqDqsGN/PmzcP48eMxduxYtG7dGkuWLIGLiwuWLl1qtHynTp3w8ccfY8SIEVCpVLVc25qjFqX9UlxbioiIqHqsFtwUFhbi4MGDiImJKa2MXI6YmBjs2bPHYs9TUFCAzMxMyY+tUasZ3BAREVmK1YKb1NRUqNVqBAQESLYHBAQgMTHRYs8zZ84ceHp66n5CQkIsdm5LkbTcsFuKiIioWqyeUFzTXnvtNWRkZOh+rl27Zu0qGVDrDZdiQjEREVH1OFjriX19faFQKJCUlCTZnpSUZNFkYZVKZfP5OZo7LTfskiIiIqo+q7XcKJVKREREID4+XrdNo9EgPj4e0dHR1qqWVWhXBGeXFBERUfVZreUGAKZOnYq4uDhERkaic+fOWLBgAXJycjB27FgAwOjRo1G/fn3MmTMHQEkS8qlTp3S/X79+HUeOHIGbmxuaNm1qteuoLo2GLTdERESWYtXgZvjw4UhJScHMmTORmJiIDh06YOPGjbok44SEBMj1Vsm+ceMGOnbsqHs8d+5czJ07Fz169MCOHTtqu/oWo2ZwQ0REZDEyIfSG6twFMjMz4enpiYyMDHh4eFi7OgCAiynZ6P3JTng4OeDY7FhrV4eIiMjmVOb+bfejpeoCttwQERFZDoMbG1Aa3PDtICIiqi7eTW1AaXBj5YoQERHZAd5ObYBunhsOBSciIqo2Bjc2QDvPDWcnJiIiqj4GNzZAO8+NA4MbIiKiamNwYwPUbLkhIiKyGAY3NkDNnBsiIiKLYXBjAzjPDRERkeUwuLEBDG6IiIgsh8GNDdANBWdwQ0REVG0MbmxAsfpOQjFzboiIiKqNwY0NYMsNERGR5TC4sQFqTcm/DG6IiIiqj8GNDeBQcCIiIsthcGMD1JqSphu23BAREVUfgxsboO2W4gzFRERE1cfgxgZwbSkiIiLLYXBTQzaeuIkZa46jSNssc8ff51NwMSVbsk2bc8Oh4ERERNXnYO0K2Ktnvz8EAGgT7InHohoCAE5cz8AT3+4DAFz5YICubLFuhuJariQREZEd4u20hqVkFeh+P3Uj02gZDZdfICIishgGNzVMP14REEbLaNeWYrcUERFR9TG4qWHmxCvaGYqZUExERFR9DG5qmEwvuhHGG250OTccCk5ERFR9DG5qmDktN9puKc5QTEREVH0MbmqYDHotNybKMKGYiIjIchjc1DBz4hU1VwUnIiKyGAY3NUxuRs6Nmi03REREFsPgpgYIvSimMjk3HApORERUfQxuaoB29FNZJue5YbcUERGRxTC4qQHF6tIgxlRrjH7rjlrNeW6IiIgshcFNDSjWlC6WqR/b6Ofc6P9eUFxSXuXAt4OIiKi6eDetAfotN3su3kLfBX/hcMJtSaeURi+6yS9SAwBUjoraqiIREZHdYnBTA4r0Wm42n0rCmcQsjPpmr6SMWj+4udNy48TghoiIqNoY3NQAtZGE4txCtaQvSr9bKq+wpOXGmcENERFRtTG4qQH63VKmaAOg80lZ2Ho6CQDg5Mi3g4iIqLp4N60BRWqN0e3Gcm4emP+Xbhu7pYiIiKqPwU0NMNYtVZbGSPzDlhsiIqLq4920BhSZ6JbSz7PRCIHiMi08Tg5suSEiIqouBjc1oNhYswzKTNwnBK6n50n2OykZ3BAREVUXg5saYGr5BXWZlptraWWCG7bcEBERVRuDGwu7lV2AtOxCo/s0ekGPRgOkZOdL9jPnhoiIqPocrF0Be5JdUIyId7ea3K8/cd/aI9dRdikpjpYiIiKqPjYVWND5pKxy9+uPovpgwxlcvy3tluIkfkRERNXH4MaC8u6sEWWKpkwuzo2Mst1SDG6IiIiqi8GNBRUUGR8lpXUjQ9pSk5QpDW64KjgREVH18W5qQRW13Py075rk8a0yicfyskk4REREVGlMKLYg7QKY5sopLAYAvDGgFXq19K+JKhEREd112HJjQdkFxZUqn1tQEgxFNPJGmJ9bTVSJiIjorsPgxoIqG9wU3ll+QclcGyIiIovhXdWCMvOLqnScijMTExERWYxNBDeLFi1CaGgonJycEBUVhX379pVb/pdffkHLli3h5OSEdu3aYf369bVU0/Jl5Veu5UaLo6SIiIgsx+p31ZUrV2Lq1KmYNWsWDh06hPDwcMTGxiI5Odlo+X/++QcjR47Ek08+icOHD2Pw4MEYPHgwTpw4Ucs1N3QtLbdKxzG4ISIishyZ0F+q2gqioqLQqVMnfP755wAAjUaDkJAQPP/885g+fbpB+eHDhyMnJwd//PGHbluXLl3QoUMHLFmypMLny8zMhKenJzIyMuDh4WGx6ziTmIm+C/6u0rFHZj4ALxelxepCRERkbypz/7Zqk0FhYSEOHjyImJgY3Ta5XI6YmBjs2bPH6DF79uyRlAeA2NhYk+VrS0ZuEZr6mx7x5OXiaHIfE4qJiIgsx6p31dTUVKjVagQEBEi2BwQEIDEx0egxiYmJlSpfUFCAzMxMyU9NiGrig80v3AcfV+MtMI9Ghpg8VqlgcENERGQpdn9XnTNnDjw9PXU/ISGmg4zqkstl8DYR3DTwdjZ5nAODGyIiIoux6l3V19cXCoUCSUlJku1JSUkIDAw0ekxgYGClyr/22mvIyMjQ/Vy7ds1oOUtxMLGEgoeT6W4pIiIishyrBjdKpRIRERGIj4/XbdNoNIiPj0d0dLTRY6KjoyXlAWDLli0my6tUKnh4eEh+apKjiVYYD2eudEFERFQbrH7HnTp1KuLi4hAZGYnOnTtjwYIFyMnJwdixYwEAo0ePRv369TFnzhwAwJQpU9CjRw988sknGDBgAH7++WccOHAAX331lTUvQ8dBwZYbIiIia7J6cDN8+HCkpKRg5syZSExMRIcOHbBx40Zd0nBCQgLk8tLWkK5du+LHH3/EG2+8gddffx3NmjXD2rVr0bZtW2tdgoSj3FTLDYMbIiKi2mD14AYAJk2ahEmTJhndt2PHDoNtw4YNw7Bhw2q4VlVjquXG3ckmXmoiIiK7x2E6FiYzHtvAVcXghoiIqDYwuLGwwmKN0e2mRlERERGRZTG4sbACE8GNgsENERFRrWBwY2EFRaZabvhSExER1QbecS2soFit+33WwNa630013MwfHl7TVSIiIrqrMLixMP1uKRelQve7zESm8ZCODWq8TkRERHcTBjcWll9U2nLjrJSOkPrj+XtruzpERER3HQY3FqbfcuNVZuK+tvU9a7s6REREdx1OvmJh+sFNt6a+6N3SH00D3KxYIyIiorsLgxsLU2uE7neFXIZvx3SyYm2IiIjuPuyWsrBGPi4AADfOSExERGQVDG4s7Nu4SPRvF4hfn4u2dlWIiIjuSmxesLCm/u74YlSEtatBRER012LLDREREdkVBjdERERkVxjcEBERkV1hcFPLXurT3NpVICIismsMbmrZpPub4btxndGwnguWjom0dnWIiIjsjkwIISouZj8yMzPh6emJjIwMeHh4WLs6REREZIbK3L/ZckNERER2hcENERER2RUGN0RERGRXGNwQERGRXWFwQ0RERHaFwQ0RERHZFQY3REREZFcY3BAREZFdYXBDREREdoXBDREREdkVBjdERERkVxjcEBERkV1hcENERER2hcENERER2RUHa1egtgkhAJQsnU5ERER1g/a+rb2Pl+euC26ysrIAACEhIVauCREREVVWVlYWPD09yy0jE+aEQHZEo9Hgxo0bcHd3h0wms+i5MzMzERISgmvXrsHDw8Oi57YF9n59gP1fo71fH2D/18jrq/vs/Rpr6vqEEMjKykJwcDDk8vKzau66lhu5XI4GDRrU6HN4eHjY5QdWy96vD7D/a7T36wPs/xp5fXWfvV9jTVxfRS02WkwoJiIiIrvC4IaIiIjsCoMbC1KpVJg1axZUKpW1q1Ij7P36APu/Rnu/PsD+r5HXV/fZ+zXawvXddQnFREREZN/YckNERER2hcENERER2RUGN0RERGRXGNwQERGRXWFwYyGLFi1CaGgonJycEBUVhX379lm7Smb766+/MHDgQAQHB0Mmk2Ht2rWS/UIIzJw5E0FBQXB2dkZMTAzOnz8vKZOWloZRo0bBw8MDXl5eePLJJ5GdnV2LV2HanDlz0KlTJ7i7u8Pf3x+DBw/G2bNnJWXy8/MxceJE+Pj4wM3NDUOHDkVSUpKkTEJCAgYMGAAXFxf4+/vj5ZdfRnFxcW1eilGLFy9G+/btdRNmRUdHY8OGDbr9dfnajPnggw8gk8nwwgsv6LbV9WucPXs2ZDKZ5Kdly5a6/XX9+gDg+vXrePzxx+Hj4wNnZ2e0a9cOBw4c0O2v639nQkNDDd5DmUyGiRMnAqj776Farcabb76Jxo0bw9nZGWFhYXjnnXck6zzZ1HsoqNp+/vlnoVQqxdKlS8XJkyfF+PHjhZeXl0hKSrJ21cyyfv16MWPGDLF69WoBQKxZs0ay/4MPPhCenp5i7dq14ujRo2LQoEGicePGIi8vT1emb9++Ijw8XPz777/i77//Fk2bNhUjR46s5SsxLjY2VixbtkycOHFCHDlyRPTv3180bNhQZGdn68o8++yzIiQkRMTHx4sDBw6ILl26iK5du+r2FxcXi7Zt24qYmBhx+PBhsX79euHr6ytee+01a1ySxLp168Sff/4pzp07J86ePStef/114ejoKE6cOCGEqNvXVta+fftEaGioaN++vZgyZYpue12/xlmzZok2bdqImzdv6n5SUlJ0++v69aWlpYlGjRqJMWPGiL1794pLly6JTZs2iQsXLujK1PW/M8nJyZL3b8uWLQKA2L59uxCi7r+H7733nvDx8RF//PGHuHz5svjll1+Em5ubWLhwoa6MLb2HDG4soHPnzmLixIm6x2q1WgQHB4s5c+ZYsVZVUza40Wg0IjAwUHz88ce6benp6UKlUomffvpJCCHEqVOnBACxf/9+XZkNGzYImUwmrl+/Xmt1N1dycrIAIHbu3CmEKLkeR0dH8csvv+jKnD59WgAQe/bsEUKUBIByuVwkJibqyixevFh4eHiIgoKC2r0AM3h7e4tvvvnGrq4tKytLNGvWTGzZskX06NFDF9zYwzXOmjVLhIeHG91nD9f36quvinvvvdfkfnv8OzNlyhQRFhYmNBqNXbyHAwYMEOPGjZNse/jhh8WoUaOEELb3HrJbqpoKCwtx8OBBxMTE6LbJ5XLExMRgz549VqyZZVy+fBmJiYmS6/P09ERUVJTu+vbs2QMvLy9ERkbqysTExEAul2Pv3r21XueKZGRkAADq1asHADh48CCKiook19iyZUs0bNhQco3t2rVDQECArkxsbCwyMzNx8uTJWqx9+dRqNX7++Wfk5OQgOjrarq5t4sSJGDBggORaAPt5/86fP4/g4GA0adIEo0aNQkJCAgD7uL5169YhMjISw4YNg7+/Pzp27Iivv/5at9/e/s4UFhbi+++/x7hx4yCTyeziPezatSvi4+Nx7tw5AMDRo0exa9cu9OvXD4DtvYd33cKZlpaamgq1Wi35QAJAQEAAzpw5Y6VaWU5iYiIAGL0+7b7ExET4+/tL9js4OKBevXq6MrZCo9HghRdeQLdu3dC2bVsAJfVXKpXw8vKSlC17jcZeA+0+azt+/Diio6ORn58PNzc3rFmzBq1bt8aRI0fq/LUBwM8//4xDhw5h//79Bvvs4f2LiorC8uXL0aJFC9y8eRNvvfUWunfvjhMnTtjF9V26dAmLFy/G1KlT8frrr2P//v2YPHkylEol4uLi7O7vzNq1a5Geno4xY8YAsI/P6PTp05GZmYmWLVtCoVBArVbjvffew6hRowDY3r2CwQ3dVSZOnIgTJ05g165d1q6KRbVo0QJHjhxBRkYGfv31V8TFxWHnzp3WrpZFXLt2DVOmTMGWLVvg5ORk7erUCO23XwBo3749oqKi0KhRI6xatQrOzs5WrJllaDQaREZG4v333wcAdOzYESdOnMCSJUsQFxdn5dpZ3rfffot+/fohODjY2lWxmFWrVuGHH37Ajz/+iDZt2uDIkSN44YUXEBwcbJPvIbulqsnX1xcKhcIg6z0pKQmBgYFWqpXlaK+hvOsLDAxEcnKyZH9xcTHS0tJs6jWYNGkS/vjjD2zfvh0NGjTQbQ8MDERhYSHS09Ml5cteo7HXQLvP2pRKJZo2bYqIiAjMmTMH4eHhWLhwoV1c28GDB5GcnIx77rkHDg4OcHBwwM6dO/Hpp5/CwcEBAQEBdf4ay/Ly8kLz5s1x4cIFu3gPg4KC0Lp1a8m2Vq1a6bre7OnvzNWrV7F161Y89dRTum328B6+/PLLmD59OkaMGIF27drhiSeewIsvvog5c+YAsL33kMFNNSmVSkRERCA+Pl63TaPRID4+HtHR0VasmWU0btwYgYGBkuvLzMzE3r17ddcXHR2N9PR0HDx4UFdm27Zt0Gg0iIqKqvU6lyWEwKRJk7BmzRps27YNjRs3luyPiIiAo6Oj5BrPnj2LhIQEyTUeP35c8h9zy5Yt8PDwMPijbQs0Gg0KCgrs4tp69+6N48eP48iRI7qfyMhIjBo1Svd7Xb/GsrKzs3Hx4kUEBQXZxXvYrVs3g+kXzp07h0aNGgGwj78zWsuWLYO/vz8GDBig22YP72Fubi7kcmnIoFAooNFoANjge2jR9OS71M8//yxUKpVYvny5OHXqlHj66aeFl5eXJOvdlmVlZYnDhw+Lw4cPCwBi3rx54vDhw+Lq1atCiJLhfV5eXuK3334Tx44dEw899JDR4X0dO3YUe/fuFbt27RLNmjWzmSGazz33nPD09BQ7duyQDNXMzc3VlXn22WdFw4YNxbZt28SBAwdEdHS0iI6O1u3XDtPs06ePOHLkiNi4caPw8/OziWGa06dPFzt37hSXL18Wx44dE9OnTxcymUxs3rxZCFG3r80U/dFSQtT9a5w2bZrYsWOHuHz5sti9e7eIiYkRvr6+Ijk5WQhR969v3759wsHBQbz33nvi/Pnz4ocffhAuLi7i+++/15Wp639nhCgZKduwYUPx6quvGuyr6+9hXFycqF+/vm4o+OrVq4Wvr6945ZVXdGVs6T1kcGMhn332mWjYsKFQKpWic+fO4t9//7V2lcy2fft2AcDgJy4uTghRMsTvzTffFAEBAUKlUonevXuLs2fPSs5x69YtMXLkSOHm5iY8PDzE2LFjRVZWlhWuxpCxawMgli1bpiuTl5cnJkyYILy9vYWLi4sYMmSIuHnzpuQ8V65cEf369RPOzs7C19dXTJs2TRQVFdXy1RgaN26caNSokVAqlcLPz0/07t1bF9gIUbevzZSywU1dv8bhw4eLoKAgoVQqRf369cXw4cMlc8DU9esTQojff/9dtG3bVqhUKtGyZUvx1VdfSfbX9b8zQgixadMmAcCg3kLU/fcwMzNTTJkyRTRs2FA4OTmJJk2aiBkzZkiGqdvSeygTQm96QSIiIqI6jjk3REREZFcY3BAREZFdYXBDREREdoXBDREREdkVBjdERERkVxjcEBERkV1hcENERER2hcENEd31ZDIZ1q5da+1qEJGFMLghIqsaM2YMZDKZwU/fvn2tXTUiqqMcrF0BIqK+ffti2bJlkm0qlcpKtSGiuo4tN0RkdSqVCoGBgZIfb29vACVdRosXL0a/fv3g7OyMJk2a4Ndff5Ucf/z4cdx///1wdnaGj48Pnn76aWRnZ0vKLF26FG3atIFKpUJQUBAmTZok2Z+amoohQ4bAxcUFzZo1w7p162r2oomoxjC4ISKb9+abb2Lo0KE4evQoRo0ahREjRuD06dMAgJycHMTGxsLb2xv79+/HL7/8gq1bt0qCl8WLF2PixIl4+umncfz4caxbtw5NmzaVPMdbb72FRx99FMeOHUP//v0xatQopKWl1ep1EpGFWHwpTiKiSoiLixMKhUK4urpKft577z0hRMmq7s8++6zkmKioKPHcc88JIYT46quvhLe3t8jOztbt//PPP4VcLheJiYlCCCGCg4PFjBkzTNYBgHjjjTd0j7OzswUAsWHDBotdJxHVHubcEJHV9erVC4sXL5Zsq1evnu736Ohoyb7o6GgcOXIEAHD69GmEh4fD1dVVt79bt27QaDQ4e/YsZDIZbty4gd69e5dbh/bt2+t+d3V1hYeHB5KTk6t6SURkRQxuiMjqXF1dDbqJLMXZ2dmsco6OjpLHMpkMGo2mJqpERDWMOTdEZPP+/fdfg8etWrUCALRq1QpHjx5FTk6Obv/u3bshl8vRokULuLu7IzQ0FPHx8bVaZyKyHrbcEJHVFRQUIDExUbLNwcEBvr6+AIBffvkFkZGRuPfee/HDDz9g3759+PbbbwEAo0aNwqxZsxAXF4fZs2cjJSUFzz//PJ544gkEBAQAAGbPno1nn30W/v7+6NevH7KysrB79248//zztXuhRFQrGNwQkdVt3LgRQUFBkm0tWrTAmTNnAJSMZPr5558xYcIEBAUF4aeffkLr1q0BAC4uLti0aROmTJmCTp06wcXFBUOHDsW8efN054qLi0N+fj7mz5+Pl156Cb6+vnjkkUdq7wKJqFbJhBDC2pUgIjJFJpNhzZo1GDx4sLWrQkR1BHNuiIiIyK4wuCEiIiK7wpwbIrJp7Dknospiyw0RERHZFQY3REREZFcY3BAREZFdYXBDREREdoXBDREREdkVBjdERERkVxjcEBERkV1hcENERER2hcENERER2ZX/B0PuaiFiZ/KEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_rho_history, label='train rho')\n",
    "plt.plot(test_rho_history, label='test rho')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('rho')\n",
    "plt.title(' Spearman\\'s rank correlation coefficient')\n",
    "plt.legend()\n",
    "#plt.savefig('2-conv1d_OneHot-rho-2pool.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4science/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1393116/2864722082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEnzymesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1393116/183841129.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;31m#target = target.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mdf_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1393116/96650961.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "test_out= EnzymesDataset(test_df, train=False)\n",
    "\n",
    "submission_df =  predict(model,test_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"effect_mutation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+ 'test.csv',index_col='seq_id')\n",
    "test_df['tm']=submission_df['target'].values\n",
    "test_df = test_df.drop(columns=['protein_sequence','pH','data_source'])\n",
    "#submission_df['tm']=submission_df['target'].values\n",
    "#submission_df['seq_id']=submission_df[''].values\n",
    "#submission_df=submission_df.drop(columns=['target'])\n",
    "#submission_df=submission_df.drop(columns=[''])\n",
    "test_df.to_csv('Effect_mutation_seq.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a401f25d14e4726c47ec3d51a0ef0f076129e7cc070ddb98f69a4ab74ec023d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
