{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path + 'train_v1.csv',index_col=\"seq_id\")\n",
    "train_df = train_df.drop(columns=['data_source'])\n",
    "train_df = train_df.dropna()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Amino-acids to numbers and create a One-Channel array for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each amino acid in the training set\n",
    "map_dict = {} \n",
    "def count_frequency(s):\n",
    "    counter = Counter(s)\n",
    "    for p in s  : \n",
    "        if p in map_dict : \n",
    "            map_dict[p]+=counter[p]\n",
    "        else : \n",
    "            map_dict[p]= counter[p]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the map_dict with the frequency of each amino acid  (TODO:  better code )\n",
    "_ = train_df.apply(lambda row : count_frequency(row['protein_sequence']),axis=1)\n",
    "map_dict = dict(sorted(map_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "# Assign a number to each amino acid based on its frequency in decreasing order\n",
    "i = 1\n",
    "for key, value in map_dict.items():\n",
    "    map_dict[key]= i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map each amino acid to its number\n",
    "def chr_to_int(s):\n",
    "    l = []\n",
    "    for ch in s :\n",
    "       l.append(map[ch])\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column in the dataframe with the  amino acids in numerical form (array of numbers from 1 to 20)\n",
    "train_df['numerical_sequence'] = train_df.apply(lambda row : chr_to_int(row['protein_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column that contains the length of each protein sequence (before padding)\n",
    "train_df['length'] = train_df['protein_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(train_df['length'])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding inverted sequences (DISCRADED) \n",
    "**Add it to report as an experiment ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inverted_df = train_df.copy()\n",
    "\n",
    "#inver the protein sequence\n",
    "inverted_df['protein_sequence'] = inverted_df.apply(lambda row : row['protein_sequence'][::-1],axis=1)\n",
    "inverted_df['numerical_sequence'] = inverted_df.apply(lambda row : chr_to_int(row['protein_sequence']),axis=1)\n",
    "\n",
    "# merge the original and inverted dataframes (size of dataset is doubled)\n",
    "train_df = pd.concat([train_df,inverted_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Padding with zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add 0 to numerical_sequence to make all of them the same length\n",
    "padded_train_df = train_df.copy()\n",
    "padded_train_df['numerical_sequence'] = train_df.apply(lambda row : row['numerical_sequence'] + [0]*(max_length - row['length']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataframe to Pass to the dataloader (will be put in a function later)\n",
    "padded_train_df.drop(columns=['protein_sequence'],inplace=True)\n",
    "padded_train_df['y'] = padded_train_df['tm']\n",
    "padded_train_df.drop(columns=['tm'],inplace=True)\n",
    "padded_train_df['numerical_sequence'] = padded_train_df.apply(lambda row : np.array(row['numerical_sequence']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splot padded_train_df into train and validation sets (will be put in a function later)\n",
    "train_df = padded_train_df.sample(frac=0.8,random_state=200)\n",
    "val_df = padded_train_df.drop(train_df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1d conv net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get DataLoader from train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnzymesDataset(Dataset):\n",
    " \n",
    "  def __init__(self,df):\n",
    "    \n",
    "    # the Amino acid sequences as an int array\n",
    "    sequence= df.iloc[:]['numerical_sequence']\n",
    "    # numerical : pH and length\n",
    "    numerical = df.iloc[:,[0,2]].values\n",
    "\n",
    "    # y : the target (tm)\n",
    "    y=df.iloc[:,3].values\n",
    "  \n",
    "    #creta tensors from the numpy arrays\n",
    "    self.x_sequence=torch.tensor(sequence)\n",
    "    self.y=torch.tensor(y,dtype=torch.float32)\n",
    "    self.num=torch.tensor(numerical,dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_sequence[idx],self.y[idx] , self.num[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001 # Suggested for Adam\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataframes\n",
    "train_df = EnzymesDataset(train_df)\n",
    "val_df = EnzymesDataset(val_df)\n",
    "\n",
    "# create pytorch dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_df, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_df, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D_OneChannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.protein_sequence = nn.Sequential(\n",
    "            nn.Conv1d(1, 1,kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        self.prot_seq_one_pooling = nn.Sequential(\n",
    "\n",
    "            #With pooling only at the end (seen in paper)\n",
    "\n",
    "            nn.Conv1d(1, 64,kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.Conv1d(64, 32, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "            nn.AdaptiveAvgPool1d(32), #argument = output size \n",
    "            nn.Conv1d(32, 1, 5, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(),\n",
    "\n",
    "\n",
    "        )\n",
    "        self.numerical = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(94, 64),#input devrait être 32 + 64 plutôt non si on utilise MaxPoolId(2)? (était marqué 128 en input avant) Comme on fait le pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        x = self.prot_seq_one_pooling(x.float())\n",
    "        y = self.numerical(y)\n",
    "       \n",
    "        x = torch.cat((x.squeeze(1), y), 1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1D_OneChannel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "# defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (seq, target,num) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq.unsqueeze(1),num)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        print(\n",
    "            f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "            f\"batch_loss={loss.item():0.2e} \"\n",
    "            \n",
    "        )\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, target,num in test_loader:\n",
    "            output = model(seq.unsqueeze(1),num)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "           \n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:0.2e} \"\n",
    "    )\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test the model (save it after each epoch)\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(\n",
    "        model, optimizer, criterion, train_dl, epoch\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    test_loss = test_epoch(model, criterion, val_df)\n",
    "\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"1-Conv1d_OneChannel_model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss plot\n",
    "\n",
    "plt.plot(train_loss_history, label='train loss')\n",
    "plt.plot(test_loss_history, label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(' train and test MSE Loss')\n",
    "plt.legend()\n",
    "plt.savefig('plots/1-conv1d_OneChannel.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bed6b85c9b3e27cd25415a3806882bf36aa2546112033e7d0217f843875ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
